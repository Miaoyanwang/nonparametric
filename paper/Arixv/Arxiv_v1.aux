\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{fan2019generalized,hamidi2019low}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\citation{zhou2014regularized,wang2014network}
\citation{wang2017generalized,fan2019generalized}
\citation{Cai2016}
\citation{Ma2016}
\citation{caruana1997multitask,fan2019generalized}
\citation{candes2011tight}
\citation{recht2010guaranteed}
\newlabel{eq:linear}{{1}{2}{Introduction}{equation.1.1}{}}
\MT@newlabel{eq:linear}
\MT@newlabel{eq:linear}
\MT@newlabel{eq:linear}
\MT@newlabel{eq:linear}
\MT@newlabel{eq:linear}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Inadequacy of low-rank trace regression}{3}{subsection.1.1}}
\newlabel{sec:limit}{{1.1}{3}{Inadequacy of low-rank trace regression}{subsection.1.1}{}}
\MT@newlabel{eq:linear}
\MT@newlabel{eq:linear}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Two examples of high-rank matrix trace models. (a) The numerical rank of the matrix $g(\bm  {B})$ versus $c$ in the transformation, where the numerical rank is defined by $\textup  {rank}(g(\bm  {B}))=\qopname  \relax m{min}\{\textup  {rank}(\bm  {C})\penalty \@M \mskip 2mu\mathpunct {}\nonscript \mkern -\thinmuskip {:}\mskip 6muplus1mu\relax \delimiter 69645069 \bm  {C}-g(\bm  {B})\delimiter 86422285 _F \leq 0.01 \delimiter 69645069 g(\bm  {B})\delimiter 86422285 _F \}$. The error bar represents standard errors from 10 realizations of $\bm  {B}$. (b) Heatmap of a full-rank matrix $\bm  {B}\in \mathbb  {R}^{d\times d}$ with the $(i,j)$-th entry equal to $\qopname  \relax o{log}(1+\qopname  \relax m{max}(i,j))$. In (a), $d=50$, and in (b), $d=10$.\relax }}{4}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:limit}{{1}{4}{Two examples of high-rank matrix trace models. (a) The numerical rank of the matrix $g(\mB )$ versus $c$ in the transformation, where the numerical rank is defined by $\rank (g(\mB ))=\min \{\rank (\mC )\colon \FnormSize {}{\mC -g(\mB )} \leq 0.01 \FnormSize {}{g(\mB )} \}$. The error bar represents standard errors from 10 realizations of $\mB $. (b) Heatmap of a full-rank matrix $\mB \in \mathbb {R}^{d\times d}$ with the $(i,j)$-th entry equal to $\log (1+\max (i,j))$. In (a), $d=50$, and in (b), $d=10$.\relax }{figure.caption.1}{}}
\newlabel{penG}{{1}{4}{Two examples of high-rank matrix trace models. (a) The numerical rank of the matrix $g(\mB )$ versus $c$ in the transformation, where the numerical rank is defined by $\rank (g(\mB ))=\min \{\rank (\mC )\colon \FnormSize {}{\mC -g(\mB )} \leq 0.01 \FnormSize {}{g(\mB )} \}$. The error bar represents standard errors from 10 realizations of $\mB $. (b) Heatmap of a full-rank matrix $\mB \in \mathbb {R}^{d\times d}$ with the $(i,j)$-th entry equal to $\log (1+\max (i,j))$. In (a), $d=50$, and in (b), $d=10$.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Our proposal and contributions}{4}{subsection.1.2}}
\citation{fan2019generalized,hamidi2019low}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Related work}{5}{subsection.1.3}}
\citation{goodfellow2016deep}
\citation{hao2019sparse}
\citation{zhou2020broadcasted}
\citation{tsybakov1997nonparametric}
\citation{gibou2018review}
\citation{wang2008probability}
\citation{singh2009adaptive}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Notation and organization}{7}{subsection.1.4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Nonparametric trace regression model}{7}{section.2}}
\newlabel{sec:idea}{{2}{7}{Nonparametric trace regression model}{section.2}{}}
\newlabel{eq:model}{{2}{7}{Nonparametric trace regression model}{equation.2.2}{}}
\MT@newlabel{eq:model}
\citation{zhou2014regularized,wang2017generalized,fan2019generalized}
\citation{balabdaoui2019least,ganti2017learning}
\citation{hu2020matrix}
\newlabel{def:caliF}{{1}{8}{Rank-$r$ sign representable function}{defn.1}{}}
\newlabel{eq:sign}{{3}{8}{Rank-$r$ sign representable function}{equation.2.3}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:sign}
\citation{cohn2013fast}
\citation{de2003nondeterministic}
\citation{chan2014consistent}
\MT@newlabel{eq:sign}
\newlabel{prop:signbasis}{{1}{9}{Sign-representable function over basis matrices}{prop.1}{}}
\MT@newlabel{eq:sign}
\newlabel{prop:signrank}{{2}{9}{Sign-rank vs. matrix rank}{prop.2}{}}
\newlabel{ex:high-rank}{{5}{10}{High-rank matrix completion model}{example.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}From classification to regression: a learning reduction approach}{10}{section.3}}
\newlabel{sec:bridge}{{3}{10}{From classification to regression: a learning reduction approach}{section.3}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:sign}
\newlabel{eq:proposal}{{4}{10}{From classification to regression: a learning reduction approach}{equation.3.4}{}}
\newlabel{eq:stepfunction}{{5}{10}{From classification to regression: a learning reduction approach}{equation.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Nonparametric matrix regression via sign function series estimation. We use a series of weighted classifications to estimate the sign functions, then obtain the regression function estimate via sign aggregations. Here, $\bm  {X}\in \mathcal  {X}$ denotes matrix-valued predictor, $f\penalty \@M \mskip 2mu\mathpunct {}\nonscript \mkern -\thinmuskip {:}\mskip 6muplus1mu\relax \mathcal  {X}\to \mathbb  {R}$ denotes regression function, and $\textup  {sgn}(f-\pi )\in \{-1,1\}$ is the sign function, where $\pi \in \{-1,\ldots  ,-1/H,0,1/H,\ldots  , 1\}$ is the series of levels to aggregate in our algorithm.\relax }}{11}{figure.caption.2}}
\newlabel{fig:method}{{2}{11}{Nonparametric matrix regression via sign function series estimation. We use a series of weighted classifications to estimate the sign functions, then obtain the regression function estimate via sign aggregations. Here, $\mX \in \tX $ denotes matrix-valued predictor, $f\colon \tX \to \mathbb {R}$ denotes regression function, and $\sign (f-\pi )\in \{-1,1\}$ is the sign function, where $\pi \in \{-1,\ldots ,-1/H,0,1/H,\ldots , 1\}$ is the series of levels to aggregate in our algorithm.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Statistical characterization of sign functions via weighted classification}{11}{subsection.3.1}}
\MT@newlabel{eq:proposal}
\newlabel{eq:loss}{{6}{11}{Statistical characterization of sign functions via weighted classification}{equation.3.6}{}}
\MT@newlabel{eq:loss}
\MT@newlabel{eq:loss}
\newlabel{eq:constrained}{{7}{11}{Statistical characterization of sign functions via weighted classification}{equation.3.7}{}}
\citation{tsybakov2004optimal,singh2009adaptive}
\MT@newlabel{eq:constrained}
\newlabel{thm:oracle}{{3.1}{12}{Global optimum of weighted classification risk}{thm.3.1}{}}
\MT@newlabel{eq:model}
\MT@newlabel{eq:sign}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Identifiability}{12}{subsection.3.2}}
\newlabel{sec:identifiability}{{3.2}{12}{Identifiability}{subsection.3.2}{}}
\newlabel{ass:decboundary}{{2}{12}{$\alpha $-smoothness}{defn.2}{}}
\newlabel{eq:mass}{{8}{12}{$\alpha $-smoothness}{equation.3.8}{}}
\MT@newlabel{eq:mass}
\MT@newlabel{eq:mass}
\MT@newlabel{eq:mass}
\citation{singh2009adaptive,xu2020class}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Three examples of CDF, $G(\pi )=\mathbb  {P}_{\bm  {X}}(f(\bm  {X})\leq \pi )$, with local smoothness index $\alpha $ at $\pi $ depicted in dashed line. (a) and (b). Function $G(\pi )$ $\alpha =1$ because the $G(\pi )$ has finite sub-derivatives in the range of $\pi $; (c). Function $G(\pi )$ with $\alpha =\infty $ at most $\pi $ (in blue), except for a total number of $|\mathcal  {N}|=r$ jump points (in red). Here $|\mathcal  {N}|$ denotes the number of jump points.\relax }}{13}{figure.caption.3}}
\newlabel{fig:CDF}{{3}{13}{Three examples of CDF, $G(\pi )=\mathbb {P}_{\mX }(f(\mX )\leq \pi )$, with local smoothness index $\alpha $ at $\pi $ depicted in dashed line. (a) and (b). Function $G(\pi )$ $\alpha =1$ because the $G(\pi )$ has finite sub-derivatives in the range of $\pi $; (c). Function $G(\pi )$ with $\alpha =\infty $ at most $\pi $ (in blue), except for a total number of $|\tN |=r$ jump points (in red). Here $|\tN |$ denotes the number of jump points.\relax }{figure.caption.3}{}}
\newlabel{thm:identifiability}{{3.2}{13}{Identifiability}{thm.3.2}{}}
\newlabel{eq:identity}{{9}{13}{Identifiability}{equation.3.9}{}}
\MT@newlabel{eq:identity}
\MT@newlabel{eq:identity}
\MT@newlabel{eq:identity}
\MT@newlabel{eq:identity}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Regression risk bound}{14}{subsection.3.3}}
\MT@newlabel{eq:proposal}
\MT@newlabel{eq:stepfunction}
\MT@newlabel{eq:proposal}
\newlabel{thm:main}{{3.3}{14}{Sign function estimation}{thm.3.3}{}}
\newlabel{eq:riskbound}{{10}{14}{Sign function estimation}{equation.3.10}{}}
\MT@newlabel{eq:riskbound}
\newlabel{thm:regression}{{3.4}{14}{Regression function estimation}{thm.3.4}{}}
\newlabel{eq:bound}{{11}{14}{Regression function estimation}{equation.3.11}{}}
\newlabel{eq:final}{{12}{14}{Regression function estimation}{equation.3.12}{}}
\citation{zhou2014regularized}
\citation{Zhang2015}
\MT@newlabel{eq:bound}
\MT@newlabel{eq:final}
\MT@newlabel{eq:riskbound}
\@writefile{toc}{\contentsline {section}{\numberline {4}Two applications of nonparametric matrix learning}{15}{section.4}}
\newlabel{sec:examples}{{4}{15}{Two applications of nonparametric matrix learning}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Low-rank sparse matrix predictor regression}{15}{subsection.4.1}}
\newlabel{sec:sparse}{{4.1}{15}{Low-rank sparse matrix predictor regression}{subsection.4.1}{}}
\MT@newlabel{eq:sign}
\MT@newlabel{eq:sign}
\newlabel{thm:sparse}{{4.1}{16}{Nonparametric low-rank two-way sparse regression}{thm.4.1}{}}
\MT@newlabel{eq:stepfunction}
\MT@newlabel{eq:stepfunction}
\newlabel{eq:final2}{{13}{16}{Nonparametric low-rank two-way sparse regression}{equation.4.13}{}}
\MT@newlabel{eq:final2}
\MT@newlabel{eq:final2}
\MT@newlabel{eq:final}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}High-rank matrix completion}{16}{subsection.4.2}}
\newlabel{sec:matrixcompletion}{{4.2}{16}{High-rank matrix completion}{subsection.4.2}{}}
\newlabel{eq:modelcompletion}{{14}{16}{High-rank matrix completion}{equation.4.14}{}}
\MT@newlabel{eq:modelcompletion}
\MT@newlabel{sec:idea}
\MT@newlabel{eq:modelcompletion}
\MT@newlabel{eq:modelcompletion}
\newlabel{eq:est}{{15}{16}{High-rank matrix completion}{equation.4.15}{}}
\citation{chi2020provable}
\citation{gao2016optimal}
\newlabel{eq:smooth}{{3}{17}{$\alpha $-smoothness for discrete distribution}{defn.3}{}}
\MT@newlabel{eq:est}
\newlabel{thm:estimation}{{4.2}{17}{Nonparametric matrix completion}{thm.4.2}{}}
\MT@newlabel{eq:modelcompletion}
\MT@newlabel{eq:est}
\newlabel{eq:real}{{16}{17}{Nonparametric matrix completion}{equation.4.16}{}}
\MT@newlabel{eq:real}
\citation{ganti2015matrix}
\citation{ganti2015matrix}
\citation{yuan2016tensor,pmlr-v119-lee20i}
\citation{shen2003psi}
\newlabel{thm:sample-complexity}{{1}{18}{Sample complexity for nonparametric completion}{corollary.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Large-margin implementation and ADMM algorithm}{18}{section.5}}
\newlabel{sec:estimation}{{5}{18}{Large-margin implementation and ADMM algorithm}{section.5}{}}
\citation{shen2003psi}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Large-margin learning}{19}{subsection.5.1}}
\MT@newlabel{eq:loss}
\newlabel{eq:large-margin}{{17}{19}{Large-margin learning}{equation.5.17}{}}
\MT@newlabel{eq:large-margin}
\newlabel{eq:stepfunction-large-margin}{{18}{19}{Large-margin learning}{equation.5.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}ADMM optimization}{19}{subsection.5.2}}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:large-margin}
\newlabel{eq:sampleoptim}{{19}{19}{ADMM optimization}{equation.5.19}{}}
\MT@newlabel{eq:sampleoptim}
\MT@newlabel{eq:sampleoptim}
\MT@newlabel{eq:sampleoptim}
\newlabel{eq:ADMM}{{5.2}{19}{ADMM optimization}{equation.5.19}{}}
\citation{wang2008probability}
\citation{shen2003psi}
\citation{Ma2013}
\citation{Ma2016}
\citation{Ma2016}
\citation{parikh2014proximal}
\newlabel{eq:primal}{{5.2}{20}{ADMM optimization}{equation.5.19}{}}
\MT@newlabel{eq:primal}
\newlabel{eq:dual}{{5.2}{20}{ADMM optimization}{equation.5.19}{}}
\MT@newlabel{eq:sampleoptim}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Hyperparameter tuning}{20}{subsection.5.3}}
\citation{scott2011surrogate}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces {\bf  Nonparametric low-rank two-way sparse matrix regression via ADMM} \relax }}{21}{algorithm.1}}
\newlabel{alg:weighted}{{1}{21}{{\bf Nonparametric low-rank two-way sparse matrix regression via ADMM} \relax }{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Large-margin statistical guarantees}{21}{subsection.5.4}}
\newlabel{sec:large-margin}{{5.4}{21}{Large-margin statistical guarantees}{subsection.5.4}{}}
\newlabel{eq:riskdef}{{5.4}{21}{Large-margin statistical guarantees}{subsection.5.4}{}}
\newlabel{ass:main}{{1}{21}{Assumptions on surrogate loss}{assumption.1}{}}
\newlabel{eq:fisher}{{5.4}{22}{Large-margin statistical guarantees}{assumption.1}{}}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:stepfunction-large-margin}
\newlabel{thm:extension}{{5.1}{22}{Large-margin estimation}{thm.5.1}{}}
\MT@newlabel{eq:stepfunction-large-margin}
\MT@newlabel{eq:large-margin}
\@writefile{toc}{\contentsline {section}{\numberline {6}Simulations}{22}{section.6}}
\newlabel{sec:simulation}{{6}{22}{Simulations}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Impacts of sample size, matrix dimension, and model complexity}{22}{subsection.6.1}}
\newlabel{sec:validation}{{6.1}{22}{Impacts of sample size, matrix dimension, and model complexity}{subsection.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Finite sample performance under a smooth function. (a) simulation setup; (b) prediction error with varying $n$ and $d=20$ for the continuous response; (c) for the binary response; (d) with varying $d$ and $n=200$. The dashed lines in panels (b)-(d) represent upper bounds $\mathcal  {O}(n^{-1/3})$, $\mathcal  {O}(n^{-1/3})$, and $\mathcal  {O}(\qopname  \relax o{log}d)$, respectively. The results are based on 30 data replications.\relax }}{23}{figure.caption.4}}
\newlabel{fig:logistic}{{4}{23}{Finite sample performance under a smooth function. (a) simulation setup; (b) prediction error with varying $n$ and $d=20$ for the continuous response; (c) for the binary response; (d) with varying $d$ and $n=200$. The dashed lines in panels (b)-(d) represent upper bounds $\tO (n^{-1/3})$, $\tO (n^{-1/3})$, and $\tO (\log d)$, respectively. The results are based on 30 data replications.\relax }{figure.caption.4}{}}
\citation{relion2019network}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Finite sample performance under a non-smooth function. The setup is similar as Fig\nobreakspace  {}\ref  {fig:logistic}. The dashed lines in panels (b)-(d) represent upper bounds $\mathcal  {O}(n^{-1/2})$, $\mathcal  {O}(n^{-1/2})$, and $\mathcal  {O}(\qopname  \relax o{log}d)$, respectively.\relax }}{24}{figure.caption.5}}
\newlabel{fig:step}{{5}{24}{Finite sample performance under a non-smooth function. The setup is similar as Fig~\ref {fig:logistic}. The dashed lines in panels (b)-(d) represent upper bounds $\tO (n^{-1/2})$, $\tO (n^{-1/2})$, and $\tO (\log d)$, respectively.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Comparison with alternative methods}{24}{subsection.6.2}}
\newlabel{sec:comparison}{{6.2}{24}{Comparison with alternative methods}{subsection.6.2}{}}
\newlabel{eq:pattern}{{20}{24}{Comparison with alternative methods}{equation.6.20}{}}
\citation{Zou2005}
\citation{relion2019network}
\citation{chollet2018deep}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Four activation patterns in simulations. The active region is divided into four or five subregions, denoted by I, II, ..., V, each of which has its own edge connectivity signal $g_{pq}(\pi )$.\relax }}{25}{figure.caption.6}}
\newlabel{fig:region}{{6}{25}{Four activation patterns in simulations. The active region is divided into four or five subregions, denoted by I, II, ..., V, each of which has its own edge connectivity signal $g_{pq}(\pi )$.\relax }{figure.caption.6}{}}
\MT@newlabel{eq:pattern}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Performance comparison of various methods under four different activation patterns. Reported are the prediction error $\delimiter 69645069 \mathaccentV {hat}05Ef - f\delimiter 86422285 _1$, denoted by ``regression", and the misclassification error at $\pi =1/2$, denoted by ``classification". The results are based on 30 data replications.\relax }}{26}{figure.caption.7}}
\newlabel{fig:compare}{{7}{26}{Performance comparison of various methods under four different activation patterns. Reported are the prediction error $\onenormSize {}{\hat f - f}$, denoted by ``regression", and the misclassification error at $\pi =1/2$, denoted by ``classification". The results are based on 30 data replications.\relax }{figure.caption.7}{}}
\citation{van2013wu}
\citation{desikan2006automated}
\citation{zhang2018mapping}
\citation{wang2019common}
\citation{hastie2015statistical}
\citation{wang2019common}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Example output returned by {\bf  \relax \fontsize  {8}{9.5}\selectfont  ASSIST} based on the moving average of the feature weights, and the scatter plot of the edge connectivity strength, averaged by each subregion, versus the estimated mean response. The dashed curve shows the true function. \relax }}{27}{figure.caption.8}}
\newlabel{fig:compare2}{{8}{27}{Example output returned by {\bf \scriptsize ASSIST} based on the moving average of the feature weights, and the scatter plot of the edge connectivity strength, averaged by each subregion, versus the estimated mean response. The dashed curve shows the true function. \relax }{figure.caption.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Real data applications}{27}{section.7}}
\newlabel{sec:realdata}{{7}{27}{Real data applications}{section.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Brain connectivity analysis}{27}{subsection.7.1}}
\newlabel{sec:brain}{{7.1}{27}{Brain connectivity analysis}{subsection.7.1}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Brain connectivity analysis. (a) Comparison of prediction accuracy measured by AUC, with standard errors over 5-fold cross validation in the parentheses. For {\relax \fontsize  {8}{9.5}\selectfont  \bf  CNN}, there is no report for node selection. (b) Top edges selected by the method {\relax \fontsize  {8}{9.5}\selectfont  \bf  ASSIST-p}. The letters ``r'' and ``l'' in node names indicate the right and left hemisphere, respectively. The $p$-value is calculated from the two-sample test of edge connection strength between two individual groups. \relax }}{28}{table.caption.9}}
\newlabel{fig:real}{{1}{28}{Brain connectivity analysis. (a) Comparison of prediction accuracy measured by AUC, with standard errors over 5-fold cross validation in the parentheses. For {\scriptsize \bf CNN}, there is no report for node selection. (b) Top edges selected by the method {\scriptsize \bf ASSIST-p}. The letters ``r'' and ``l'' in node names indicate the right and left hemisphere, respectively. The $p$-value is calculated from the two-sample test of edge connection strength between two individual groups. \relax }{table.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Brain connectivity analysis. (a) Top edges overlaid on a brain template. (b) Edge connectivity strength versus estimated mean response. Colored curves represent the moving averages of connectivity strengths, gray bands represent one standard error, and jitter points represent the raw connectivity values (0 or 1).\relax }}{28}{figure.caption.10}}
\newlabel{fig:real2}{{9}{28}{Brain connectivity analysis. (a) Top edges overlaid on a brain template. (b) Edge connectivity strength versus estimated mean response. Colored curves represent the moving averages of connectivity strengths, gray bands represent one standard error, and jitter points represent the raw connectivity values (0 or 1).\relax }{figure.caption.10}{}}
\citation{hastie2015matrix}
\citation{mazumder2010spectral}
\citation{rennie2005fast}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Matrix completion analysis. (a)-(b) correspond to the 40\% missing rate, and (c)-(d) the 80\% missing rate. Error bars represent the standard error over 5-fold cross-validation. Numbers in the parentheses represent the selected tuning parameters for each method. In (a) and (c), we omit the worst method {\bf  \relax \fontsize  {8}{9.5}\selectfont  ALT} for space consideration.\relax }}{29}{figure.caption.11}}
\newlabel{fig:braincv}{{10}{29}{Matrix completion analysis. (a)-(b) correspond to the 40\% missing rate, and (c)-(d) the 80\% missing rate. Error bars represent the standard error over 5-fold cross-validation. Numbers in the parentheses represent the selected tuning parameters for each method. In (a) and (c), we omit the worst method {\bf \scriptsize ALT} for space consideration.\relax }{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Imaging matrix completion}{29}{subsection.7.2}}
\newlabel{sec:completion}{{7.2}{29}{Imaging matrix completion}{subsection.7.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Discussion}{29}{section.8}}
\newlabel{sec:discussion}{{8}{29}{Discussion}{section.8}{}}
\bibstyle{plainnat}
\bibdata{ref-trace.bib}
\bibcite{balabdaoui2019least}{{1}{2019}{{Balabdaoui et~al.}}{{Balabdaoui, Durot, and Jankowski}}}
\bibcite{bartlett2006convexity}{{2}{2006}{{Bartlett et~al.}}{{Bartlett, Jordan, and McAuliffe}}}
\bibcite{Cai2016}{{3}{2016}{{Cai et~al.}}{{Cai, Cai, and Zhang}}}
\bibcite{candes2011tight}{{4}{2011}{{Candes and Plan}}{{}}}
\bibcite{caruana1997multitask}{{5}{1997}{{Caruana}}{{}}}
\bibcite{chan2014consistent}{{6}{2014}{{Chan and Airoldi}}{{}}}
\bibcite{chi2020provable}{{7}{2020}{{Chi et~al.}}{{Chi, Gaines, Sun, Zhou, and Yang}}}
\bibcite{chollet2018deep}{{8}{2018}{{Chollet and Allaire}}{{}}}
\bibcite{cohn2013fast}{{9}{2013}{{Cohn and Umans}}{{}}}
\bibcite{de2003nondeterministic}{{10}{2003}{{De~Wolf}}{{}}}
\bibcite{desikan2006automated}{{11}{2006}{{Desikan et~al.}}{{Desikan, S{\'e}gonne, Fischl, Quinn, Dickerson, Blacker, Buckner, Dale, Maguire, Hyman, et~al.}}}
\bibcite{fan2019generalized}{{12}{2019}{{Fan et~al.}}{{Fan, Gong, and Zhu}}}
\bibcite{ganti2017learning}{{13}{2017}{{Ganti et~al.}}{{Ganti, Rao, Balzano, Willett, and Nowak}}}
\bibcite{ganti2015matrix}{{14}{2015}{{Ganti et~al.}}{{Ganti, Balzano, and Willett}}}
\bibcite{gao2016optimal}{{15}{2016}{{Gao et~al.}}{{Gao, Lu, Ma, and Zhou}}}
\bibcite{gibou2018review}{{16}{2018}{{Gibou et~al.}}{{Gibou, Fedkiw, and Osher}}}
\bibcite{goodfellow2016deep}{{17}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{hamidi2019low}{{18}{2019}{{Hamidi and Bayati}}{{}}}
\bibcite{hao2019sparse}{{19}{2019}{{Hao et~al.}}{{Hao, Wang, Wang, Zhang, Yang, and Sun}}}
\bibcite{hastie2015matrix}{{20}{2015{a}}{{Hastie et~al.}}{{Hastie, Mazumder, Lee, and Zadeh}}}
\bibcite{hastie2015statistical}{{21}{2015{b}}{{Hastie et~al.}}{{Hastie, Tibshirani, and Wainwright}}}
\bibcite{hu2020matrix}{{22}{2020}{{Hu et~al.}}{{Hu, Shen, Zhou, and Kong}}}
\bibcite{kosorok2007introduction}{{23}{2007}{{Kosorok}}{{}}}
\bibcite{pmlr-v119-lee20i}{{24}{2020}{{Lee and Wang}}{{}}}
\bibcite{Ma2013}{{25}{2013}{{Ma}}{{}}}
\bibcite{mazumder2010spectral}{{26}{2010}{{Mazumder et~al.}}{{Mazumder, Hastie, and Tibshirani}}}
\bibcite{parikh2014proximal}{{27}{2014}{{Parikh and Boyd}}{{}}}
\bibcite{recht2010guaranteed}{{28}{2010}{{Recht et~al.}}{{Recht, Fazel, and Parrilo}}}
\bibcite{relion2019network}{{29}{2019}{{Reli{\'o}n et~al.}}{{Reli{\'o}n, Kessler, Levina, and Taylor}}}
\bibcite{rennie2005fast}{{30}{2005}{{Rennie and Srebro}}{{}}}
\bibcite{scott2011surrogate}{{31}{2011}{{Scott}}{{}}}
\bibcite{shen1994convergence}{{32}{1994}{{Shen and Wong}}{{}}}
\bibcite{shen2003psi}{{33}{2003}{{Shen et~al.}}{{Shen, Tseng, Zhang, and Wong}}}
\bibcite{singh2009adaptive}{{34}{2009}{{Singh et~al.}}{{Singh, Scott, and Nowak}}}
\bibcite{tsybakov2004optimal}{{35}{2004}{{Tsybakov}}{{}}}
\bibcite{tsybakov1997nonparametric}{{36}{1997}{{Tsybakov}}{{}}}
\bibcite{van2013wu}{{37}{2013}{{Van~Essen et~al.}}{{Van~Essen, Smith, Barch, Behrens, Yacoub, Ugurbil, and Consortium}}}
\bibcite{wang2008probability}{{38}{2008}{{Wang et~al.}}{{Wang, Shen, and Liu}}}
\bibcite{wang2019common}{{39}{2019}{{Wang et~al.}}{{Wang, Zhang, and Dunson}}}
\bibcite{wang2017generalized}{{40}{2017}{{Wang et~al.}}{{Wang, Zhu, and Initiative}}}
\bibcite{wang2014network}{{41}{2014}{{Wang et~al.}}{{Wang, Curry, and Montana}}}
\bibcite{xu2020class}{{42}{2020}{{Xu et~al.}}{{Xu, Dan, Khim, and Ravikumar}}}
\bibcite{Ma2016}{{43}{2016}{{Yang et~al.}}{{Yang, Ma, and Buja}}}
\bibcite{yuan2016tensor}{{44}{2016}{{Yuan and Zhang}}{{}}}
\bibcite{Zhang2015}{{45}{2015}{{Zhang et~al.}}{{Zhang, Wu, Li, Caffo, and Boatman-Reich}}}
\bibcite{zhang2018mapping}{{46}{2018}{{Zhang et~al.}}{{Zhang, Descoteaux, Zhang, Girard, Chamberland, Dunson, Srivastava, and Zhu}}}
\bibcite{zhou2014regularized}{{47}{2014}{{Zhou and Li}}{{}}}
\bibcite{zhou2020broadcasted}{{48}{2020}{{Zhou et~al.}}{{Zhou, Wong, and He}}}
\bibcite{Zou2005}{{49}{2005}{{Zou and Hastie}}{{}}}
\@writefile{toc}{\contentsline {section}{\numberline {A}Additional theoretical results}{35}{appendix.A}}
\newlabel{sec:additional}{{A}{35}{Additional theoretical results}{appendix.A}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Sign rank and matrix rank}{35}{subsection.A.1}}
\newlabel{sec:signrank}{{A.1}{35}{Sign rank and matrix rank}{subsection.A.1}{}}
\newlabel{example:max}{{8}{35}{Max graphon}{example.8}{}}
\newlabel{eq:matrix}{{A.1}{35}{Sign rank and matrix rank}{example.8}{}}
\newlabel{eq:max}{{21}{35}{Min/Max graphon}{equation.A.21}{}}
\MT@newlabel{eq:max}
\newlabel{eq:support}{{22}{36}{Sign rank and matrix rank}{equation.A.22}{}}
\MT@newlabel{eq:support}
\newlabel{eq:indicator}{{23}{36}{Sign rank and matrix rank}{equation.A.23}{}}
\MT@newlabel{eq:indicator}
\newlabel{eq:sum}{{24}{36}{Sign rank and matrix rank}{equation.A.24}{}}
\MT@newlabel{eq:sum}
\newlabel{example:banded}{{9}{37}{Banded matrices}{example.9}{}}
\newlabel{eq:A}{{25}{37}{Sign rank and matrix rank}{equation.A.25}{}}
\newlabel{eq:decrease}{{26}{37}{Sign rank and matrix rank}{equation.A.26}{}}
\MT@newlabel{eq:decrease}
\MT@newlabel{eq:A}
\MT@newlabel{eq:A}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Extension to sub-Gaussian noise}{38}{subsection.A.2}}
\newlabel{sec:sub-Gaussian}{{A.2}{38}{Extension to sub-Gaussian noise}{subsection.A.2}{}}
\newlabel{assm:subg}{{2}{38}{Sub-Gaussian noise}{assumption.2}{}}
\newlabel{thm:extension_gaussian}{{A.1}{39}{Extension of Theorem~\ref {thm:estimation} to sub-Gaussian noise}{thm.A.1}{}}
\newlabel{eq:matrix_sign}{{27}{39}{Extension of Theorem~\ref {thm:estimation} to sub-Gaussian noise}{Item.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Extension to unbounded number of mass points}{39}{subsection.A.3}}
\newlabel{sec:unbounded}{{A.3}{39}{Extension to unbounded number of mass points}{subsection.A.3}{}}
\newlabel{eq:inf}{{28}{40}{Extension to unbounded number of mass points}{equation.A.28}{}}
\MT@newlabel{eq:inf}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.4}Connection to structured matrix model with functional coefficients}{40}{subsection.A.4}}
\newlabel{sec:joint}{{A.4}{40}{Connection to structured matrix model with functional coefficients}{subsection.A.4}{}}
\newlabel{eq:scheme}{{29}{40}{Connection to structured matrix model with functional coefficients}{equation.A.29}{}}
\MT@newlabel{eq:scheme}
\MT@newlabel{eq:scheme}
\MT@newlabel{eq:scheme}
\MT@newlabel{eq:scheme}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.5}Adjusting for intercept and additional covariates}{41}{subsection.A.5}}
\newlabel{sec:intercept}{{A.5}{41}{Adjusting for intercept and additional covariates}{subsection.A.5}{}}
\newlabel{eq:phi}{{30}{41}{Adjusting for intercept and additional covariates}{equation.A.30}{}}
\newlabel{lem:intercept}{{1}{41}{bounded intercept}{lem.1}{}}
\MT@newlabel{eq:phi}
\MT@newlabel{eq:phi}
\newlabel{eq:b}{{31}{42}{Adjusting for intercept and additional covariates}{equation.A.31}{}}
\MT@newlabel{eq:phi}
\MT@newlabel{eq:b}
\MT@newlabel{eq:b}
\MT@newlabel{eq:b}
\MT@newlabel{eq:phi}
\MT@newlabel{eq:b}
\MT@newlabel{eq:b}
\newlabel{eq:new}{{32}{43}{Adjusting for intercept and additional covariates}{Item.12}{}}
\MT@newlabel{eq:new}
\MT@newlabel{eq:phi}
\MT@newlabel{eq:b}
\MT@newlabel{eq:b}
\@writefile{toc}{\contentsline {section}{\numberline {B}Proofs}{44}{appendix.B}}
\newlabel{sec:proofs}{{B}{44}{Proofs}{appendix.B}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Main notation}{44}{subsection.B.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Proof of Theorem\nobreakspace  {}\ref  {thm:oracle}}{45}{subsection.B.2}}
\newlabel{eq:risk}{{33}{45}{Proof of Theorem~\ref {thm:oracle}}{equation.B.33}{}}
\newlabel{eq:I}{{34}{45}{Proof of Theorem~\ref {thm:oracle}}{equation.B.34}{}}
\MT@newlabel{eq:I}
\MT@newlabel{eq:risk}
\newlabel{eq:minimum}{{B.2}{45}{Proof of Theorem~\ref {thm:oracle}}{equation.B.34}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.3}Proof of Theorem\nobreakspace  {}\ref  {thm:identifiability}}{45}{subsection.B.3}}
\MT@newlabel{eq:I}
\newlabel{eq:excess}{{35}{46}{Proof of Theorem~\ref {thm:identifiability}}{equation.B.35}{}}
\MT@newlabel{eq:excess}
\newlabel{eq:tail2}{{36}{46}{Proof of Theorem~\ref {thm:identifiability}}{Item.13}{}}
\MT@newlabel{eq:tail2}
\MT@newlabel{eq:tail2}
\newlabel{eq:cmultiidentity}{{37}{46}{Proof of Theorem~\ref {thm:identifiability}}{Item.14}{}}
\MT@newlabel{eq:tail2}
\newlabel{eq:infty}{{38}{46}{Proof of Theorem~\ref {thm:identifiability}}{Item.15}{}}
\MT@newlabel{eq:cmultiidentity}
\MT@newlabel{eq:infty}
\newlabel{eq:rmk}{{3}{46}{Bounding $L_1$ distance by classification risk}{rmk.3}{}}
\newlabel{eq:L1}{{3}{47}{Bounding $L_1$ distance by classification risk}{rmk.3}{}}
\MT@newlabel{eq:Tphi}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.4}Proofs of Theorem\nobreakspace  {}\ref  {thm:main} and Part (a) in Theorems\nobreakspace  {}\ref  {thm:extension}}{47}{subsection.B.4}}
\newlabel{sec:sign}{{B.4}{47}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{subsection.B.4}{}}
\citation{shen1994convergence}
\citation{wang2008probability}
\newlabel{thm:unified}{{B.1}{48}{Sign estimation}{thm.B.1}{}}
\MT@newlabel{eq:phi}
\newlabel{eq:unified_sign}{{39}{48}{Sign estimation}{equation.B.39}{}}
\MT@newlabel{eq:unified_sign}
\newlabel{rmk:lt}{{4}{48}{One-sided tail}{rmk.4}{}}
\MT@newlabel{eq:unified_sign}
\citation{shen1994convergence}
\citation{wang2008probability}
\citation{bartlett2006convexity}
\newlabel{lem:prepare}{{2}{49}{Conversion inequalities}{lem.2}{}}
\newlabel{eq:b1}{{40}{49}{Conversion inequalities}{equation.B.40}{}}
\newlabel{eq:b2}{{41}{49}{Conversion inequalities}{equation.B.41}{}}
\newlabel{lem:risk}{{3}{49}{Classification risk error}{lem.3}{}}
\citation{wang2008probability}
\citation{scott2011surrogate}
\newlabel{eq:mae}{{42}{50}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.16}{}}
\MT@newlabel{eq:b2}
\MT@newlabel{eq:mae}
\citation{wang2008probability}
\MT@newlabel{eq:b1}
\MT@newlabel{eq:b1}
\MT@newlabel{eq:b1}
\MT@newlabel{eq:b2}
\newlabel{eq:excess-hinge}{{43}{51}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.17}{}}
\MT@newlabel{eq:excess-hinge}
\MT@newlabel{eq:b1}
\newlabel{eq:Tphi}{{44}{52}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.18}{}}
\MT@newlabel{eq:b1}
\MT@newlabel{eq:b1}
\MT@newlabel{eq:b2}
\newlabel{eq:F}{{B.4}{53}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.18}{}}
\newlabel{eq:def2}{{45}{53}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.19}{}}
\newlabel{eq:outer}{{46}{54}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.20}{}}
\MT@newlabel{eq:outer}
\MT@newlabel{eq:outer}
\newlabel{eq:empro}{{47}{54}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.21}{}}
\newlabel{eq:first}{{48}{54}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.22}{}}
\MT@newlabel{eq:empro}
\MT@newlabel{eq:first}
\MT@newlabel{eq:outer}
\newlabel{eq:union}{{49}{55}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.23}{}}
\MT@newlabel{eq:delta}
\newlabel{eq:second}{{50}{55}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.24}{}}
\MT@newlabel{eq:union}
\newlabel{eq:gamma}{{51}{55}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.25}{}}
\newlabel{eq:equation}{{52}{55}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.26}{}}
\MT@newlabel{eq:equation}
\MT@newlabel{eq:equation}
\MT@newlabel{eq:equation}
\newlabel{eq:delta}{{53}{56}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.27}{}}
\newlabel{eq:tn}{{54}{56}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.28}{}}
\MT@newlabel{eq:union}
\MT@newlabel{eq:gamma}
\MT@newlabel{eq:union}
\MT@newlabel{eq:gamma}
\MT@newlabel{eq:delta}
\MT@newlabel{eq:gamma}
\newlabel{eq:tail}{{B.4}{56}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.28}{}}
\MT@newlabel{eq:delta}
\MT@newlabel{eq:tn}
\MT@newlabel{eq:def2}
\MT@newlabel{eq:outer}
\MT@newlabel{eq:outer}
\MT@newlabel{eq:second}
\newlabel{eq:01bd}{{55}{57}{Proofs of Theorem~\ref {thm:main} and Part (a) in Theorems~\ref {thm:extension}}{Item.29}{}}
\MT@newlabel{eq:01bd}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.5}Proofs of Theorem\nobreakspace  {}\ref  {thm:regression}, Theorem\nobreakspace  {}\ref  {thm:sparse}, and Part (b) in Theorem\nobreakspace  {}\ref  {thm:extension}}{58}{subsection.B.5}}
\newlabel{sec:regression}{{B.5}{58}{Proofs of Theorem~\ref {thm:regression}, Theorem~\ref {thm:sparse}, and Part (b) in Theorem~\ref {thm:extension}}{subsection.B.5}{}}
\newlabel{eq:mb}{{56}{58}{Proofs of Theorem~\ref {thm:regression}, Theorem~\ref {thm:sparse}, and Part (b) in Theorem~\ref {thm:extension}}{equation.B.56}{}}
\newlabel{eq:pfmain}{{57}{58}{Proofs of Theorem~\ref {thm:regression}, Theorem~\ref {thm:sparse}, and Part (b) in Theorem~\ref {thm:extension}}{equation.B.57}{}}
\MT@newlabel{eq:pfmain}
\MT@newlabel{eq:pfmain}
\newlabel{eq:twobounds}{{58}{59}{Proofs of Theorem~\ref {thm:regression}, Theorem~\ref {thm:sparse}, and Part (b) in Theorem~\ref {thm:extension}}{equation.B.58}{}}
\MT@newlabel{eq:twobounds}
\newlabel{eq:Hset}{{59}{59}{Proofs of Theorem~\ref {thm:regression}, Theorem~\ref {thm:sparse}, and Part (b) in Theorem~\ref {thm:extension}}{equation.B.59}{}}
\MT@newlabel{eq:pfmain}
\MT@newlabel{eq:twobounds}
\MT@newlabel{eq:Hset}
\MT@newlabel{eq:mb}
\newlabel{eq:A}{{60}{59}{Proofs of Theorem~\ref {thm:regression}, Theorem~\ref {thm:sparse}, and Part (b) in Theorem~\ref {thm:extension}}{equation.B.60}{}}
\newlabel{eq:prob}{{61}{59}{Proofs of Theorem~\ref {thm:regression}, Theorem~\ref {thm:sparse}, and Part (b) in Theorem~\ref {thm:extension}}{equation.B.61}{}}
\MT@newlabel{eq:prob}
\MT@newlabel{eq:A}
\MT@newlabel{eq:prob}
\newlabel{lem:H}{{4}{60}{}{lem.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.6}Proofs of Theorem\nobreakspace  {}\ref  {thm:estimation} and Theorem\nobreakspace  {}\ref  {thm:extension_gaussian}}{60}{subsection.B.6}}
\newlabel{sec:sub-Gaussianproof}{{B.6}{60}{Proofs of Theorem~\ref {thm:estimation} and Theorem~\ref {thm:extension_gaussian}}{subsection.B.6}{}}
\newlabel{eq:variance2}{{62}{61}{Proofs of Theorem~\ref {thm:estimation} and Theorem~\ref {thm:extension_gaussian}}{equation.B.62}{}}
\MT@newlabel{eq:variance2}
\newlabel{eq:vartomean}{{63}{61}{Proofs of Theorem~\ref {thm:estimation} and Theorem~\ref {thm:extension_gaussian}}{equation.B.63}{}}
\MT@newlabel{eq:vartomean}
\newlabel{eq:empriskbd}{{64}{61}{Proofs of Theorem~\ref {thm:estimation} and Theorem~\ref {thm:extension_gaussian}}{equation.B.64}{}}
\newlabel{eq:subgbd}{{65}{61}{Proofs of Theorem~\ref {thm:estimation} and Theorem~\ref {thm:extension_gaussian}}{equation.B.65}{}}
\MT@newlabel{eq:empriskbd}
\MT@newlabel{eq:subgbd}
\newlabel{eq:riskunbd}{{66}{61}{Proofs of Theorem~\ref {thm:estimation} and Theorem~\ref {thm:extension_gaussian}}{equation.B.66}{}}
\MT@newlabel{eq:riskunbd}
\MT@newlabel{eq:matrix_sign}
\@writefile{toc}{\contentsline {section}{\numberline {C}Auxiliary lemmas}{62}{appendix.C}}
\newlabel{sec:auxiliary}{{C}{62}{Auxiliary lemmas}{appendix.C}{}}
\newlabel{lem:hingeL1}{{5}{62}{Hinge loss and $L$-1 distance}{lem.5}{}}
\newlabel{eq:L}{{5}{62}{Hinge loss and $L$-1 distance}{lem.5}{}}
\newlabel{rmk:truncate}{{7}{62}{Truncated hinge loss and $L$-1 distance}{rmk.7}{}}
\MT@newlabel{eq:Tphi}
\newlabel{eq:function}{{67}{62}{Auxiliary lemmas}{equation.C.67}{}}
\MT@newlabel{eq:function}
\MT@newlabel{eq:function}
\MT@newlabel{eq:function}
\MT@newlabel{eq:function}
\newlabel{eq:integral}{{68}{63}{Auxiliary lemmas}{equation.C.68}{}}
\MT@newlabel{eq:integral}
\newlabel{pro:inftynorm}{{4}{63}{Bracketing number}{defn.4}{}}
\newlabel{lem:entropy}{{6}{63}{Bracketing number for bounded functions in $\Phi (r,s_1,s_2)$ and $\Phi (r)$}{lem.6}{}}
\citation{kosorok2007introduction}
\citation{candes2011tight}
\newlabel{eq:bracketsparse}{{69}{64}{Auxiliary lemmas}{equation.C.69}{}}
\MT@newlabel{eq:bracketsparse}
\newlabel{lem:metric}{{7}{65}{Local complexity of $\Phi (r,s_1,s_2)$ and $\Phi (r)$}{lem.7}{}}
\newlabel{eq:specification}{{70}{65}{Local complexity of $\Phi (r,s_1,s_2)$ and $\Phi (r)$}{equation.C.70}{}}
\MT@newlabel{eq:specification}
\newlabel{eq:complexity}{{C}{65}{Auxiliary lemmas}{equation.C.70}{}}
\newlabel{eq:g}{{71}{65}{Auxiliary lemmas}{equation.C.71}{}}
\MT@newlabel{eq:g}
\newlabel{lem:subg}{{8}{65}{sub-Gaussian maximum}{lem.8}{}}
\citation{scott2011surrogate}
\citation{scott2011surrogate}
\citation{shen1994convergence}
\newlabel{thm:scott}{{C.1}{66}{Theorem 1 in~\cite {scott2011surrogate}}{thm.C.1}{}}
\newlabel{thm:refer}{{C.2}{66}{Theorem 3 in~\cite {shen1994convergence}}{thm.C.2}{}}
\newlabel{eq:oneside}{{C.2}{67}{Theorem 3 in~\cite {shen1994convergence}}{thm.C.2}{}}
