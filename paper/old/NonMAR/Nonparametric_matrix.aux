\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{van2013wu}
\citation{zhou2014regularized}
\citation{fan2019generalized}
\citation{relion2019network}
\citation{wasserman2006all}
\citation{tsybakov2008introduction}
\citation{murdoch2019definitions}
\citation{vapnik2013nature}
\citation{wasserman2006all}
\citation{relion2019network}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Our contributions}{3}{subsection.1.1}}
\citation{gibou2018review}
\citation{gibou2018review}
\citation{fan2019generalized}
\citation{hamidi2019low}
\citation{relion2019network}
\citation{guha2020bayesian}
\citation{zhou2014regularized}
\citation{wang2017generalized}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Related work}{4}{subsection.1.2}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Parametric matrix regression}{4}{subsubsection.1.2.1}}
\citation{hardle1993optimal}
\citation{altman1992introduction}
\citation{wahba1990spline}
\citation{donoho1998minimax}
\citation{geenens2011curse}
\citation{guntuboyina2018nonparametric}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) Our learning reduction approach (solid line) to the three problems of interest. The classical plug-in approaches are depicted in dashed line. (b) Schematic diagram for nonparametric function estimation via level set aggregations (i.e,\ a series of weighted classifications). Figure (b) is modified from\nobreakspace  {}\cite  {gibou2018review}. \relax }}{5}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:diagram}{{1}{5}{(a) Our learning reduction approach (solid line) to the three problems of interest. The classical plug-in approaches are depicted in dashed line. (b) Schematic diagram for nonparametric function estimation via level set aggregations (i.e,\ a series of weighted classifications). Figure (b) is modified from~\cite {gibou2018review}. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Common nonparametric strategies}{5}{subsubsection.1.2.2}}
\citation{chen2019nonparametric}
\citation{hao2019sparse}
\citation{zhou2020broadcasted}
\citation{goodfellow2016deep}
\citation{tsybakov1997nonparametric}
\citation{gibou2018review}
\citation{atzmon2019controlling}
\citation{chen2017density}
\citation{varshney2010classification}
\citation{shekhar2019multiscale}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Nonparametric methods with manifolds}{6}{subsubsection.1.2.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.4}Regression level set estimation}{6}{subsubsection.1.2.4}}
\citation{tsybakov1997nonparametric}
\citation{singh2009adaptive}
\citation{audibert2007fast}
\citation{rigollet2009optimal}
\citation{wang2008probability}
\citation{singh2009adaptive}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Notation and organization}{7}{subsection.1.3}}
\citation{willett2007minimax}
\citation{scott2007regression}
\citation{wang2008probability}
\@writefile{toc}{\contentsline {section}{\numberline {2}Review: Three learning problems}{8}{section.2}}
\newlabel{sec:problem}{{2}{8}{Review: Three learning problems}{section.2}{}}
\newlabel{eq:classloss}{{1}{8}{Review: Three learning problems}{equation.2.1}{}}
\MT@newlabel{eq:classloss}
\newlabel{eq:bayes}{{{{2.1}}}{8}{Review: Three learning problems}{equation.2.1}{}}
\MT@newlabel{eq:classloss}
\MT@newlabel{eq:bayes}
\MT@newlabel{eq:classloss}
\newlabel{eq:level}{{{{2.2}}}{8}{Review: Three learning problems}{Item.2}{}}
\newlabel{eq:risklevel}{{2}{9}{Review: Three learning problems}{equation.2.2}{}}
\MT@newlabel{eq:classloss}
\MT@newlabel{eq:risklevel}
\newlabel{eq:regression}{{3}{9}{Review: Three learning problems}{equation.2.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}From classification to regression: a new deal}{9}{section.3}}
\newlabel{sec:idea}{{3}{9}{From classification to regression: a new deal}{section.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Level-set approaches to matrix binary nonparametric regression. We use weighted classification to find the level-set in the matrix space, and then estimate the target regression function via aggregation. \relax }}{10}{figure.caption.2}}
\newlabel{fig:method}{{2}{10}{Level-set approaches to matrix binary nonparametric regression. We use weighted classification to find the level-set in the matrix space, and then estimate the target regression function via aggregation. \relax }{figure.caption.2}{}}
\MT@newlabel{eq:classloss}
\MT@newlabel{eq:risklevel}
\MT@newlabel{eq:regression}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Level set approaches to nonparametric binary regression}{10}{subsection.3.1}}
\newlabel{sec:bridge}{{3.1}{10}{Level set approaches to nonparametric binary regression}{subsection.3.1}{}}
\newlabel{eq:stepfunction}{{4}{10}{Level set approaches to nonparametric binary regression}{equation.3.4}{}}
\newlabel{eq:constrained}{{5}{11}{Level set approaches to nonparametric binary regression}{equation.3.5}{}}
\MT@newlabel{eq:constrained}
\newlabel{ass:decboundary}{{2}{11}{$\alpha $-regularity}{defn.2}{}}
\newlabel{eq:mass}{{6}{11}{$\alpha $-regularity}{equation.3.6}{}}
\MT@newlabel{eq:mass}
\MT@newlabel{eq:constrained}
\newlabel{eq:riskdiff}{{3.1}{11}{Level set approaches to nonparametric binary regression}{equation.3.6}{}}
\newlabel{eq:setdiff}{{3.1}{11}{Level set approaches to nonparametric binary regression}{equation.3.6}{}}
\citation{singh2009adaptive}
\citation{xu2020class}
\newlabel{thm:identifiability}{{1}{12}{Identifiability}{prop.1}{}}
\newlabel{eq:identity}{{7}{12}{Identifiability}{equation.3.7}{}}
\MT@newlabel{eq:identity}
\MT@newlabel{eq:identity}
\MT@newlabel{eq:stepfunction}
\MT@newlabel{eq:constrained}
\newlabel{thm:twobounds}{{1}{12}{Nonparametric regression via weighted classifications}{thm.1}{}}
\MT@newlabel{eq:stepfunction}
\newlabel{eq:approximation}{{8}{12}{Nonparametric regression via weighted classifications}{equation.3.8}{}}
\MT@newlabel{eq:approximation}
\MT@newlabel{eq:constrained}
\citation{alquier2013sparse}
\citation{ganti2017learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Sparse and low-rank function boundaries}{13}{subsection.3.2}}
\newlabel{subsec:linear class}{{3.2}{13}{Sparse and low-rank function boundaries}{subsection.3.2}{}}
\MT@newlabel{eq:constrained}
\MT@newlabel{eq:constrained}
\newlabel{eq:optimization}{{9}{13}{Sparse and low-rank function boundaries}{equation.3.9}{}}
\newlabel{eq:class}{{10}{13}{Sparse and low-rank function boundaries}{equation.3.10}{}}
\MT@newlabel{eq:stepfunction}
\MT@newlabel{eq:optimization}
\MT@newlabel{eq:class}
\MT@newlabel{eq:class}
\MT@newlabel{eq:optimization}
\MT@newlabel{eq:class}
\citation{zhou2014regularized}
\citation{guha2020bayesian}
\citation{relion2019network}
\citation{hu2020matrix}
\newlabel{example:1}{{1}{14}{Monotonic single index models~\citep {alquier2013sparse,ganti2017learning}}{example.1}{}}
\newlabel{example2}{{2}{14}{Multivariate normal mixtures}{example.2}{}}
\MT@newlabel{eq:optimization}
\MT@newlabel{eq:class}
\@writefile{toc}{\contentsline {section}{\numberline {4}Estimation}{14}{section.4}}
\newlabel{sec:estimation}{{4}{14}{Estimation}{section.4}{}}
\citation{bartlett2006convexity}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Large-margin learning with high dimensional matrices}{15}{subsection.4.1}}
\newlabel{eq:stepfunctionsample}{{11}{15}{Large-margin learning with high dimensional matrices}{equation.4.11}{}}
\newlabel{eq:large-margin}{{12}{15}{Large-margin learning with high dimensional matrices}{equation.4.12}{}}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:optimization}
\MT@newlabel{eq:optimization}
\MT@newlabel{eq:optimization}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Alternating optimization for structural risk minimization}{15}{subsection.4.2}}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:class}
\MT@newlabel{eq:large-margin}
\citation{yang2016rate}
\citation{parikh2014proximal}
\newlabel{eq:sampleoptim}{{13}{16}{Alternating optimization for structural risk minimization}{equation.4.13}{}}
\MT@newlabel{eq:sampleoptim}
\newlabel{eq:ADMM}{{14}{16}{Alternating optimization for structural risk minimization}{equation.4.14}{}}
\MT@newlabel{eq:ADMM}
\MT@newlabel{eq:ADMM}
\newlabel{eq:primal}{{15}{16}{Alternating optimization for structural risk minimization}{equation.4.15}{}}
\MT@newlabel{eq:primal}
\newlabel{eq:dual}{{16}{16}{Alternating optimization for structural risk minimization}{equation.4.16}{}}
\MT@newlabel{eq:sampleoptim}
\MT@newlabel{eq:dual}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces {\bf  Matrix classification and level-set estimation via ADMM} \relax }}{17}{algocf.1}}
\newlabel{alg:weighted}{{1}{17}{Alternating optimization for structural risk minimization}{algocf.1}{}}
\MT@newlabel{eq:stepfunctionsample}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces {\bf  Nonparamatrix matrix regression} \relax }}{17}{algocf.2}}
\newlabel{alg:regest}{{2}{17}{Alternating optimization for structural risk minimization}{algocf.2}{}}
\citation{bartlett2006convexity}
\citation{scott2011surrogate}
\citation{tsybakov2004optimal}
\citation{shen2006discussion}
\citation{audibert2007fast}
\@writefile{toc}{\contentsline {section}{\numberline {5}Statistical learning theory}{18}{section.5}}
\newlabel{sec:theory}{{5}{18}{Statistical learning theory}{section.5}{}}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:stepfunctionsample}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:large-margin}
\newlabel{eq:excess-hinge}{{17}{18}{Statistical learning theory}{equation.5.17}{}}
\MT@newlabel{eq:excess-hinge}
\MT@newlabel{eq:excess-hinge}
\newlabel{ass:main}{{1}{18}{Approximation error}{assumption.1}{}}
\newlabel{thm:main}{{2}{18}{Accuracy for weighted matrix classification}{thm.2}{}}
\MT@newlabel{eq:large-margin}
\newlabel{eq:riskbound}{{18}{18}{Accuracy for weighted matrix classification}{equation.5.18}{}}
\MT@newlabel{eq:riskbound}
\MT@newlabel{eq:riskbound}
\MT@newlabel{eq:large-margin}
\newlabel{thm:level}{{1}{19}{Accuracy for level set estimation}{corollary.1}{}}
\newlabel{eq:set}{{19}{19}{Accuracy for level set estimation}{equation.5.19}{}}
\MT@newlabel{eq:set}
\newlabel{thm:regression}{{3}{19}{Accuracy for nonparametric matrix regression}{thm.3}{}}
\MT@newlabel{eq:stepfunctionsample}
\newlabel{eq:final}{{20}{19}{High-dimensional consistency}{equation.5.20}{}}
\citation{balabdaoui2019least}
\MT@newlabel{eq:final}
\MT@newlabel{eq:riskbound}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical experiments}{20}{section.6}}
\newlabel{sec:simulation}{{6}{20}{Numerical experiments}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Impacts of sample size, matrix dimension, and model complexity}{20}{subsection.6.1}}
\newlabel{sec:validation}{{6.1}{20}{Impacts of sample size, matrix dimension, and model complexity}{subsection.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Finite sample accuracy of matrix classification and regression. (a) simulation setup. (b) classification error with sample size when $d=30$. (c) classification error with matrix dimension when $n=200$. (d) regression error with sample size. The dashed line in panels (b)-(d) represent theoretical rates $\mathcal  {O}(n^{-2/3})$, $\mathcal  {O}(\qopname  \relax o{log}d)$, and $\mathcal  {O}(n^{-1/2})$, respectively. The reported statistics are averaged across 30 simulation replicates, with standard error given in the error bar.\relax }}{21}{figure.caption.5}}
\newlabel{fig:logistic}{{3}{21}{Finite sample accuracy of matrix classification and regression. (a) simulation setup. (b) classification error with sample size when $d=30$. (c) classification error with matrix dimension when $n=200$. (d) regression error with sample size. The dashed line in panels (b)-(d) represent theoretical rates $\tO (n^{-2/3})$, $\tO (\log d)$, and $\tO (n^{-1/2})$, respectively. The reported statistics are averaged across 30 simulation replicates, with standard error given in the error bar.\relax }{figure.caption.5}{}}
\MT@newlabel{eq:stepfunctionsample}
\citation{friedman2010regularization}
\citation{relion2019network}
\citation{chollet2018deep}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Finite sample accuracy under a different setting. (a) simulation setup. (b) classification error with sample size when $d=30$. (c) classification error with matrix dimension when $n=200$. (d) regression error with sample size. \relax }}{22}{figure.caption.6}}
\newlabel{fig:step}{{4}{22}{Finite sample accuracy under a different setting. (a) simulation setup. (b) classification error with sample size when $d=30$. (c) classification error with matrix dimension when $n=200$. (d) regression error with sample size. \relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Comparison with other methods}{22}{subsection.6.2}}
\newlabel{sec:comparison}{{6.2}{22}{Comparison with other methods}{subsection.6.2}{}}
\citation{relion2019network}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Four active pattern in simulations. The active region is divided into four or five subregions (denoted I, II, ...), each of which has its own edge connectivity signal $g_{pq}(\pi )$.\relax }}{23}{figure.caption.7}}
\newlabel{fig:region}{{5}{23}{Four active pattern in simulations. The active region is divided into four or five subregions (denoted I, II, ...), each of which has its own edge connectivity signal $g_{pq}(\pi )$.\relax }{figure.caption.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Performance comparison between various methods under four different active patterns. \relax }}{24}{figure.caption.8}}
\newlabel{fig:compare}{{6}{24}{Performance comparison between various methods under four different active patterns. \relax }{figure.caption.8}{}}
\citation{van2013wu}
\citation{zhang2018mapping}
\citation{desikan2006automated}
\citation{moore2015development}
\citation{wang2019common}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Example outputs returned by {\bf  \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip NonMAR}. Panels (a) and (c) plot the top edges selected by our method. Panels (b) and (d) are scatter plots of the edge connectivity strength (averaged by subregion) versus the estimated response probability. The ground truth function is depicted in dashed curve.\relax }}{25}{figure.caption.9}}
\newlabel{fig:compare2}{{7}{25}{Example outputs returned by {\bf \small NonMAR}. Panels (a) and (c) plot the top edges selected by our method. Panels (b) and (d) are scatter plots of the edge connectivity strength (averaged by subregion) versus the estimated response probability. The ground truth function is depicted in dashed curve.\relax }{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}human brain connectome data}{25}{subsection.6.3}}
\newlabel{sec:real}{{6.3}{25}{human brain connectome data}{subsection.6.3}{}}
\citation{hastie2015statistical}
\citation{wang2019common}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces HCP analysis results based on our method {\bf  \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip NonMAR}. (a) Comparison of prediction accuracy. (b) Top edges selected by our method {\relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip \bf  NonMAR-p}. \relax }}{26}{figure.caption.10}}
\newlabel{fig:real}{{8}{26}{HCP analysis results based on our method {\bf \small NonMAR}. (a) Comparison of prediction accuracy. (b) Top edges selected by our method {\small \bf NonMAR-p}. \relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces HCP analysis results based on our method {\bf  \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip NonMAR}. (a) Top edges overlaid on brain template. (b) Edge connectivity strength versus estimated response probability. Colored curves represent the moving averages of connectivity strengths, gray bands represent one standard error, and jitter points represent the raw connectivity values (0 or 1). \relax }}{26}{figure.caption.11}}
\newlabel{fig:real2}{{9}{26}{HCP analysis results based on our method {\bf \small NonMAR}. (a) Top edges overlaid on brain template. (b) Edge connectivity strength versus estimated response probability. Colored curves represent the moving averages of connectivity strengths, gray bands represent one standard error, and jitter points represent the raw connectivity values (0 or 1). \relax }{figure.caption.11}{}}
\citation{wang2016classification}
\citation{arroyo2020simultaneous}
\citation{willett2007minimax}
\citation{scott2007regression}
\bibstyle{chicago}
\bibdata{tensor_wang}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{27}{section.7}}
\newlabel{eq:continuousrisk}{{21}{27}{Discussion}{equation.7.21}{}}
\MT@newlabel{eq:continuousrisk}
\MT@newlabel{eq:risklevel}
\MT@newlabel{eq:continuousrisk}
\bibcite{alquier2013sparse}{{1}{2013}{{Alquier and Biau}}{{Alquier and Biau}}}
\bibcite{altman1992introduction}{{2}{1992}{{Altman}}{{Altman}}}
\bibcite{arroyo2020simultaneous}{{3}{2020}{{Arroyo and Levina}}{{Arroyo and Levina}}}
\bibcite{atzmon2019controlling}{{4}{2019}{{Atzmon et~al.}}{{Atzmon, Haim, Yariv, Israelov, Maron, and Lipman}}}
\bibcite{audibert2007fast}{{5}{2007}{{Audibert et~al.}}{{Audibert, Tsybakov, et~al.}}}
\bibcite{balabdaoui2019least}{{6}{2019}{{Balabdaoui et~al.}}{{Balabdaoui, Durot, Jankowski, et~al.}}}
\bibcite{bartlett2006convexity}{{7}{2006}{{Bartlett et~al.}}{{Bartlett, Jordan, and McAuliffe}}}
\bibcite{candes2011tight}{{8}{2011}{{Candes and Plan}}{{Candes and Plan}}}
\bibcite{chen2019nonparametric}{{9}{2019}{{Chen et~al.}}{{Chen, Jiang, Liao, and Zhao}}}
\bibcite{chen2017density}{{10}{2017}{{Chen et~al.}}{{Chen, Genovese, and Wasserman}}}
\bibcite{chollet2018deep}{{11}{2018}{{Chollet and Allaire}}{{Chollet and Allaire}}}
\bibcite{desikan2006automated}{{12}{2006}{{Desikan et~al.}}{{Desikan, S{\'e}gonne, Fischl, Quinn, Dickerson, Blacker, Buckner, Dale, Maguire, Hyman, et~al.}}}
\bibcite{donoho1998minimax}{{13}{1998}{{Donoho et~al.}}{{Donoho, Johnstone, et~al.}}}
\bibcite{fan2019generalized}{{14}{2019}{{Fan et~al.}}{{Fan, Gong, and Zhu}}}
\bibcite{friedman2010regularization}{{15}{2010}{{Friedman et~al.}}{{Friedman, Hastie, and Tibshirani}}}
\bibcite{ganti2017learning}{{16}{2017}{{Ganti et~al.}}{{Ganti, Rao, Balzano, Willett, and Nowak}}}
\bibcite{geenens2011curse}{{17}{2011}{{Geenens et~al.}}{{Geenens et~al.}}}
\bibcite{gibou2018review}{{18}{2018}{{Gibou et~al.}}{{Gibou, Fedkiw, and Osher}}}
\bibcite{goodfellow2016deep}{{19}{2016}{{Goodfellow et~al.}}{{Goodfellow, Bengio, Courville, and Bengio}}}
\bibcite{guha2020bayesian}{{20}{2020}{{Guha and Rodriguez}}{{Guha and Rodriguez}}}
\bibcite{guntuboyina2018nonparametric}{{21}{2018}{{Guntuboyina et~al.}}{{Guntuboyina, Sen, et~al.}}}
\bibcite{hamidi2019low}{{22}{2019}{{Hamidi and Bayati}}{{Hamidi and Bayati}}}
\bibcite{hao2019sparse}{{23}{2019}{{Hao et~al.}}{{Hao, Wang, Wang, Zhang, Yang, and Sun}}}
\bibcite{hardle1993optimal}{{24}{1993}{{Hardle et~al.}}{{Hardle, Hall, and Ichimura}}}
\bibcite{hastie2015statistical}{{25}{2015}{{Hastie et~al.}}{{Hastie, Tibshirani, and Wainwright}}}
\bibcite{hu2020matrix}{{26}{2020}{{Hu et~al.}}{{Hu, Shen, Zhou, and Kong}}}
\bibcite{kosorok2007introduction}{{27}{2007}{{Kosorok}}{{Kosorok}}}
\bibcite{moore2015development}{{28}{2015}{{Moore et~al.}}{{Moore, Scott, Reise, Port, Jackson, Ruparel, Savitt, Gur, and Gur}}}
\bibcite{murdoch2019definitions}{{29}{2019}{{Murdoch et~al.}}{{Murdoch, Singh, Kumbier, Abbasi-Asl, and Yu}}}
\bibcite{parikh2014proximal}{{30}{2014}{{Parikh and Boyd}}{{Parikh and Boyd}}}
\bibcite{relion2019network}{{31}{2019}{{Reli{\'o}n et~al.}}{{Reli{\'o}n, Kessler, Levina, and Taylor}}}
\bibcite{rigollet2009optimal}{{32}{2009}{{Rigollet et~al.}}{{Rigollet, Vert, et~al.}}}
\bibcite{scott2011surrogate}{{33}{2011}{{Scott}}{{Scott}}}
\bibcite{scott2007regression}{{34}{2007}{{Scott and Davenport}}{{Scott and Davenport}}}
\bibcite{shekhar2019multiscale}{{35}{2019}{{Shekhar and Javidi}}{{Shekhar and Javidi}}}
\bibcite{shen2006discussion}{{36}{2006}{{Shen and Wang}}{{Shen and Wang}}}
\bibcite{singh2009adaptive}{{37}{2009}{{Singh et~al.}}{{Singh, Scott, Nowak, et~al.}}}
\bibcite{tsybakov2008introduction}{{38}{2008}{{Tsybakov}}{{Tsybakov}}}
\bibcite{tsybakov1997nonparametric}{{39}{1997}{{Tsybakov et~al.}}{{Tsybakov et~al.}}}
\bibcite{tsybakov2004optimal}{{40}{2004}{{Tsybakov et~al.}}{{Tsybakov et~al.}}}
\bibcite{van2013wu}{{41}{2013}{{Van~Essen et~al.}}{{Van~Essen, Smith, Barch, Behrens, Yacoub, Ugurbil, Consortium, et~al.}}}
\bibcite{vapnik2013nature}{{42}{2013}{{Vapnik}}{{Vapnik}}}
\bibcite{varshney2010classification}{{43}{2010}{{Varshney and Willsky}}{{Varshney and Willsky}}}
\bibcite{wahba1990spline}{{44}{1990}{{Wahba}}{{Wahba}}}
\bibcite{wang2008probability}{{45}{2008}{{Wang et~al.}}{{Wang, Shen, and Liu}}}
\bibcite{wang2016classification}{{46}{2016}{{Wang et~al.}}{{Wang, Shen, Sun, and Qu}}}
\bibcite{wang2019common}{{47}{2019}{{Wang et~al.}}{{Wang, Zhang, and Dunson}}}
\bibcite{wang2017generalized}{{48}{2017}{{Wang et~al.}}{{Wang, Zhu, and Initiative}}}
\bibcite{wasserman2006all}{{49}{2006}{{Wasserman}}{{Wasserman}}}
\bibcite{willett2007minimax}{{50}{2007}{{Willett and Nowak}}{{Willett and Nowak}}}
\bibcite{xu2020class}{{51}{2020}{{Xu et~al.}}{{Xu, Dan, Khim, and Ravikumar}}}
\bibcite{yang2016rate}{{52}{2016}{{Yang et~al.}}{{Yang, Ma, and Buja}}}
\bibcite{zhang2018mapping}{{53}{2018}{{Zhang et~al.}}{{Zhang, Descoteaux, Zhang, Girard, Chamberland, Dunson, Srivastava, and Zhu}}}
\bibcite{zhou2014regularized}{{54}{2014}{{Zhou and Li}}{{Zhou and Li}}}
\bibcite{zhou2020broadcasted}{{55}{2020}{{Zhou et~al.}}{{Zhou, Wong, and He}}}
\@writefile{toc}{\contentsline {section}{Appendices}{32}{section*.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A}Connection to joint matrix decomposition with functional coefficients}{32}{subsection.7.1}}
\newlabel{sec:joint}{{A}{32}{Connection to joint matrix decomposition with functional coefficients}{subsection.7.1}{}}
\newlabel{ex:matrix_decomposition}{{5}{32}{Functionally decomposable matrices}{example.5}{}}
\newlabel{prop:px}{{2}{32}{Low-rank and sparse boundaries}{prop.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B}Supplementary figures}{34}{subsection.7.2}}
\newlabel{sec:sfigure}{{B}{34}{Supplementary figures}{subsection.7.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S1}{\ignorespaces Finite sample accuracy of matrix classification and regression. (a) simulation setup. (b) classification error with sample size when $d=30$. (c) classification error with matrix dimension when $n=200$. (d) regression error with sample size. The dash line in panels (b)-(d) represent theoretical rate $\mathcal  {O}(n^{-2/3})$, $\mathcal  {O}(\qopname  \relax o{log}d)$, and $\mathcal  {O}(n^{-1/3})$, respectively. \relax }}{34}{figure.caption.14}}
\newlabel{fig:linear}{{S1}{34}{Finite sample accuracy of matrix classification and regression. (a) simulation setup. (b) classification error with sample size when $d=30$. (c) classification error with matrix dimension when $n=200$. (d) regression error with sample size. The dash line in panels (b)-(d) represent theoretical rate $\tO (n^{-2/3})$, $\tO (\log d)$, and $\tO (n^{-1/3})$, respectively. \relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {S2}{\ignorespaces Example outputs returned by {\bf  \relax \fontsize  {10}{12}\selectfont  \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip NonMAR}. Panels (a) and (c) plot the top edges selected by our method. Panels (b) and (d) are scatter plots of the edge connectivity strength (averaged by subregion) versus the estimated response probability. The ground truth function is depicted in dashed curve.\relax }}{34}{figure.caption.15}}
\newlabel{fig:compare3}{{S2}{34}{Example outputs returned by {\bf \small NonMAR}. Panels (a) and (c) plot the top edges selected by our method. Panels (b) and (d) are scatter plots of the edge connectivity strength (averaged by subregion) versus the estimated response probability. The ground truth function is depicted in dashed curve.\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C}Proofs}{35}{subsection.7.3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.1}Proof of Proposition\nobreakspace  {}\ref  {thm:identifiability}}{35}{subsubsection.7.3.1}}
\newlabel{eq:excess}{{22}{35}{Proof of Proposition~\ref {thm:identifiability}}{equation.7.22}{}}
\MT@newlabel{eq:excess}
\newlabel{eq:tail2}{{23}{35}{Proof of Proposition~\ref {thm:identifiability}}{Item.4}{}}
\MT@newlabel{eq:tail2}
\MT@newlabel{eq:tail2}
\newlabel{eq:cmultiidentity}{{24}{35}{Proof of Proposition~\ref {thm:identifiability}}{Item.5}{}}
\MT@newlabel{eq:tail2}
\MT@newlabel{eq:cmultiidentity}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.2}Proof of Theorem\nobreakspace  {}\ref  {thm:twobounds}}{36}{subsubsection.7.3.2}}
\MT@newlabel{eq:regression}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.3}Proof of Proposition\nobreakspace  {}\ref  {prop:polynomial}}{37}{subsubsection.7.3.3}}
\newlabel{sec:population}{{C.3}{37}{Proof of Proposition~\ref {prop:polynomial}}{subsubsection.7.3.3}{}}
\newlabel{prop:polynomial}{{3}{37}{Polynomial continuity of inverse links}{prop.3}{}}
\newlabel{eq:concentration}{{3}{37}{Polynomial continuity of inverse links}{prop.3}{}}
\newlabel{eq:mainineq}{{25}{37}{Proof of Proposition~\ref {prop:polynomial}}{equation.7.25}{}}
\citation{wang2008probability}
\citation{wang2008probability}
\citation{wang2008probability}
\MT@newlabel{eq:mainineq}
\MT@newlabel{eq:mainineq}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.4}Proof of Theorem\nobreakspace  {}\ref  {thm:main}}{38}{subsubsection.7.3.4}}
\newlabel{eq:rate}{{26}{39}{Proof of Theorem~\ref {thm:main}}{equation.7.26}{}}
\MT@newlabel{eq:excess-hinge}
\MT@newlabel{eq:rate}
\newlabel{eq:equation}{{27}{39}{Proof of Theorem~\ref {thm:main}}{equation.7.27}{}}
\MT@newlabel{eq:equation}
\MT@newlabel{eq:equation}
\MT@newlabel{eq:equation}
\newlabel{eq:delta}{{28}{39}{Proof of Theorem~\ref {thm:main}}{equation.7.28}{}}
\MT@newlabel{eq:delta}
\MT@newlabel{eq:rate}
\newlabel{eq:tail}{{C.4}{39}{Proof of Theorem~\ref {thm:main}}{equation.7.28}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {C.5}Proofs of Theorem\nobreakspace  {}\ref  {thm:regression}}{39}{subsubsection.7.3.5}}
\newlabel{lem:main}{{1}{39}{Multiple level-sets estimation}{lem.1}{}}
\newlabel{eq:total}{{29}{39}{Multiple level-sets estimation}{equation.7.29}{}}
\newlabel{eq:conversion2}{{30}{40}{Proofs of Theorem~\ref {thm:regression}}{equation.7.30}{}}
\MT@newlabel{eq:conversion2}
\MT@newlabel{eq:total}
\newlabel{eq:twobounds}{{31}{40}{Proofs of Theorem~\ref {thm:regression}}{equation.7.31}{}}
\MT@newlabel{eq:twobounds}
\newlabel{lem:H}{{2}{41}{}{lem.2}{}}
\newlabel{prop:equivalance}{{4}{41}{}{prop.4}{}}
\newlabel{eq:conversion}{{1}{41}{}{Item.10}{}}
\MT@newlabel{eq:excess-hinge}
\newlabel{eq:levelreg}{{C.5}{43}{Proofs of Theorem~\ref {thm:regression}}{Item.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D}Auxiliary lemmas}{44}{subsection.7.4}}
\newlabel{lem:hingeL1}{{3}{44}{Hinge excess loss and $L$-1 distance}{lem.3}{}}
\newlabel{eq:function}{{32}{44}{Auxiliary lemmas}{equation.7.32}{}}
\MT@newlabel{eq:function}
\MT@newlabel{eq:function}
\MT@newlabel{eq:function}
\MT@newlabel{eq:function}
\newlabel{eq:integral}{{D}{45}{Auxiliary lemmas}{equation.7.32}{}}
\MT@newlabel{eq:prob2}
\newlabel{lem:product}{{4}{45}{Expectation of function products}{lem.4}{}}
\newlabel{eq:prob2}{{33}{46}{Expectation of function products}{equation.7.33}{}}
\MT@newlabel{eq:prob2}
\newlabel{eq:lowerbound}{{34}{46}{Auxiliary lemmas}{equation.7.34}{}}
\MT@newlabel{eq:prob2}
\MT@newlabel{eq:lowerbound}
\MT@newlabel{eq:lowerbound}
\newlabel{pro:inftynorm}{{3}{46}{Bracketing number}{defn.3}{}}
\citation{kosorok2007introduction}
\citation{candes2011tight}
\newlabel{lem:entropy}{{5}{47}{Bracketing number for bounded functions in $\tF (r,s_1,s_2)$}{lem.5}{}}
\MT@newlabel{eq:class}
\newlabel{lem:metric}{{6}{48}{Local complexity of $\tF (r,s_1,s_2)$}{lem.6}{}}
\newlabel{eq:specification}{{6}{48}{Local complexity of $\tF (r,s_1,s_2)$}{lem.6}{}}
\newlabel{eq:complexity}{{D}{48}{Auxiliary lemmas}{lem.6}{}}
\newlabel{eq:g}{{35}{48}{Auxiliary lemmas}{equation.7.35}{}}
\MT@newlabel{eq:g}
