\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{wang2019common}
\citation{zhou2014regularized}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction }{1}{section.1}}
\citation{friedman2010regularization}
\citation{fan2019generalized}
\citation{relion2019network}
\citation{hu2020matrix}
\@writefile{toc}{\contentsline {section}{\numberline {2}Three learning problems}{3}{section.2}}
\citation{scott2007regression,wang2008probability}
\newlabel{eq:classloss}{{1}{4}{Three learning problems}{equation.2.1}{}}
\MT@newlabel{eq:classloss}
\newlabel{eq:level}{{{{2.2}}}{4}{Three learning problems}{Item.2}{}}
\newlabel{eq:risklevel}{{2}{4}{Three learning problems}{equation.2.2}{}}
\MT@newlabel{eq:classloss}
\MT@newlabel{eq:risklevel}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) Our learning reduction approach (solid line) to the three problems of interest. The classical plug-in approaches are depicted in dashed line. (b) Matrix nonparametric regression via $\pi $-weighted classification.\relax }}{5}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:diagram}{{1}{5}{(a) Our learning reduction approach (solid line) to the three problems of interest. The classical plug-in approaches are depicted in dashed line. (b) Matrix nonparametric regression via $\pi $-weighted classification.\relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}From classification to regression: a new deal}{5}{section.3}}
\newlabel{sec:idea}{{3}{5}{From classification to regression: a new deal}{section.3}{}}
\citation{bartlett2006convexity}
\newlabel{eq:stepfunction}{{3}{6}{From classification to regression: a new deal}{equation.3.3}{}}
\newlabel{eq:large-margin}{{4}{6}{From classification to regression: a new deal}{equation.3.4}{}}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:risklevel}
\MT@newlabel{eq:large-margin}
\newlabel{eq:class}{{5}{6}{From classification to regression: a new deal}{equation.3.5}{}}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:class}
\citation{alquier2013sparse}
\citation{hu2020matrix}
\MT@newlabel{eq:class}
\newlabel{example:1}{{1}{7}{Single index models~\citep {alquier2013sparse}}{example.1}{}}
\MT@newlabel{eq:stepfunction}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:class}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces {\bf  Matrix classification and level-set estimation via ADMM} \relax }}{7}{algocf.1}}
\newlabel{alg:weighted}{{1}{7}{From classification to regression: a new deal}{algocf.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Statistical learning theories}{8}{section.4}}
\newlabel{sec:theory}{{4}{8}{Statistical learning theories}{section.4}{}}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:stepfunction}
\MT@newlabel{eq:risklevel}
\newlabel{ass:decboundary}{{1}{8}{Global regularity}{defn.1}{}}
\newlabel{eq:mass}{{6}{8}{Global regularity}{equation.4.6}{}}
\MT@newlabel{eq:mass}
\MT@newlabel{eq:mass}
\MT@newlabel{eq:large-margin}
\newlabel{ass:main}{{1}{8}{Approximation error}{assumption.1}{}}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:large-margin}
\newlabel{thm:main}{{4.1}{9}{Accuracy for matrix classification}{thm.4.1}{}}
\MT@newlabel{eq:large-margin}
\newlabel{eq:riskbound}{{7}{9}{Accuracy for matrix classification}{equation.4.7}{}}
\MT@newlabel{eq:riskbound}
\MT@newlabel{eq:riskbound}
\newlabel{thm:regression}{{4.2}{9}{Accuracy for nonparametric matrix regression}{thm.4.2}{}}
\MT@newlabel{eq:stepfunction}
\newlabel{eq:final}{{8}{10}{High-dimensional consistency}{equation.4.8}{}}
\MT@newlabel{eq:final}
\MT@newlabel{eq:riskbound}
\MT@newlabel{eq:large-margin}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical experiments}{10}{section.5}}
\newlabel{sec:data}{{5}{10}{Numerical experiments}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Finite sample accuracy under two settings. Panels (a) and (e) summarize the simulation setup, panels (b, d) and (f, h) assess the performance in matrix classification, and panels (c, g) assess the matrix regression. \relax }}{11}{figure.caption.3}}
\newlabel{fig:logistic}{{2}{11}{Finite sample accuracy under two settings. Panels (a) and (e) summarize the simulation setup, panels (b, d) and (f, h) assess the performance in matrix classification, and panels (c, g) assess the matrix regression. \relax }{figure.caption.3}{}}
\citation{friedman2010regularization}
\citation{relion2019network}
\citation{relion2019network}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Four active pattern in simulations. The active region is divided into four or five subregions (denoted I, II, ...), each of which has its own edge connectivity signal $g_{pq}(\pi )$.\relax }}{12}{figure.caption.4}}
\newlabel{fig:region}{{3}{12}{Four active pattern in simulations. The active region is divided into four or five subregions (denoted I, II, ...), each of which has its own edge connectivity signal $g_{pq}(\pi )$.\relax }{figure.caption.4}{}}
\citation{wang2019common}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Performance comparison between various methods under four different active patterns. \relax }}{13}{figure.caption.5}}
\newlabel{fig:compare}{{4}{13}{Performance comparison between various methods under four different active patterns. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Example outputs returned by {\bf  \relax \fontsize  {9}{11}\selectfont  \abovedisplayskip 8\p@ plus2\p@ minus4\p@ \abovedisplayshortskip \z@ plus\p@ \belowdisplayshortskip 4\p@ plus2\p@ minus2\p@ \def \leftmargin \leftmargini \topsep \z@ \parsep \parskip \itemsep \z@ {\leftmargin \leftmargini \topsep 4\p@ plus2\p@ minus2\p@ \parsep 2\p@ plus\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip NonMAR}. Panels (a) and (c) plot the top edges selected by our method. Panels (b) and (d) are scatter plots of the edge connectivity strength (averaged by subregion) versus the estimated response probability. The ground truth function is depicted in dashed curve.\relax }}{13}{figure.caption.6}}
\newlabel{fig:compare2}{{5}{13}{Example outputs returned by {\bf \footnotesize NonMAR}. Panels (a) and (c) plot the top edges selected by our method. Panels (b) and (d) are scatter plots of the edge connectivity strength (averaged by subregion) versus the estimated response probability. The ground truth function is depicted in dashed curve.\relax }{figure.caption.6}{}}
\citation{hastie2015statistical}
\citation{wang2019common}
\@writefile{toc}{\contentsline {section}{\numberline {6}Application to human brain connectome data}{14}{section.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces HCP analysis results. (a) Comparison of prediction accuracy. (b) Top edges selected by our method {\relax \fontsize  {8}{9.5}\selectfont  \bf  NonMAR-p}. (c) Top edges overlaid on brain template. (d) Edge connectivity strength versus estimated response probability. Colored curves represent the moving averages of connectivity strengths, gray bands represent one standard error, and jitter points represent the raw connectivity values (0 or 1). \relax }}{14}{figure.caption.7}}
\newlabel{fig:real}{{6}{14}{HCP analysis results. (a) Comparison of prediction accuracy. (b) Top edges selected by our method {\scriptsize \bf NonMAR-p}. (c) Top edges overlaid on brain template. (d) Edge connectivity strength versus estimated response probability. Colored curves represent the moving averages of connectivity strengths, gray bands represent one standard error, and jitter points represent the raw connectivity values (0 or 1). \relax }{figure.caption.7}{}}
\bibstyle{chicago}
\bibdata{tensor_wang}
\bibcite{alquier2013sparse}{{1}{2013}{{Alquier and Biau}}{{Alquier and Biau}}}
\bibcite{bartlett2006convexity}{{2}{2006}{{Bartlett et~al.}}{{Bartlett, Jordan, and McAuliffe}}}
\bibcite{fan2019generalized}{{3}{2019}{{Fan et~al.}}{{Fan, Gong, and Zhu}}}
\bibcite{friedman2010regularization}{{4}{2010}{{Friedman et~al.}}{{Friedman, Hastie, and Tibshirani}}}
\bibcite{hastie2015statistical}{{5}{2015}{{Hastie et~al.}}{{Hastie, Tibshirani, and Wainwright}}}
\bibcite{hu2020matrix}{{6}{2020}{{Hu et~al.}}{{Hu, Shen, Zhou, and Kong}}}
\bibcite{relion2019network}{{7}{2019}{{Reli{\'o}n et~al.}}{{Reli{\'o}n, Kessler, Levina, Taylor, et~al.}}}
\bibcite{scott2007regression}{{8}{2007}{{Scott and Davenport}}{{Scott and Davenport}}}
\bibcite{wang2008probability}{{9}{2008}{{Wang et~al.}}{{Wang, Shen, and Liu}}}
\bibcite{wang2019common}{{10}{2019}{{Wang et~al.}}{{Wang, Zhang, Dunson, et~al.}}}
\bibcite{zhou2014regularized}{{11}{2014}{{Zhou and Li}}{{Zhou and Li}}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{15}{section.7}}
