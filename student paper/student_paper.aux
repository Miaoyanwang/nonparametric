\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{zhou2014regularized}
\citation{Shashua2004Pedestrian}
\citation{wang2017bayesian}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction }{1}{section.1}\protected@file@percent }
\citation{friedman2010regularization}
\citation{agresti1998approximate}
\citation{relion2019network}
\citation{chollet2015keras}
\@writefile{toc}{\contentsline {section}{\numberline {2}Three learning problems}{3}{section.2}\protected@file@percent }
\citation{willett2007minimax,scott2007regression,wang2008probability}
\newlabel{eq:classloss}{{1}{4}{Three learning problems}{equation.2.1}{}}
\MT@newlabel{eq:classloss}
\MT@newlabel{eq:classloss}
\newlabel{eq:level}{{{{2.2}}}{4}{Three learning problems}{Item.2}{}}
\newlabel{eq:risklevel}{{2}{4}{Three learning problems}{equation.2.2}{}}
\MT@newlabel{eq:classloss}
\MT@newlabel{eq:risklevel}
\citation{gibou2018review}
\citation{gibou2018review}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) Our learning reduction approach (solid line) to three learning problems and classical plug-in approaches (dashed line). (b) Schematic diagram for nonparametric function estimation via level set estimation. Figure (b) is modified from\nobreakspace  {}\cite  {gibou2018review}. \relax }}{5}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:diagram}{{1}{5}{(a) Our learning reduction approach (solid line) to three learning problems and classical plug-in approaches (dashed line). (b) Schematic diagram for nonparametric function estimation via level set estimation. Figure (b) is modified from~\cite {gibou2018review}. \relax }{figure.caption.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Estimation}{5}{section.3}\protected@file@percent }
\newlabel{sec:idea}{{3}{5}{Estimation}{section.3}{}}
\citation{scott2011surrogate,bartlett2006convexity}
\citation{scott2011surrogate}
\newlabel{eq:stepfunction}{{3}{6}{Estimation}{section.3}{}}
\MT@newlabel{eq:large-margin}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Matrix nonparametric regression via weighted classification. We use a sequence of $\pi $-weighted classification to find level sets in the matrix space, and then approximate the target regression function.\relax }}{6}{figure.caption.2}\protected@file@percent }
\newlabel{fig:method}{{2}{6}{Matrix nonparametric regression via weighted classification. We use a sequence of $\pi $-weighted classification to find level sets in the matrix space, and then approximate the target regression function.\relax }{figure.caption.2}{}}
\newlabel{eq:large-margin}{{3}{6}{Estimation}{equation.3.3}{}}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:risklevel}
\MT@newlabel{eq:risklevel}
\MT@newlabel{eq:risklevel}
\newlabel{eq:class}{{4}{7}{Estimation}{equation.3.4}{}}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:class}
\@writefile{toc}{\contentsline {section}{\numberline {4}Statistical properties}{7}{section.4}\protected@file@percent }
\newlabel{sec:theory}{{4}{7}{Statistical properties}{section.4}{}}
\newlabel{ass:decboundary}{{1}{7}{Global regularity}{defn.1}{}}
\newlabel{ass:main}{{1}{8}{Approximation error}{assumption.1}{}}
\MT@newlabel{eq:large-margin}
\newlabel{thm:main}{{4.1}{8}{Accuracy for weighted classification}{thm.4.1}{}}
\MT@newlabel{eq:large-margin}
\newlabel{eq:riskbound}{{5}{9}{Accuracy for weighted classification}{equation.4.5}{}}
\MT@newlabel{eq:riskbound}
\MT@newlabel{eq:riskbound}
\newlabel{thm:regression}{{4.2}{9}{Accuracy for nonparametric matrix regression}{thm.4.2}{}}
\newlabel{eq:final}{{6}{10}{High-dimensional consistency}{equation.4.6}{}}
\MT@newlabel{eq:final}
\MT@newlabel{eq:riskbound}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces {\bf  Matrix classification and level-set estimation (ADMM)} \relax }}{10}{algocf.1}\protected@file@percent }
\newlabel{alg:weighted}{{1}{10}{Statistical properties}{algocf.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Numerical experiments}{10}{section.5}\protected@file@percent }
\newlabel{sec:data}{{5}{10}{Numerical experiments}{section.5}{}}
\MT@newlabel{eq:large-margin}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Impacts of sample size, matrix dimension, and model complexity}{11}{subsection.5.1}\protected@file@percent }
\newlabel{sec:validation}{{5.1}{11}{Impacts of sample size, matrix dimension, and model complexity}{subsection.5.1}{}}
\MT@newlabel{eq:large-margin}
\citation{friedman2010regularization}
\citation{relion2019network}
\citation{chollet2015keras}
\citation{relion2019network}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Performance for matrix classification and regression. (a) Relationship between the nonlinear predictor $\mathaccentV {bar}016z$ and the Bernoulli probability $p(\bm  {X})$. The distribution for each is plotted on the top and left side, respectively. (b) classification error with sample size. (c) classification error with matrix dimension. (d) regression error with sample size. The dash line in panels (b)-(d) represent theoretical rates $\mathcal  {O}(n^{-2/3})$, $\mathcal  {O}(\qopname  \relax o{log}d)$, and $\mathcal  {O}(n^{-1/2})$, respectively.\relax }}{12}{figure.caption.4}\protected@file@percent }
\newlabel{fig:logistic}{{3}{12}{Performance for matrix classification and regression. (a) Relationship between the nonlinear predictor $\bar z$ and the Bernoulli probability $p(\mX )$. The distribution for each is plotted on the top and left side, respectively. (b) classification error with sample size. (c) classification error with matrix dimension. (d) regression error with sample size. The dash line in panels (b)-(d) represent theoretical rates $\tO (n^{-2/3})$, $\tO (\log d)$, and $\tO (n^{-1/2})$, respectively.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Comparison with other methods}{12}{subsection.5.2}\protected@file@percent }
\newlabel{sec:comparison}{{5.2}{12}{Comparison with other methods}{subsection.5.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Rank of support matrices: 3 (cross), 7 (block), almost full-rank (star), almost full-rank (circle)\relax }}{12}{figure.caption.5}\protected@file@percent }
\newlabel{fig:region}{{4}{12}{Rank of support matrices: 3 (cross), 7 (block), almost full-rank (star), almost full-rank (circle)\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Comparison of prediction errors between various methods. (a)-(d) represent four different activation patterns. \relax }}{13}{figure.caption.6}\protected@file@percent }
\newlabel{fig:compare}{{5}{13}{Comparison of prediction errors between various methods. (a)-(d) represent four different activation patterns. \relax }{figure.caption.6}{}}
\bibstyle{chicago}
\bibdata{tensor_wang}
\bibcite{agresti1998approximate}{{1}{1998}{{Agresti and Coull}}{{Agresti and Coull}}}
\bibcite{bartlett2006convexity}{{2}{2006}{{Bartlett et~al.}}{{Bartlett, Jordan, and McAuliffe}}}
\bibcite{chollet2015keras}{{3}{2015}{{Chollet et~al.}}{{Chollet et~al.}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Rank of support matrices: 3 (cross), 7 (block), almost full-rank (star), almost full-rank (circle)\relax }}{14}{figure.caption.7}\protected@file@percent }
\newlabel{fig:compare2}{{6}{14}{Rank of support matrices: 3 (cross), 7 (block), almost full-rank (star), almost full-rank (circle)\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{14}{section.6}\protected@file@percent }
\bibcite{friedman2010regularization}{{4}{2010}{{Friedman et~al.}}{{Friedman, Hastie, and Tibshirani}}}
\bibcite{gibou2018review}{{5}{2018}{{Gibou et~al.}}{{Gibou, Fedkiw, and Osher}}}
\bibcite{relion2019network}{{6}{2019}{{Reli{\'o}n et~al.}}{{Reli{\'o}n, Kessler, Levina, Taylor, et~al.}}}
\bibcite{scott2011surrogate}{{7}{2011}{{Scott}}{{Scott}}}
\bibcite{scott2007regression}{{8}{2007}{{Scott and Davenport}}{{Scott and Davenport}}}
\bibcite{Shashua2004Pedestrian}{{9}{2004}{{Shashua et~al.}}{{Shashua, Gdalyahu, and Hayun}}}
\bibcite{wang2008probability}{{10}{2008}{{Wang et~al.}}{{Wang, Shen, and Liu}}}
\bibcite{wang2017bayesian}{{11}{2017}{{Wang et~al.}}{{Wang, Durante, Jung, and Dunson}}}
\bibcite{willett2007minimax}{{12}{2007}{{Willett and Nowak}}{{Willett and Nowak}}}
\bibcite{zhou2014regularized}{{13}{2014}{{Zhou and Li}}{{Zhou and Li}}}
