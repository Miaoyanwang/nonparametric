\documentclass[11pt]{article}
\newcommand{\blind}{1}

\usepackage[nodisplayskipstretch]{setspace}
\setstretch{1}

\usepackage[utf8]{inputenc}
\usepackage{lipsum}

\usepackage{amssymb,amsbsy,amsfonts,amsmath,xspace,amsthm}
\usepackage{mathrsfs}
\usepackage{graphicx}

\allowdisplaybreaks

\usepackage{caption}
\usepackage{setspace}
\usepackage{comment}
%\doublespacing
\usepackage[margin=1in]{geometry}
%\usepackage{enumitem} 
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{epstopdf}
\usepackage{epsfig}
\usepackage[colorlinks,citecolor=blue]{hyperref}

\usepackage[labelfont=bf]{caption}
\usepackage{url}
\usepackage[toc,page]{appendix}
\usepackage{float}
\usepackage{natbib}
\usepackage{mathrsfs}
\usepackage{color}
\usepackage[dvipsnames]{xcolor}
\usepackage{verbatim}
\usepackage{authblk}
\usepackage[normalem]{ulem}

\def\ack{\section*{Acknowledgements}%
  \addtocontents{toc}{\protect\vspace{6pt}}%
  \addcontentsline{toc}{section}{Acknowledgements}%
}


\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    citecolor=blue
}
 
\newcommand*{\KeepStyleUnderBrace}[1]{%f
\mathop{%
\mathchoice
{\underbrace{\displaystyle#1}}%
{\underbrace{\textstyle#1}}%
{\underbrace{\scriptstyle#1}}%
{\underbrace{\scriptscriptstyle#1}}%
}\limits
}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{amsmath,amssymb,amsthm,bm}
\usepackage{dsfont,listings}

 \renewcommand\footnotemark{}

\usepackage[ruled,vlined]{algorithm2e}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}


\usepackage{enumitem}
\theoremstyle{plain} 
\newtheorem{thm}{Theorem}[subsection]
\newtheorem{lem}{Lemma}

\theoremstyle{definition}
\newtheorem{prop}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}

\def\caliF{\tF_{\textup{sgn}}}
\def\caliM{\tM_{\textup{sgn}}}

\usepackage{xcolor}
\allowdisplaybreaks
\input macros.tex

\setcounter{secnumdepth}{3}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}
\def\fixme#1#2{\textbf{\color{red}[FIXME (#1): #2]}}
\def\mycomment#1{\textbf{\color{blue}#1}}
\def\ccomment#1{\textbf{\color{ForestGreen}#1}}
\usepackage[parfill]{parskip}
\usepackage{bm}


\newcommand{\Hnorm}[1]{\left\lVert#1\right\rVert_{\tH_\alpha}}
\newcommand{\nullnorm}[1]{\left\lVert#1\right\rVert}
\def\trueB{\mB^{\text{true}}}
\def\newX{\mX_{\textup{new}}}
\def\newy{y_{\textup{new}}}
\def\sign{\textup{sgn}}
\def\bayesS{S_{\textup{bayes}}}
\def\bayespif{f_{\textup{bayes},\pi}}
\def\CNN{\text{\bf \small CNN }}
\def\Lasso{\text{\bf \small Lasso }}
\def\NonparaM{\text{\bf \small NonMAR }}
\def\LogisticM{\text{\bf \small LogisticM }}

\def\srank{\textup{srank}}
\def\rank{\textup{rank}}

\def\risk{\textup{Risk}_\pi}
 \def\caliF{\tF_{\textup{sgn}}}
\def\caliM{\tM_{\textup{sgn}}}

\def\shift{\bar Y_\pi}

\def\riskF{\textup{Risk}_{\pi,F}}
\def\eriskF{\widehat{\textup{Risk}}_{\pi,F}}


\usepackage{setspace}
\onehalfspacing
 
 
\usepackage{xr}
\externaldocument{trace-2021-04-20}



\addcontentsline{toc}{section}{Appendices}
\renewcommand{\thesection}{\Alph{section}}
\setcounter{figure}{0}
\setcounter{table}{0}
\renewcommand{\thetable}{S\arabic{table}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\figurename}{Supplemental Figure}
\renewcommand{\tablename}{Supplemental Table}



\begin{document}


%\section*{Appendix}

\tableofcontents


\clearpage
\section{Additional theoretical results}\label{sec:additional}
\subsection{Sign rank and matrix rank}\label{sec:signrank}

In the main paper, we have provided several examples with high matrix rank but low sign rank. This section provides more examples and their proofs. 

\begin{example}[Max graphon]\label{example:max} Suppose the matrix $\mTheta\in\mathbb{R}^{d\times d}$ takes the form 
\[
\mTheta(i,j)=\log\left(1+{1\over d}\max(i,j)\right), \ \text{for all }(i,j)\in[d]^2.
\]
 Then 
 \[
 \rank(\mTheta)=d, \quad \text{and}\quad \srank(\mTheta-\pi)\leq 2\ \text{for all }\pi\in\mathbb{R}. 
 \]
\end{example}
\begin{proof}
The full-rankness of $\mTheta$ is verified from elementary row operations as follows
\begin{align}
\begin{pmatrix}
(\mTheta_2-\mTheta_1)/(\log(1+\frac{2}{d})-\log(1+\frac{1}{d}))\\(\mTheta_3-\mTheta_2)/(\log(1+\frac{3}{d})-\log(1+\frac{2}{d}))\\\vdots\\ (\mTheta_d-\mTheta_{d-1})/(\log(1+\frac{d}{d})-\log(1+\frac{d-1}{d}))\\\mTheta_d/\log(1+\frac{d}{d})
\end{pmatrix} = \begin{pmatrix}
 1&          0  &      \ddots  &        \ddots       &          0 \\
1& 1 & \ddots &            \ddots   &   \ddots          \\
      \vdots &     \vdots & \ddots &       \ddots &    \ddots         \\
 1 & 1 &1 & 1 &0\\
 1 & 1 &1 & 1 &1
\end{pmatrix},
\end{align}
where $\mTheta_i$ denotes the $i$-th row of $\mTheta$. 
Now it suffices to show $\srank(\mTheta-\pi)\leq 2$ for $\pi$ in the feasible range $(\log(1+{1\over d}),\ \log 2)$. In this case, there exists an index $i^*\in\{2,\ldots,d\}$, such that $\log(1+{i^*-1\over d})< \pi\leq \log(1+{i^*\over d})$. By definition, the sign matrix $\sign (\mTheta-\pi)$ takes the form
\begin{equation}\label{eq:matrix}
\sign (\mTheta(i,j)-\pi)=
\begin{cases}
-1, & \text{both $i$ and $j$ are smaller than $i^*$};\\
1, & \text{otherwise}.
\end{cases}
\end{equation}
Therefore, the matrix $\sign (\mTheta-\pi)$ is a rank-2 block matrix, which implies $\srank(\mTheta-\pi)=2$. 
\end{proof}


In fact, Example~\ref{example:max} is a special case of the following proposition. 

\begin{prop}[Min/Max graphon] Let $g\colon \mathbb{R}\to \mathbb{R}$ be a continuous function such that $g(z)=0$ has at most $r\geq 1$ distinct real roots. For given numbers $x_i, y_j\in[0,1]$ all $(i,j)\in[d]^2$, define a matrix $\mTheta \in\mathbb{R}^{d\times d}$ with entries
\begin{equation}\label{eq:max}
\mTheta(i,j)=g(\max(x_i,y_j)), \quad (i,j)\in[d]^2.
\end{equation}
Then, the sign rank of $\mTheta$ satisfies
\[
\srank(\mTheta)\leq 2r.
\]
The same conclusion holds if we use $\min$ in place of $\max$ in~\eqref{eq:max}. 
\end{prop}



\begin{proof} 
Without loss of generality, assume $x_1\leq \cdots\leq x_d$ and $y_1\leq \ldots \leq y_d$. Based on the construction of $\mTheta$, the reordering does not change the rank of $\mTheta$. Let $z_1<\cdots<z_r$ be the $r$ distinct real roots for the equation $g(z)=0$. We separate the proof for two cases, $r=1$ and $r\geq 2$. 

\begin{itemize}[leftmargin=*,topsep=0pt,itemsep=-1ex,partopsep=1ex,parsep=1ex]
\item When $r=1$. The continuity of $g(\cdot)$ implies that the function $g(z)$ has at most one sign change point. Based on the similar argument as in Example~\ref{example:max}, the matrix $\sign(\mTheta)$ is a rank-2 block matrix; i.e., 
\begin{align}
\sign(\mTheta)=1-2\ma\otimes \mb \quad \text{ or } \quad \sign(\mTheta) = 2\ma\otimes\mb -1,
\end{align}
where $\ma, \mb$ are binary vectors defined by
\[
\ma=(\KeepStyleUnderBrace{1,\ldots,1,}_{\text{positions for which $x_{i}<z_1$}}0,\ldots,0)^T,\quad\mb=(\KeepStyleUnderBrace{1,\ldots,1,}_{\text{positions for which $y_j<z_1$}}0,\ldots,0)^T
\]
Therefore, $\srank(\mTheta)\leq \rank(\sign(\mTheta)) = 2$. 

\item When $r\geq 2$. By continuity, the function $g(z)$ is non-zero and remains an unchanged sign in each of the intervals $(z_s, z_{s+1})$ for $1\leq s\leq r-1$. Define the index set 
\[
\tI=\{s\in\mathbb{N}_{+}\colon \text{the interval $(z_s, z_{s+1})$ in which $g(z)<0$}\}.
\] 
We now prove that the sign matrix $\sign(\mTheta)$ has rank bounded by $2r-1$. To see this, consider the matrix indices for which $\sign(\mTheta)=-1$,
\begin{align}\label{eq:support}
\{(i,j)\colon \mTheta(i,j) <0 \} & = \{(i,j) \colon g(\max(x_i,y_j))<0\} \notag \\
&=\cup_{s\in \tI} \{(i,j)\colon \max(x_i,y_j)\in(z_s,z_{s+1})\}\notag\\
&=\cup_{s\in \tI}\Big( \{(i,j)\colon x_{i}< z_{s+1}, y_j<z_{s+1}\} \cap \{(i,j)\colon x_{i}\leq z_{s}, y_j\leq z_{s+1}\}^c\Big).
\end{align}
The equation~\eqref{eq:support} is equivalent to 
\begin{align}\label{eq:indicator}
\mathds{1}(\mTheta(i,j)< 0)&
=\sum_{s\in \tI}\left(  \mathds{1}(x_{i}< z_{s+1}) \mathds{1}(y_{j}< z_{s+1})- \mathds{1}(x_{i}\leq z_{s})\mathds{1}(y_{j}\leq z_{s})\right),
\end{align}
for all $(i,j)\in[d]^2$, where $\mathds{1}(\cdot)\in\{0,1\}$ denotes the indicator function. The equation~\eqref{eq:indicator} implies the low-rank representation of $\sign(\mTheta)$,
\begin{equation}\label{eq:sum}
\sign(\mTheta)=1-2\sum_{s\in \tI } \left(\ma_{s+1}\otimes \mb_{s+1} - \bar \ma_s\otimes \bar \mb_s\right),
\end{equation}
where $\ma_{s+1}, \bar \ma_{s}$ are binary vectors defined
\[
\ma_{s+1}=(\KeepStyleUnderBrace{1,\ldots,1,}_{\text{positions for which $x_{i}<z_{s+1}$}}0,\ldots,0)^T,\quad \text{and}\quad
\bar \ma_s=(\KeepStyleUnderBrace{1,\ldots,1,}_{\text{positions for which $x_{i}\leq z_{s}$}}0,\ldots,0)^T,
\]
and $\mb_{s+1}, \bar \mb_{s}$ are binary vectors defined similarly by using $y_j$ in place of $x_i$. 
Therefore, by~\eqref{eq:sum} and the assumption $|\tI|\leq r-1$, we conclude that 
\[
\srank(\mTheta)\leq 1+2(r-1)=2r-1.
\]
\end{itemize}
Combining two cases yields that $\srank(\mTheta)\leq 2r$ for any $r\geq 1$.
\end{proof}


\begin{example}[Banded matrices]\label{example:banded} Let $\ma=(1,2,\ldots,d)^T$ be a $d$-dimensional vector, and define a $d$-by-$d$ banded matrix $\mM=|\ma\otimes \mathbf{1}-\mathbf{1}\otimes \ma|$. Then
\[
\rank(\mM)=d,\quad \text{and}\quad \srank(\mM-\pi)\leq 3, \quad \text{for all }\pi\in \mathbb{R}.
\]
\end{example}
\begin{proof}
Note that $\mM$ is a banded matrix with entries
\[
\mM(i,j)={|i-j|}, \quad \text{for all }(i,j)\in[d]^2.
\]
Elementary row operation directly shows that $\mM$ is full rank as follows,
\begin{align}
\begin{pmatrix}
(\mM_1+\mM_d)/(d-1)\\
\mM_1-\mM_2\\
\mM_2-\mM_3\\
\vdots\\
\mM_{d-1}-\mM_{d}
\end{pmatrix} = 
\begin{pmatrix}
1&1&1&\cdots&1&1\\
-1&1&1&\cdots&1&1\\
-1&-1&1&\cdots&1&1\\
\vdots & \vdots & \vdots & \vdots & \vdots & \vdots\\
-1&-1&-1&\cdots&-1&1
\end{pmatrix}.
\end{align}

We now show $\srank(\mM-\pi)\leq 3$ by construction. Define two vectors $\mb=(2^{-1},2^{-2},\ldots,2^{-d})^T\in\mathbb{R}^d$ and $\text{rev}(\mb)=(2^{-d},\ldots,2^{-1})^T\in\mathbb{R}^d$. We construct the following matrix
\begin{equation}\label{eq:A}
\mA=\mb\otimes\text{rev}(\mb)+\text{rev}(\mb)\otimes\mb.
\end{equation}
The matrix $\mA\in\mathbb{R}^{d\times d}$ is banded with entries
\[
\mA(i,j)=\mA(j,i)=\mA(d-i,d-j)=\mA(d-j,d-i)=2^{-d-1}\left(2^{j-i}+2^{i-j}\right),\ \text{for all }(i,j)\in[d]^2.
\] 
Furthermore, the entry value $\mA(i,j)$ decreases with respect to $|i-j|$; i.e., 
\begin{equation}\label{eq:decrease}
\mA(i,j) \geq \mA(i',j'), \quad \text{for all }|i-j|\geq |i'-j'|.
\end{equation}
Notice that for a given $\pi\in\mathbb{R}$, there exists $\pi'\in\mathbb{R}$ such that $\sign(\mA-\pi')=\sign(\mM-\pi)$. This is because both $\mA$ and $\mM$ are banded matrices satisfying monotonicity~\eqref{eq:decrease}. By definition~\eqref{eq:A}, $\mA$ is a rank-2 matrix. Henceforce, $\srank(\mM-\pi)=\srank(\mA-\pi')\leq 3.$
\end{proof}

\begin{example}[Identity matrices]
Let $\mI$ be a $d$-by-$d$ identity matrix. Then
\[
\rank(\mI)=d,\quad\text{and}\quad  \srank(\mI-\pi)\leq 3 \ \text{for all }\pi\in\mathbb{R}.
\]
\end{example}
\begin{proof}
Depending on the value of $\pi$, the sign matrix $\sign(\mI-\pi)$ falls into one of the two cases: 
\begin{enumerate}
\item $\sign(\mI-\pi)$ is a matrix of all $1$, or of all $-1$; 
\item $\sign(\mI-\pi)=2\mI-\mathbf{1}_d\otimes \mathbf{1}_d$.
\end{enumerate}
The former case is trivial, so it suffices to show $\srank(\mI-\pi)\leq 3$ in the second case. Based on Example~\ref{example:banded}, the rank-2 matrix $\mA$ in~\eqref{eq:A} satisfies 
\[
\mA(i,j)
\begin{cases}
=2^{-d}, & i=j,\\
\geq 2^{-d}+2^{-d-2}, & i\neq j.
\end{cases}
\]
Therefore, $\sign\left(2^{-d}+2^{-d-3}-\mA\right)=2\mI-\mathbf{1}_d\otimes \mathbf{1}_d$. We conclude that $\srank(\mI-\pi)\leq \rank(2^{-d}+2^{-d-3}-\mA)=3$. 
\end{proof}


\subsection{Adjusting for additional covariates}
Our method allows a mixture of matrix-valued predictors and usual vector-valued predictors, i.e., classifiers of the type $f(\mX)=\langle \mX, \mB\rangle+\mW^T\mC$, where $\mX\in\mathbb{R}^{d_1\times d_2}$ represents the matrix-valued predictor of our interest, and $\mW\in\mathbb{R}^{p}$ represents the additional covariate including intercept. In our neuroimaging analysis (see Section~\ref{sec:brain} of the main paper), for example, we have used $\mW$ to capture covariate such as age, gender, etc in the prediction model. Our algorithm can be easily extended to allow additional covariates in the model. Specifically, we allow classifiers of the type $f(\mX,\mW)=\langle \mX,\mB\rangle+\mW^T\mC$, where $\mW\in\mathbb{R}^p$ is the usual vector-valued covariate, and $\mC\in\mathbb{R}^p$ is the unconstrained coefficient. The only change is the primal update in the algorithm (Line 4 in Algorithm~\ref{alg:weighted} of main paper). The decision variables now consist of $(\mB,\mC)$ and we solve them simultaneously. Because both $\mB$ and $\mC$ are unconstrained decision variables, the algorithm lends itself well to this context. 


\subsection{Extension to sub-Gaussian noise}\label{sec:sub-Gaussian}
In the main paper, we have assumed the bounded noise (and thus bounded response) in the regression model. Here we extend the results to unbounded response with sub-Gaussian noise. For notational simplicity, we state the results for the matrix completion problem with $d_1=d_2=d$. The results extend similarly to general nonparamatrix matrix regression; we omit the elaboration but only state the difference in the remark. 

Consider the signal plus noise model on matrix $\mY\in\mathbb{R}^{d\times d}$,
\begin{align*}
\mY = \mTheta+\mE,
\end{align*}
where $\mE$ consists of zero-mean, independent noise entries, and $\mTheta\in\caliM(r)$ is an $\alpha$-smooth matrix.
Theoretical results in Section~\ref{sec:examples} of the main paper are based on bounded observation $\|\mY\|_\infty\leq 1$. Here, we extend the results to unbounded observation with the following assumption.

\begin{assumption}[Sub-Gaussian noise]\label{assm:subg}\hfill

\begin{enumerate}
\item There exists a constant $\beta>0$, independent of matrix dimension, such that $\|\mTheta\|_\infty\leq \beta$. Without loss of generality, we set $\beta = 1$.
\item The noise entries $\mE(\omega)$ are independent zero-mean sub-Gaussian random variables with variance proxy $\sigma^2>0$; i.e, $\mathbb{P}(|\mE(\omega)|\geq B)\leq 2e^{-B^2/2\sigma^2}$ for all $B>0$.  
\end{enumerate}
\end{assumption}

We say that an event $E$ occurs ``with high probability'' if $\mathbb{P}(E)$ tends to 1 as the dimension $d\to \infty$. The following result show that the sub-Gaussian noise incurs an additional $\log d$ factor compared to the bounded case. 

\begin{thm}[Extension of Theorem~\ref{thm:estimation} to sub-Gaussian noise]\label{thm:extension_gaussian} Consider the same conditions of Theorem~\ref{thm:estimation}. Under Assumption~\ref{assm:subg}, with high probability over training data $\mY_{\Omega}$, we have
\begin{enumerate}
\item [(a)](Sign matrix estimation). For all $\pi\in [-1,1]$ except for a finite number of levels,
\begin{equation}\label{eq:matrix_sign}
 \textup{MAE}(\sign \hat Z_\pi,\sign (\mTheta-\pi))\lesssim \left({r \sigma^2d\log d \over |\Omega|}\right)^{\alpha+1\over \alpha+2}+{1\over \rho(\pi,\tN)}\left({r \sigma^2d\log d \over |\Omega|}\right).
\end{equation}
\item [(b)](Signal matrix estimation) Set $H\asymp \left({ |\Omega|\over r \sigma^2d}\right)^{1/2}$. We have
\[
 \textup{MAE}(\hat \mTheta,\mTheta)\lesssim \tilde \tO\left\{\left({r \sigma^2d\log d\over |\Omega|}\right)^{\min({\alpha\over\alpha+2},\frac{1}{2})}\right\}.
\]
\end{enumerate}
\end{thm}
The proof is provided in Section~\ref{sec:sub-Gaussian}.  

\begin{rmk}[Extending to general non-parametric matrix regression] For matrix nonparametrix regression (Theorem~\ref{thm:regression} of the main paper), the extension of bounded noise to sub-Gaussian noise incurs an additional $\log n$ factor, where $n$ is the sample size. The techniques of handling sub-Gaussian noise is identical to the above extension, and is thus omitted in the paper. 
\end{rmk}

\subsection{Extension to unbounded number of mass points}\label{eq:unbounded}
Theorem~\ref{thm:estimation} of our main paper assumes the bounded $|\tN|_{\text{cover}}<c<\infty$ for some constant $c>0$, where $|\tN|_{\text{cover}}$ is defined as the covering number of $\tN$ with $2\Delta s$-bin's. Recall that $\tN$ corresponds to regions of jumps greater than $\Delta s = {1/d^2}$ in the CDF $G(\pi)=\mathbb{P}_{\omega\sim \Pi}(\Theta(\omega)\leq \pi)$. This setup gives a cleaner exposition of our results but may be restricted in some cases. For example, the high-rank matrices in Example~\ref{ex:high-rank} and Figure~\ref{fig:limit}(b) are excluded, because $\alpha=\infty$ and $|\tN|_{\text{cover}}=d$ in this setup. Fortunately, our framework still applies to this family of matrices with a little amendment. 

We now extend the setup to allow for more general structured matrices including those in Example~\ref{ex:high-rank}. Redefine $\Delta s = {1/d}$. Correspondingly, redefine the smoothness index $\alpha$ and the set $\tN$ for the psudo density of $\mTheta(\omega)$ with new bin width $2\Delta s$. Let $|\tN|_{\text{cover}}$ be the covering number of $\tN$ with new $2\Delta s$-bin's. Under this new setup, the signal matrix in Example~\ref{ex:high-rank} has $|\tN|_{\text{cover}}=0$ and $\alpha<\infty$. Following the same line as in Theorem~\ref{thm:estimation} and use the fact that $\Delta s\lesssim t_n$, we obtain that
\[
\textup{MAE}(\hat \mTheta, \mTheta)\lesssim t^{\alpha/(\alpha+2)}_d\log H +{1\over H}+t_dH\log H,\quad \text{with }t_d={d r\over |\Omega|}.
\]
Therefore, setting $H\asymp \sqrt{1+|\tN|_{\text{cover}}\over t_d}$ yields the error bound
\begin{equation}\label{eq:inf}
\textup{MAE}(\hat \mTheta, \mTheta)\leq \tilde\tO\left\{\left( {dr\over |\Omega|}\right)^{\min({\alpha\over 2+\alpha},\ {1\over 2})}\right\}.
\end{equation}
The result~\eqref{eq:inf} applies to cases when the signal matrices belong to $\caliM(r)$ and have at most $d$ distinct entries with repetition patterns. 


\subsection{Connection to structured matrix model with functional coefficients}\label{sec:joint}
In Section~\ref{sec:comparison} of the main paper, we simulate data $(\mX_i,Y_i)_{i=1}^n$ from latent variable model $(\mX,Y)|\pi$ based on the following scheme,
\[
\pi \stackrel{\text{i.i.d.}}{\sim} \text{Unif}[0,1] \stackrel{\text{conditional on $\pi$}}{\longrightarrow}
\begin{cases}
Y\sim \text{Ber}(\pi),\ Y\perp \mX|\pi, \\
\mX=\entry{\mX_{pq}}, \ \text{where\ } \mX_{pq}\stackrel{\text{indep.}}{\sim} \tN(g_{pq}(\pi)\mathds{1}(\text{edge $(p,q)$ is active}), \sigma^2).
\end{cases}
\]
Notice that, for any given $\pi$, $\mX$ is a rank-$r$, $(s_1,s_2)$ matrix as shown in Fig~\ref{fig:region} of the main paper. 

Here we provide justification to this simulation. We will show that the, in the absence of noise $\sigma=0$, the conditional expectation $\mathbb{E}(Y|\mX)=f(\mX)$ from the above simulation falls into the low-rank sign-representable function family of our interest. 


Specifically, we consider a structured matrix model with functional coefficients
\begin{equation}\label{eq:scheme}
\mX_\pi\stackrel{\text{def}}{=}\mB_0+ \sum_{s=1}^rg_s(\pi)\mB_s+\sigma \mE,\quad Y_\pi\sim\text{Ber}(\pi),\quad \mX_\pi \perp Y_\pi |\pi,
\end{equation}
where $\pi \in[0,1]$ is drawn from $\text{Unif}[0,1]$; $\mE$ is a noise matrix consisting of i.i.d.\ entries in $N(0,1)$; $\sigma$ is the noise level; $\mB_0$ is an arbitrary baseline matrix; $(\mB_s)_{s=1}^r$ is a set of rank-1 matrices in $\{0,1\}^{d_1\times d_2}$ that satisfy three conditions:
\begin{enumerate}
\item non-overlapping supports, i.e., $\langle \mB_s, \mB_{s'}\rangle=0$ for all $s\neq s'$
\item bounded total support, i.e., $\sum_{s\in[r]}\text{supp}(\mB_s)\leq (s_1,s_2)$;
\item At least one of the functions $(g_s)_{s=1}^r$ is strictly monotonic with respect to $\pi$ for all $s\in[r]$. \\
\end{enumerate}

\begin{prop}[Connection to structured matrix model with functional coefficients] Let $\mathbb{P}_{\mX,Y}$ denote the joint distribution induced by $(\mX_\pi,Y_\pi)_{\pi\in[0,1]}$ drawn from from~\eqref{eq:scheme}. In the noiseless case $\sigma = 0$, let $f(\mX)=\mathbb{E}(Y|\mX)$ denote the regression function based on $\mathbb{P}_{\mX,Y}$. Then $f\in\caliF(r,s_1,s_2)$.
\end{prop}

\begin{proof}
We restrict ourselves to the noiseless case with $\sigma=0$ in~\eqref{eq:scheme}. 
Let 
\[
\tX=\{\mX_\pi\colon \mX_\pi \text{ has structure specified in~\eqref{eq:scheme} for $\pi\in[0,1]$}\}
\]
denote the predictor space. The mapping between $\pi$ and $\mX\in \tX$ is one-to-one based on the construction of $\mX_\pi$. We use $\Pi\colon [0,1]\to \tX$ to denote the mapping and $\Pi^{-1}$ the inverse. Based on the property 3, without loss of generality, assume $g_1$ is a strictly increasing function. 


For any given $\pi\in[0,1]$, we have
\[
\mathbb{E}_{Y|\pi}[Y|\pi]=\pi=\Pi^{-1}(\mX).
\]
This implies the regression function $f=\Pi^{-1}$. To show $f\in \caliF(r,s_1,s_2)$, it suffices to show $\Pi^{-1}\in\caliF(r,s_1,s_2)$. For any given $\pi'\in[0,1]$, write 
\begin{align}
\{\mX\in \tX \colon \sign(\Pi^{-1}-\pi')=1\}&=\{\mX\in \tX \colon \Pi^{-1}(\mX)\geq \pi'\}\\
&=\{\mX\in \tX \colon g_1(\Pi^{-1}(\mX))\geq g_1(\pi')\}\\
&=\left\{\mX\in \tX \colon \langle \mX, \mB_1\rangle \geq g_1(\pi')\langle \mB_1,\mB_1\rangle + \langle \mB_0, \mB_1 \rangle \right\},
\end{align}
where the second line uses the fact that $g_1$ is strictly increasing. 

Therefore, the sign function $\sign(\Pi^{-1}-\pi')$ can be expressed as the sign of trace function,
\[
\sign(\Pi^{-1}-\pi')=\sign(\KeepStyleUnderBrace{\langle \mX,\mB_1 \rangle}_{\text{trace}} - \KeepStyleUnderBrace{g_1(\pi')\langle \mB_1,\mB_1\rangle-\langle \mB_0,\mB_1\rangle}_{\text{intercept}}),\quad \text{for all }\mX\in\tX,
\]
where $\mB_1$ is a rank-1, supp-$(s_1,s_2)$ matrix coefficient. The proof is complete. 
\end{proof}

\begin{rmk}
The above result shows the connection of our method to joint matrix model~\eqref{eq:scheme} $(\mX_\pi, Y_\pi)_{\pi\in[0,1]}$. We should point out, despite of the seeming similarity, a fundamental challenge arises in our setting when the latent index $\pi$ is unobserved. Our level-set approach essentially learns the right ordering of $\mX_\pi$ against the index $\pi\in[0,1]$ (see Figure~\ref{fig:method} of the main paper), thereby facilitating the estimation of regression function $f$. 
\end{rmk}

\clearpage
\section{Proofs}\label{sec:proofs}
\subsection{Main notation}
\begin{table}[ht]
\begin{tabular}{l|l}
Notation & Definition \\
\hline
$(\mX,Y)$ & matrix predictor and univariate response\\ 
$(\mX_i,Y_i)_{i=1}^n$ & a sample of size $n$\\
$\tX$ & predictor space \\
$\shift=Y-\pi$ & shifted response\\
$f\colon \mX\mapsto \mathbb{E}(Y|\mX)$ & ground truth regression function \\
$\hat f\colon \mX\mapsto \mathbb{R}$ & estimated regression function \\
$\bayespif=\sign(f-\pi)$ & Bayes classifier at level $\pi$\\
$\bayesS(\pi)=\{\mX\in\tX\colon f(\mX)\geq \pi\}$ & Indicator set corresponding to $\bayespif$\\
$r$& matrix rank\\
$(s_1,s_2)$ & support parameter \\
$\caliF(r)$ & set of $r$-sign representable functions\\
$\Phi(r,s_1,s_2)$ & rank-$r$, supp-$(s_1,s_2)$ trace functions\\
$\Phi(r)$ & family of rank-$r$ trace functions\\
$\mB$ & rank-$r$, supp-$(s_1,s_2)$ matrix in trace function\\
$\alpha$ & smoothness index of $G(\pi)$\\
$\tN$ & set of mass points associated with CDF $G(\pi)=\mathbb{P}_{\mX}\left[f(\mX)\leq \pi\right]$ \\
$\rho(\pi,\tN)$ & distance from $\pi$ to nearest point in $\tN$\\
$H$ & resolution parameter in sign aggregation \\
$\phi$ & an arbitrary classifier function from $\tX$ to $\mathbb{R}$\\
$S_{\phi} = \{\mX\in\tX\colon \phi(\mX)\geq 0\}$ & Indicator set corresponding to $\phi$\\
$F$ & surrogate large-margin loss function from $\mathbb{R}$ to $\mathbb{R}_{\geq 0}$\\
$\hat \phi_{\pi,F}$ & estimated classifier function based on regularized empirical $F$-risk\\
$\ell_{\pi,F}$ & weighted $F$-loss function, i.e., $\ell_{\pi,F}(\phi;(\mX,Y)) = |\bar{Y}_\pi|F(\phi(\mX)\sign\bar{Y}_\pi)$\\
$\risk$ & weighted 0-1 risk \\
$\riskF$ & weighted surrogate $F$-risk\\
$\eriskF$ & empirical weighted  $F$-risk\\
$S$, $S_1$, $S_2$ & subsets in $\tX$\\
$d_{\Delta}(S_1,S_2)$&probability set difference, equal to $\mathbb{P}_{\mX}(\mX\in\tX\colon \mX\in S_1/S_2 \text{ or }S_2/S_1)$\\
$d_{\pi}(S_1,S_2)$& risk difference, equal to $\risk(\sign S_1)-\risk(\sign S_2)$\\
%$\mathbb{\hat E}\Delta(\phi;(\mX,Y))$ & empirical excess risk\\
%$\Delta(\phi;(\mX,Y))$ & excess $F$-loss\\
$\mY$ & data matrix with complete observation\\
$\Omega\subset[d_1]\times[d_2]$ & index set of observations\\
$\mY_{\Omega}$ & data matrix with incomplete observation\\
$\caliM(r)$ & family of rank-$r$ sign representable matrices\\
$\mTheta\in\caliM(r)$ & signal matrix in matrix completion problem\\
$\mE$ & noise matrix\\
$\mZ$ & an arbitrary matrix\\
\end{tabular}
\end{table}
\clearpage

\subsection{Proof of Theorem~\ref{thm:oracle}}
\begin{proof}
Fix $\pi\in[-1,1]$. For any arbitrary function $\phi\in \Phi(r)$, we evaluate the excess risk between $\sign(f-\pi)$ and $\sign \phi$,
\begin{align}\label{eq:risk}
&\risk(\sign \phi)- \risk(\sign(f-\pi)) \notag \\
= &\ {1\over 2}\mathbb{E}_{\mX}\KeepStyleUnderBrace{\mathbb{E}_{Y|\mX}\left\{|Y-\pi|\left[\left|\sign(Y-\pi)-\sign\phi \right|-\left|\sign(Y-\pi)-\sign(f-\pi)\right|\right]\right\}}_{\stackrel{\text{def}}{=}I}.
\end{align}
Here, $I=I(\mX)$ is a function of $\mX$, and its expression can be simplified as
\begin{align}\label{eq:I}
I&= \mathbb{E}_{Y|\mX}\left[ (Y-\pi)(\sign(f-\pi) - \sign \phi)\mathds{1}(Y\geq \pi)+(\pi-Y)(\sign\phi -\sign (f-\pi))\mathds{1}(Y< \pi)\right]\notag \\
&= \mathbb{E}_{Y|\mX}\left[(\sign(f-\pi)-\sign \phi) (Y-\pi)\right]\notag \\
&= \left[\sign(f-\pi)-\sign\phi \right]\left[f-\pi\right]\notag \\
&= |\sign(f-\pi)-\sign \phi ||f-\pi|,
\end{align}
where the third line uses the fact $\mathbb{E}_{Y|\mX}Y=f(\mX)$. Combining~\eqref{eq:I} with~\eqref{eq:risk}, we conclude that, for all $\phi\in\Phi(r)$, 
\begin{equation}\label{eq:minimum}
\risk(\sign \phi)- \risk(\sign(f-\pi)) ={1\over 2} \mathbb{E}_{\mX} |\sign(f-\pi)-\sign \phi ||f-\pi|\geq 0,
\end{equation}
where the last line equals to zero when $\sign \phi=\sign(f-\pi)$ or $f\equiv \pi$ is a constant function. Note that $(f-\pi)$ is $r$-sign representable by assumption. Therefore, 
\[
\risk(\sign(f-\pi))=\inf\{\risk(\sign \phi )\colon \phi\in \Phi(r)\}. 
\]
Based on the definition of 0-1 classification loss, the $\risk(\cdot)$ relies only on the sign of the argument function. Therefore, for all functions $\bar f \colon \tX\to\mathbb{R}$ that have the same sign as $\sign(f-\pi)$, we have
\[
\risk(\bar f)=\inf\{\risk(\sign \phi )\colon \phi\in \Phi(r)\}=\inf\{\risk( \phi )\colon \phi\in \Phi(r)\}.
\]
\end{proof}



\subsection{Proof of Theorem~\ref{thm:identifiability}}
\begin{proof}
Fix $\pi\in[-1,1]$. For ease of notation, we drop the dependence of $\pi$ in $\bayesS(\pi)$ and simply write $\bayesS$. Based on~\eqref{eq:I} in the proof of Theorem~\ref{thm:oracle}, we have
\begin{align}\label{eq:excess}
d_\pi(S,\bayesS) &\stackrel{\text{def}}{=} \risk(\sign (S))-\risk(\sign(\bayesS))\notag \\
&={1\over 2}\mathbb{E}_{\mX}\left( \left|\sign(S)-\sign(\bayesS) \right||\pi-f| \right)\notag \\
&=\int_{\mX\in S\Delta S_{\text{bayes}}}|f(\mX)-\pi|d \mathbb{P}_{\mX}.
\end{align}

We divide the proof into two cases: $\alpha >0$ and $\alpha=\infty$. 
\begin{enumerate}[label={2.\arabic*},wide, labelwidth=!, labelindent=0pt]
\item[Case 1:] $\alpha >0$.

Consider an arbitrary set $S\subset\mathbb{R}^{d_1\times d_2}$. Let $t$ be an arbitrary number in the interval $[0,1]$, and define the set $A=\{\mX\in \tX \colon |f(\mX)-\pi|>t\}$. 
\begin{align}
\int_{\mX\in S\Delta S_{\text{bayes}}}|f(\mX)-\pi|d \mathbb{P}_{\mX} &\geq t \left[\mathbb{P}_{\mX}(\left(S\Delta S_{\text{bayes}}) \cap A\right)\right] \\
&\geq t\left( \mathbb{P}_{\mX}\left(S\Delta S_{\text{bayes}}\right) - \mathbb{P}_{\mX}(A^c)\right)\\
&\geq t\left( \mathbb{P}_{\mX}\left(S\Delta S_{\text{bayes}}\right) - Ct^{\alpha}\right),\quad \text{for all }0\leq t < \rho(\pi,\tN),
\end{align}
where the last inequality is from $\alpha$-globally smoothness condition.
Combining the above inequality with the identity~\eqref{eq:excess} yields
\begin{equation}\label{eq:tail2}
d_\pi(S,\bayesS)\geq t\left( \mathbb{P}_{\mX}\left(S\Delta S_{\text{bayes}}\right) - Ct^{\alpha}\right),\quad \text{for all }0\leq t < \rho(\pi,\tN).
\end{equation}
We maximize the lower bound of~\eqref{eq:tail2} with respect to $t$, and obtain the optimal $t_{\text{opt}}$,
\[
t_{\text{opt}}=\begin{cases}
\rho(\pi,\tN), & \text{if}\quad \mathbb{P}_{\mX}\left(S\Delta S_{\text{bayes}}\right)> C(1+\alpha)\rho^\alpha(\pi,\tN),\\
\left[{1\over 2C(1+\alpha)}\mathbb{P}_{\mX}\left(S\Delta S_{\text{bayes}}\right)\right]^{1 / \alpha}, & \text{if}\quad \mathbb{P}_{\mX}\left(S\Delta S_{\text{bayes}}\right)\leq C(1+\alpha) \rho^\alpha(\pi,\tN).
\end{cases}
\]
The corresponding lower bound of the inequality~\eqref{eq:tail2} becomes
\begin{align}
  d_\pi(S,\bayesS)\geq\begin{cases}
c_1\rho(\pi,\tN)\mathbb{P}_{\mX}(S\Delta \bayesS), & \text{if}\quad \mathbb{P}_{\mX}\left(S\Delta S_{\text{bayes}}\right)> C(1+\alpha)\rho^\alpha(\pi,\tN),\\
c_2 \left[\mathbb{P}_{\mX}\left(S\Delta S_{\text{bayes}}\right)\right]^{1+\alpha \over \alpha}, & \text{if}\quad \mathbb{P}_{\mX}\left(S\Delta S_{\text{bayes}}\right)\leq C(1+\alpha) \rho^\alpha(\pi,\tN),
\end{cases}
\end{align}
where $c_1,c_2>0$ are two constants independent of $S$. Combining both cases gives
\begin{align}\label{eq:cmultiidentity}
    d_\Delta(S,\bayesS) \stackrel{\text{def}}{=}\mathbb{P}_{\mX}(S\Delta\bayesS)\lesssim \left[d_\pi(S,\bayesS)\right]^{\alpha \over 1+\alpha}+\frac{1}{\rho(\pi,\tN)}d_\pi(S,\bayesS),
\end{align}
where we have absorbed the constants into the relationship $\lesssim$. 

\item [Case 2:] $\alpha = \infty$.

The inequality~\eqref{eq:tail2} now becomes
\begin{equation}\label{eq:infty}
d_\pi(S,\bayesS)\geq t\mathbb{P}_{\mX}(S\Delta \bayesS) = td_\Delta(S,\bayesS), \quad \text{for all }0\leq t < \rho(\pi,\tN).
\end{equation}
The conclusion \eqref{eq:cmultiidentity} follows by taking $t={\rho(\pi,\tN)\over 2}$ in the inequality~\eqref{eq:infty}. 
\end{enumerate}
\end{proof}


\begin{rmk}[Bounding $L_1$ distance by classification risk]\label{eq:rmk} The bound controls the $L_1$ distance to $\bayespif=\sign(f-\pi)$ using the classification excess risk to $\risk(\bayespif)$. The result applies uniformly to $\pi\in[-1,1]$ if $f$ is globally-$\alpha$ smooth; i.e., the bound
\begin{equation}\label{eq:L1}
\onenormSize{}{\sign \phi-\bayespif} \lesssim \left[\risk(\phi)-\risk(\bayespif)\right]^{\alpha \over 1+\alpha}+{1\over \rho(\pi, \tN)}\left[\risk(\phi)-\risk(\bayespif)\right]
\end{equation}
holds for all functions $\phi\colon \tX\to\mathbb{R}$ and for all $\pi\in[-1,1]$ except for a finite number of points. 
In fact, the similar inequality holds by replacing the 0-1 risk to hinge risk or $T$-truncated hinge risk. Specifically, the following bound holds for all functions $\phi\colon \tX\to \mathbb{R}$ and all $\pi\in[-1,1]$ except for a finite number of points.
\begin{itemize}
\item For hinge loss $F(z)=(1-z)_{+}$,
\[
\onenormSize{}{\phi-\bayespif} \lesssim \left[\riskF(\phi)-\riskF(\bayespif)\right]^{\alpha \over 1+\alpha}+{1\over \rho(\pi, \tN)}\left[\riskF(\phi)-\riskF(\bayespif)\right]
\]
\item For $T$-truncated hinge loss $F(z)=\min((1-z)_{+},T)$ with $T\geq 2$,
\[
\onenormSize{}{\phi^T-\bayespif} \lesssim \left[\riskF(\phi)-\riskF(\bayespif)\right]^{\alpha \over 1+\alpha}+{1\over \rho(\pi, \tN)}\left[\riskF(\phi)-\riskF(\bayespif)\right],
\]
where $\phi^T$ is a truncation of function $\phi$ (see formal definition in~\eqref{eq:Tphi}).
\end{itemize}
See Lemma~\ref{lem:hingeL1}.
\end{rmk}


\subsection{Proofs of Theorem~\ref{thm:main}, Part (a) in Theorems~\ref{thm:extension}}\label{sec:sign}
We provide a unified framework that incorporates Theorem~\ref{thm:main}, Part (a) in Theorems~\ref{thm:extension} in the main paper. In addition, part of the proof in Theorem~\ref{thm:sparse} is given with the same framework.  For any $\pi\in[-1,1]$, write $\shift=Y-\pi$, and let $\ell_{\pi,F}(\phi; (\mX, Y))$ denote the weighted $F$-loss
\[
\ell_{\pi,F}(\phi; (\mX, Y))\stackrel{\text{def}}{=}|\shift| F(\sign(Y-\pi)\phi(\mX)),
\]
where the loss function $F$ could be either standard 0-1 loss $F(z)=\mathds{1}(z>0)$ or surrogate loss satisfying Assumption~\ref{ass:main}. Consider the large-margin estimate
\begin{align}\label{eq:phi}
\hat \phi_{\pi,F}=\argmin_{\phi \in\Phi(r,s_1,s_2)}\left\{ {1\over n}\sum_{i=1}^n \ell_{\pi,F}(\phi; (\mX_i,Y_i))+ \lambda \FnormSize{}{\phi}^2\right\}.
\end{align}
The following theorem states the accuracy for sign function estimate $\sign(\hat \phi_{\pi,F})\colon \tX\to\{-1,1\}$. We say that an event $E$ occurs ``with high probability'' if $\mathbb{P}(E)$ tends to 1 as the dimension $d\to \infty$.


 \begin{thm}[Large-margin sign estimation]~\label{thm:unified} Fix $\pi\notin\tN$. Suppose the regression function $f\in\caliF(r,s_1,s_2)$ is $(\pi,\alpha)$-smooth over $\tX$. Then, with high probability at least $1-\exp(-nt_n)$ over training data $(\mX_i,Y_i)_{i\in[n]}$, the estimate~\eqref{eq:phi} satisfies
\begin{equation}\label{eq:unified_sign}
\onenormSize{}{\sign \hat \phi_{\pi,F}-\bayespif}\lesssim t_n^{\alpha/( 2+\alpha)}+{1\over \rho^2(\pi,\tN)}t_n,
\end{equation}
under the following three specifications:
\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]
\item[(a)] (Theorem~\ref{thm:main}) 0-1 loss $F(z)=\mathds{1}(z>0)$, no penalization $\lambda=0$, $(s_1,s_2)=(d_1,d_2)$, and $t_n={1\over n}rd_{\max} $.
\item[(b)] (Theorem~\ref{thm:sparse}) 0-1 loss $F(z)=\mathds{1}(z>0)$, no penalization $\lambda=0$, constant $(s_1,s_2)$,  and $t_n={1\over n}r(s_1+s_2) \log d_{\max}$. 
\item[(c)] (Theorem~\ref{thm:extension}) Surrogate loss satisfying Assumption~\ref{ass:main}, constant $(s_1,s_2)$, $t_n={1\over n} r(s_1+s_2) \log d_{\max}$, penalization $\lambda\asymp (t^{(\alpha+1)/(\alpha+2)}_n+\rho t_n )$, approximation error $a^{(\alpha+1)/(\alpha+2)}_n \leq t_n$.
\end{enumerate}
Here, the constants absorbed in the $\lesssim$ of~\eqref{eq:unified_sign} are independent of $\pi$. 
\end{thm}
\begin{rmk}[Ridge penalization]
The estimation under 0-1 loss requires no penalization, because only the sign, but not the magnitude, of $\phi$ affects the 0-1 risk. One can constrain $\FnormSize{}{\phi}=1$ in the empirical 0-1 risk minimization without altering the solution. In contrast, the surrogate loss such as as hinge loss is scale-sensitive, rending the possible unboundedness of $\phi$. We impose penalization to control the magnitude of the $\FnormSize{}{\phi}$ and thus the local complexity. The resulting estimation enjoys the fast convergence as in sieve estimate~\citep{shen1994convergence} under well tuned $\lambda$.   
\end{rmk}


We provide the proof after introducing two main lemmas.
There are two key ingredients in the proof. The first step is to quantify the convergence of $\hat \phi_{\pi,F}$'s excess $F$-risk using Lemmas~\ref{lem:prepare} and~\ref{lem:risk}. The second step is to relate the excess $F$-risk to excess 0-1 risk using Lemma~\ref{lem:prepare}, and then establish the sign function accuracy using Theorem~\ref{thm:identifiability}.

Recall that $\hat \phi_{\pi,F}$ is the minimizer of empirical $F$-risk.  To quantify the $\hat \phi_{\pi,F}$'s excess $F$-risk, we notice that 
\begin{align}
&\riskF(\hat \phi_{\pi,F})-\inf_{\text{all }{\phi}}\riskF(\phi)\\
=&
 \KeepStyleUnderBrace{\riskF(\hat \phi_{\pi,F})-\inf_{\phi\in\Phi(r,s_1,s_2)}\riskF(\phi_\pi^*)}_{\text{estimation error}}+\KeepStyleUnderBrace{
 \inf_{\phi\in\Phi(r,s_1,s_2)}\riskF(\phi)-\inf_{\text{all }\phi}\riskF(\phi)}_{\text{approximation error}},
 \end{align}
The simplest way to bound $\hat \phi_{\pi,F}$'s excess risk is to use a uniform convergence of excess risk over classifiers $\Phi(r,s_1,s_2)$; however, this approach ignores the local complexity around $\hat \phi_{\pi,F}$ and yields a suboptimal rate. Here we adopt the local iterative techniques of~\citet[Theorem 3]{wang2008probability} to obtain a better rate. The improvement stems from the fact that, under considered assumptions, the variance of the excess loss is bounded in terms of its expectation. Because the variance decreases as we approach the optimal $\phi^*_{\pi}$, the risk of the empirical minimizer converges more quickly to the optimal risk than the simple uniform converge results would suggest. 

The following result summarizes the key properties of four common losses: 0-1 loss, hinge loss, $T$-truncated hinge loss, and psi-loss. Here, the $T$-truncated hinge loss is defined as $F(z)=\min((1-z)_{+},T)$ for a given $T\geq 2$. We will use $T$-truncated hinge loss to faciliate the proofs of Lemma~\ref{lem:risk} and Theorem~\ref{thm:unified}. 

\begin{lem}[Conversion inequalities]\label{lem:prepare} Suppose the regression function $f$ is $(\pi,\alpha)$-smooth, and denote $\bayespif=\sign(f-\pi)$ for $\pi\in[-1,1]$. Let $F$ be 0-1 loss, hinge loss, $T$-truncated hinge loss, or psi-loss. Then, the following three properties hold for all $\pi\in[-1,1]$.
\begin{itemize}[label={2.\arabic*},wide, labelwidth=!, labelindent=0pt]
\item[(a)] Optimality: $\inf_{\text{all }\phi}\riskF(\phi)=\riskF(\bayespif)$.
\item[(b)] Excess risk bound: for all classifers $\phi\colon \tX\to\mathbb{R}$,
\begin{align}\label{eq:b1}
\risk(\phi)-\risk(\bayespif) \leq  C\left[\riskF(\phi)-\riskF(\bayespif)\right],
\end{align}
where $C=1$ for 0-1, hinge loss or $T$-truncated loss, and $C=2$ for psi-loss. 
\item[(c)] Variance-to-mean relationship: Suppose $F$ is 0-1 loss, $T$-truncated loss, or psi-loss. Then, for all classifiers $\phi\colon \tX\to\mathbb{R}$, 
\begin{align}\label{eq:b2}
&\textup{Var}\left[\ell_{\pi,F}(\phi; (\mX,Y))-\ell_{\pi,F}(\bayespif; (\mX,Y))\right]  \notag \\
\lesssim &\ 
\left[\riskF(\phi)-\riskF(\bayespif)\right]^{\alpha/(1+\alpha)}+ {1\over \rho(\pi, \tN)}\left[\riskF(\phi)-\riskF(\bayespif)\right].
 \end{align}
 \end{itemize}
\end{lem}
\begin{rmk}
The property (c) holds only for bounded loss functions, i.e, excluding hinge loss. 
\end{rmk}

\begin{proof}
\begin{enumerate}[label={2.\arabic*},wide, labelwidth=!, labelindent=0pt]

\item[Case 1:] $F(z)=\mathds{1}(z<0)$ is 0-1 loss. 

Properties (a) and (b) directly follow from Theorem~\ref{thm:oracle}. To prove (c), we expand the variance by
\begin{align}\label{eq:mae}
\textup{Var}\left[\ell_{\pi}(\phi;(\mX,Y))-\ell_{\pi}(\bayespif,(\mX,Y)\right] &\lesssim \mathbb{E}|\ell_{\pi}(\phi;(\mX,Y))-\ell_{\pi}(\bayespif,(\mX,Y)|^2\notag \\
&\lesssim \mathbb{E}|\ell_{\pi}(\phi;(\mX,Y))-\ell_{\pi}(\bayespif,(\mX,Y)|\notag \\
& \lesssim \mathbb{E}\left||\sign \shift - \sign \phi(\mX)|-|\sign \shift -\bayespif(\mX)|\right|\notag\\
&\leq \mathbb{E}|\sign \phi-\bayespif|,
\end{align}
where the second line comes from the boundedness of 0-1 loss, and the third line comes from the boundedness of weight $|\shift|$, and fourth line comes from the inequality $||a-b|-|c-b||\leq |a-b|$ for $a,b,c\in\{-1,1\}$. Here we have absorbed the constant multipliers in $\lesssim$. Therefore, the conclusion~\eqref{eq:b2} then directly follows by applying Remark~\ref{eq:rmk} to~\eqref{eq:mae}. 

\item[Case 2:] $F(z)=(1-z)_{+}$ is hinge loss. 

Property (a) was firstly introduced in~\citet[Lemma 1]{wang2008probability}, and here we provide an alternative proof. A direct calculation (see\ Lemma~\ref{lem:hingeL1}) shows that
\[
\riskF(\phi)-\riskF(\bayespif)\geq \mathbb{E}|\phi-\bayespif||f-\pi|\geq0,
\]
Therefore, $\inf_{\text{all }\phi}\riskF(\phi)=\riskF(\bayespif)$. Property~\eqref{eq:b1} is from \citet[Corollary 1]{scott2011surrogate}.

\item[Case 3:] When $F(z) = 2\min(1,(1-z)_+)$ is psi-loss. 

Again, the property (a) follows from~\citet[Lemma 1]{wang2008probability}. For the property~\eqref{eq:b1}, we use~\citet[Theorem 1]{scott2011surrogate} to find the transformation function $\psi$ that relates 0-1 risk to F-risk:
\[
\psi(\risk(\phi)-\risk(\bayespif))\leq \riskF(\phi)-\riskF(\bayespif). 
\]
To put our problem in the context of~\cite{scott2011surrogate}, we need additional notation. For any function measurable $g\colon x\mapsto g(x)$, we write $g=g^{+}-g^{-1}$, where $g^{+}$ and $g^{-}$ are two non-negative functions given by
\begin{align}
g^{+}(x)=\max\{ g(x),0 \} =
\begin{cases}
g(x), & \text{if }g(x)>0,\\
0, & \text{otherwise},\\
\end{cases}\quad 
g^{-}(x)&=\max\{ -g(x),0 \} =
\begin{cases}
-g(x), & \text{if }g(x)<0,\\
0, & \text{otherwise}.
\end{cases}
\end{align}
Under this notation, we have $|g|=g^{+}+g^{-1}$. 

Define the conditional $F$-risk
\[
C_{\pi,F}(\mX,t):=F(t)\mathbb{E}_{Y|\mX}(Y-\pi)^{+}+F(-t)\mathbb{E}_{Y|\mX}(Y-\pi)^{-}.
\]
A direct calculation shows that
\[
C_{\pi,F}(\mX,t)=
\begin{cases}
2\mathbb{E}_{Y|\mX}(Y-\pi)^{-}, & \text{if }t\geq 1,\\
2\mathbb{E}_{Y|\mX}|Y-\pi|-2t\mathbb{E}_{Y|\mX}(Y-\pi)^{+},  &\text{if }t\in[0,1),\\
2\mathbb{E}_{Y|\mX}|Y-\pi|+2t\mathbb{E}_{Y|\mX}(Y-\pi)^{-},  &\text{if }t\in[-1,0),\\
2\mathbb{E}_{Y|\mX}(Y-\pi)^{+}, & \text{if }t<-1.
\end{cases}
\]
Therefore, following the notation of~\cite{scott2011surrogate}, we have
\begin{align}
H_{\pi,F}(\mX)&:=\inf_{t\in\mathbb{R}\colon t(f(\mX)-\pi)\leq 0}C_{\pi,F}(\mX, t)-\inf_{t\in\mathbb{R}} C_{\pi,F}(\mX, t) = 2|f(\mX)-\pi|.
\end{align}
Applying~\citet[Theorem 1]{scott2011surrogate} to the above setup gives the excess risk transformation rule: $\psi: z\to2|z|$. Therefore, the property~\eqref{eq:b1} is proved. 

To prove~\eqref{eq:b2}, notice that 
\begin{align}\label{eq:excess-hinge}
&\text{Var}\left\{|\shift|\left[F(\phi(\mX)\sign \shift)-F(\bayespif(\mX)\sign \shift)\right]\right\} \notag \\
\lesssim &\  \mathbb{E}|\shift||F(\phi(\mX)\sign\shift)-F(\bayespif(\mX)\sign \shift)|\notag \\
 \lesssim &\  \KeepStyleUnderBrace{\mathbb{E}\left| 1-\sign(\phi(\mX)\shift)- F(\bayespif(\mX)\sign \shift)\right
|}_{=:\text{(i)}} +\KeepStyleUnderBrace{\mathbb{E}|\shift|\left|F( \phi(\mX)\sign \shift)- \left(1-\sign(\phi(\mX)\shift)\right) \right|}_{=:\text{(ii)}}.
\end{align}
The first term (i) is bounded as follows
\begin{align}
\text{(i)}=\mathbb{E}\left|\sign (\phi(\mX)\shift)-\sign (\bayespif(\mX)\shift)\right|&\lesssim d_\Delta(S_\phi,\bayesS(\pi))\\&\lesssim  d^\alpha_{\pi}(S_\phi,\bayesS(\pi))+{1\over \rho(\pi, \tN)}d_{\pi}(S_\phi,\bayesS(\pi)),
\end{align}
where the first line uses the fact that $F(1)=0$ and $F(-1)=2$, and last inequality is from Theorem~\ref{thm:identifiability}. Here we define indicator set corresponding $\phi$ as $S_\phi = \{\mX\in\tX\colon \phi(\mX)\geq 0\}$.
The second term (ii) is bounded as follows
\begin{align}
    \text{(ii)}
    &=\mathbb{E}\left[ |\shift| F(\phi(\mX)\sign \shift)- |\shift|\left(1-\sign(\phi(\mX)\shift)\right)\right] \\
    &= \mathbb{E}\left[|\shift|F(\phi(\mX)\sign \shift)-|\shift|F(\bayespif(\mX)\sign \shift)\right]\\
    & \quad + \mathbb{E}\left[|\shift|(1-\sign(\bayespif\shift))-|\shift|(1-\sign(\phi(\mX)\shift))\right]\\
    &\leq [\riskF(\phi)-\riskF(\bayespif)]+d_\pi(S_\phi,\bayesS(\pi)),
\end{align}
where the first equality is based on $F(z) = 1-\sign(z)$ if $z = 1$ or $-1$, and the last inequality is from definition of $d_\pi(\cdot,\cdot)$.  Notice we have  $d_\pi(S_\phi,\bayesS(\pi)) = \risk(\phi)-\risk(\bayespif)$ by definition. Therefore, the proof is complete by combining \eqref{eq:excess-hinge}, \eqref{eq:b1} and bounds (i)-(ii).


\item[Case 4:] $F(z)=\min((1-z)_+,T)$ for $T$-truncated hinge loss, for given $T\geq 2$. 
A direct calculation (c.f. Remark~\ref{rmk:truncate} after Lemma~\ref{lem:hingeL1}) shows that
\[
\riskF(\phi)-\riskF(\bayespif)\geq \mathbb{E}|\phi^T -\bayespif||f-\pi|\geq0, 
\]
where $\phi^T\colon \tX\to[-(T-1),\ (T-1)]$ denotes the $(T-1)$-truncation of $\phi$,
\begin{equation}\label{eq:Tphi}
\phi^T=
\begin{cases}
T-1 & \text{if }\phi>T-1,\\
\phi, & \text{if }|\phi|\leq T-1,\\
-(T-1), & \text{if }\phi<-(T-1).
\end{cases}
\end{equation}
Therefore, $\inf_{\text{all }\phi}\riskF(\phi)=\riskF(\bayespif)$. To show property~\eqref{eq:b1}, we again use~\citet[Theorem 1]{scott2011surrogate} to find the transformation function $\psi$ that relates 0-1 risk to F-risk:
\[
\psi(\risk(\phi)-\risk(\bayespif))\leq \riskF(\phi)-\riskF(\bayespif). 
\]
Using similar arguments as in Case 3, we obtain the conditional $F$-risk
\[
C_{\pi,F}(\mX,t)=
\begin{cases}
\min\left\{T,\ (1+t)\mathbb{E}_{Y|\mX}(Y-\pi)^{-}\right\}, & \text{if } t\geq 1,\\
\mathbb{E}_{Y|\mX}|Y-\pi|-t(f(\mX)-\pi), & \text{if } t\in[0,1),\\
\mathbb{E}_{Y|\mX}|Y-\pi|+t(f(\mX)-\pi), & \text{if } t\in[-1,0),\\
\min\left\{T,\ (1-t)\mathbb{E}_{Y|\mX}(Y-\pi)^{+}\right\}, & \text{if } t<-1.\\
\end{cases}
\]
Therefore, following the notation of~\cite{scott2011surrogate}, we have
\begin{align}
H_{\pi,F}(\mX)&:=\inf_{t\in\mathbb{R}\colon t(f(\mX)-\pi)\leq 0}C_{\pi,F}(\mX, t)-\inf_{t\in\mathbb{R}} C_{\pi,F}(\mX, t) = |f(\mX)-\pi|.
\end{align}
Applying~\citet[Theorem 1]{scott2011surrogate} to the above setup gives the excess risk transformation rule: $\psi: z\to|z|$. Therefore, the property~\eqref{eq:b1} is proved. 

To prove~\eqref{eq:b2}, we use Lemma~\ref{lem:hingeL1} and the boundedness condition of $\norm{F}_\infty\leq T$. Specifically, we bound the variance using the $L$-1 distance between $\phi$ and $\bayespif$, 
\begin{align}
&\text{Var}\left\{|\shift|\left[F(\phi(\mX)\sign \shift)-F(\bayespif(\mX)\sign \shift\right]\right\}\\
\leq&\ 4 \mathbb{E}|F(\phi(\mX)\sign \shift)-F(\bayespif(\mX)\sign \shift)|^2\\
\lesssim &\ T\mathbb{E}|F(\phi(\mX)\sign \shift)-F(\bayespif(\mX)\sign \shift)|\\
\lesssim &\ T\mathbb{E}|\phi^T-\bayespif|
\end{align}
where $T>0$ is the upper bound of truncated hinge loss , the second inequality comes from the boundedness of the $T$-truncated hinge loss, and the last line comes from 1-Lipschitz continuity and the definition of $F$. Applying Remark~\ref{rmk:truncate} in Lemma~\ref{lem:hingeL1} on the last inequality complete the proof.

\end{enumerate}
\end{proof}

Below we establish the estimation convergence rate for $\hat \phi_{\pi,F}$'s excess F-risk. The variance-to-mean relationship in Lemma~\ref{lem:prepare} plays a key role in determining the convergence rate based on empirical process theory~\citep{shen1994convergence}.
The proof adopts the local iterative techniques from~\citet[Theorem 3]{wang2008probability}. Similar techniques have been used in \citet[Theorem 4]{bartlett2006convexity} for similar estimate but without ridge penalization.

\begin{lem}[Excess $F$-risk error]\label{lem:risk}Consider the set-up as in Theorem~\ref{thm:unified}. Then, with high probability and $t_n$ specified in Theorem~\ref{thm:unified}, the following holds for all $\pi\notin\tN$. 
\begin{enumerate}[label={2.\arabic*},wide, labelwidth=!, labelindent=0pt]
\item[(a)] If $F$ is 0-1 loss or psi-loss, then
\[
\risk(\hat \phi_{\pi,F})-\risk(\bayespif)\lesssim \riskF(\hat \phi_{\pi,F})-\riskF(\bayespif) \lesssim t_n^{(\alpha+1)/(\alpha+2)}+ {1\over \rho(\pi,\tN)}t_n.
\]
\item[(b)] If $F$ is hinge loss, then
\[
\risk(\hat \phi_{\pi,F})-\risk(\bayespif)\lesssim \textup{Risk}_{F'}(\hat \phi_{\pi,F})-\textup{Risk}_{F'}(\bayespif) \lesssim t_n^{(\alpha+1)/(\alpha+2)}+ {1\over \rho(\pi,\tN)}t_n,
\]
where $\text{Risk}_{F'}(\phi):=\mathbb{E}\left[|\shift|F'( \phi(\mX)\sign\shift)\right]$ denotes the risk evaluated under $T$-truncated hinge loss $F'=\min(T,\ (1-z)_{+})$, and $T=\max(2,J) \geq \max(2, \|{\phi_\pi^{(n)}}\|_{\infty}, \|{\phi_\pi^{(n)}}\|_{\infty})$ is a constant based on Assumption~\ref{ass:main}(a). 
\end{enumerate}
\end{lem}
\begin{proof}[Proof of Lemma~\ref{lem:risk}]
 In Theorem~\ref{thm:main} and~\ref{thm:sparse}, we use 0-1 loss and have  $\lambda = a_n = 0$.   Without loss of generality, we consider the most general case where  $\lambda,a_n\geq 0$  as in Theorem~\ref{thm:extension} and incorporate all theorems.   This is because penalization $\lambda$ does not affect the result for 0-1 loss case and the condition for $a_n$ in Theorem~\ref{thm:extension} simply holds when $a_n=0$.  
 
 
We first consider the bounded loss (0-1 loss or psi-loss).The modification for unbounded loss (hinge loss) incurs only slight difference in the proof. 

Fix $\pi\notin\tN$, and write $\rho=\rho(\pi,\tN)$. For any function $\phi\in\Phi(r,s_1,s_2)$ of consideration, define empirical weighted $F$-risk
\begin{equation}\label{eq:F}
\eriskF(\phi)	= \frac{1}{n}\sum_{i=1}^n\ell_{\pi,F}(\phi; (\mX_i,Y_i)).
\end{equation}

Under the notation, our estimate $\hat \phi_{\pi,F}$ is the minimizer of the regularized empirical $F$-risk,
\begin{equation}\label{eq:def2}
\hat \phi_{\pi,F}=\argmin_{\phi\in\Phi(r,s_1,s_2)}\Big\{ \eriskF(\phi)+\lambda\FnormSize{}{\phi}^2\Big\}.
\end{equation}
We are interested in the convergence of $\hat \phi_{\pi,F}$'s  excess risk, $\riskF(\hat\phi_{\pi,F})-\riskF(\bayespif)$. 

Let $L_n$ denote the convergence rate to seek. By the definition of $\hat \phi_{\pi,F}$, we have
\[
 \eriskF(\hat\phi_{\pi,F})+\lambda\FnormSize{}{\hat \phi_{\pi,F}}^2\leq \eriskF(\phi_{\pi}^{(n)})+\lambda J,
\]
where $\phi_{\pi}^{(n)}$ is a sequence of functions in Assumption~\ref{ass:main}(a). Therefore, we have the following inclusion of probability events,
\begin{align}\label{eq:outer}
&\left\{(\mX_i,Y_i)_{i\in[n]}\colon \riskF(\hat\phi_{\pi,F})-\riskF(\bayespif)\geq 2L_n \right\}\notag \\
 \subset &
 \bigg\{(\mX_i,Y_i)_{i\in[n]}\colon \exists \phi\in\Phi(r,s_1,s_2), \text{s.t.}\  \riskF(\phi; (\mX,Y))-\riskF(\bayespif)\geq 2L_n, \notag \\&\hspace*{4.6cm} \text{and}\  \eriskF(\hat\phi_{\pi,F})+\lambda\FnormSize{}{\hat \phi_{\pi,F}}^2\leq \eriskF(\phi_{\pi}^{(n)})+\lambda J \bigg\}\notag \\
 \subset &
\left\{(\mX_i,Y_i)_{i\in[n]} \colon \sup_{\substack{\phi\in\Phi(r,s_1,s_2)\\  
\riskF(\phi; (\mX,Y))-\riskF(\bayespif)\geq 2L_n  }}\left[\eriskF(\phi_\pi^{(n)})-\eriskF(\phi)\right]\geq \lambda\FnormSize{}{\phi}^2-\lambda J  \right\} \notag \\
\subset & \bigcup_{\phi\in A_{s,k}}\left\{(\mX_i,Y_i)_{i\in[n]}\colon \sup_{\phi\in A_{s,k}} \left[\eriskF(\phi_\pi^{(n)})-\eriskF(\phi)\right]\geq \lambda\FnormSize{}{\phi}^2-\lambda J \right \}.
\end{align}
In the last line of~\eqref{eq:outer}, we have partitioned the set $\{\phi\in\Phi(r,s_1,s_2)\colon \mathbb{E}\Delta(\phi;(\mX,Y))\geq L_n\}$ into a union of $A_{s,k}$, with 
\begin{align}
A_{s,k}&=\{\phi\in \Phi(r,s_1,s_2)\colon (s+1)L_n\leq \riskF(\phi)-\riskF(\bayespif)< (s+2) L_n, (k-1) J \leq \FnormSize{}{\phi}^2< k J\},
\end{align}
for $s,k=1,2,\ldots$. 


Let $\Gamma$ denote the target probability for the first line in~\eqref{eq:outer}. To bound $\Gamma$, it suffices to bound the sum of probabilities over sets $A_{s,k}$.  For  each $ A_{s,k}$, we consider the scaled empirical process,
\begin{align}\label{eq:empro}
v_n(\phi)&:=\left[\eriskF(\phi_\pi^{(n)})-\eriskF(\phi)\right]-\left[\riskF(\phi_\pi^{(n)})-\riskF(\phi)\right]\\&=\frac{1}{n}\sum_{i}^n\left(\ell_{\pi,F}(\phi_\pi^{(n)};(\mX_i,Y_i))-\ell_{\pi,F}(\phi;(\mX_i,Y_i))-\mathbb{E}\left[\ell_{\pi,F}(\phi_\pi^{(n)};(\mX_i,Y_i))-\ell_{\pi,F}(\phi;(\mX_i,Y_i))\right]\right) .
\end{align}

Notice that 
\begin{align}\label{eq:first}
\riskF(\phi)-\riskF(\phi_\pi^{(n)}) &= \riskF(\phi)-\riskF(\bayespif)+\riskF(\bayespif)-\riskF(\phi_\pi^{(n)})\nonumber\\&\geq (s+1)L_n -a_n\nonumber
\\&\geq  sL_n,
\end{align}
where the first inequality is from the fact that $\phi\in A_{s,k}$ and Assumption~\ref{ass:main}(a) and the last inequality uses the condition that $a_n\lesssim L_n$.

Combining the definition of $v_n$ in \eqref{eq:empro} and \eqref{eq:first} gives \eqref{eq:outer} as
\begin{align}\label{eq:union}
\Gamma&\leq \sum_{s,k=1}^\infty\mathbb{P}\left\{\sup_{\phi\in A_{s,k}}  v_n(\phi)\geq sL_n + \lambda\FnormSize{}{\phi}^2-\lambda J\right\}\nonumber\\&\leq \sum_{s,k=1}^\infty\mathbb{P}\left\{\sup_{\phi\in A_{s,k}}  v_n(\phi)\geq sL_n + \lambda(k-2) J=: M(s,k)\right\},
\end{align}
where  $M(s,k)>0$ for all $s,k\in\mathbb{N}$ from the condition $\lambda J\leq L_n/2$.  Checking this condition is deferred to when we specify $L_n$ in \eqref{eq:delta}. 

The variance of the empirical process is bounded by,
\begin{align}\label{eq:second}
&\sup_{A_{s,k}}\textup{Var}\left[\ell_{\pi,F}(\phi_\pi^{(n)};(\mX,Y))-\ell_{\pi,F}(\phi;(\mX,Y)\right]\\&\leq\sup_{A_{s,k}}2\left\{ \textup{Var}\left[\ell_{\pi,F}(\phi_\pi^{(n)};(\mX,Y))-\ell_{\pi,F}(\bayespif;(\mX,Y)\right]+\textup{Var}\left[\ell_{\pi,F}(\phi;(\mX,Y))-\ell_{\pi,F}(\bayespif;(\mX,Y)\right]\right\}\notag \\
&\lesssim [M(s,k)]^{\alpha/(1+\alpha)}+{M(s,k)\over \rho}=:V(s,k),
\end{align}
where the last inequality is from Lemma~\ref{lem:prepare}.


We next bound the right-hand-side of~\eqref{eq:union} choosing an $L_n$ that satisfies the conditions in~\citet[Theorem 3]{shen1994convergence}. (The specification of $L_n$ is deferred to the next paragraph). Once such $L_n$ is chosen, then it follows from~\citet[Theorem 3]{shen1994convergence} that
\begin{align}\label{eq:gamma}
\Gamma& \lesssim \sum_{s,k}\exp\left( - {nM^2(s,k)\over V(s,k)+M(s,k)}\right)\notag \\
&\lesssim \sum_{s,k}\exp(-\rho nM(s,k)) \notag\\
&= \sum_{s,k}\exp\left(-n\rho sL_n-n\rho J \lambda(k-2)\right)\notag\\
&\leq \left({e^{-n\rho L_n}\over 1-e^{-n\rho L_n} }\right) \left({e^{n\rho \lambda J} \over 1-e^{-n\rho \lambda J}}\right)\\&\leq {e^{-n\rho L_n/2}\over (1-e^{-n\rho L_n})(1-e^{-n\rho\lambda J})},
\end{align}
where the last inequality is form the condition $\lambda J\leq L_n/2$.


Now, we specify an  $L_n$ that satisfies the condition of \citet[Theorem 3]{shen1994convergence}. The convergence rate $L_n>0$ is determined by the solution to the following inequality,
\begin{equation}\label{eq:equation}
\sup_{k\geq 2}{1\over x}\int_{x}^{\sqrt{x^{\alpha/(\alpha+1)}+\rho^{-1}x}}\sqrt{\tH_{[\ ]}(\varepsilon, \Phi^{k},\vnormSize{}{\cdot})}d\varepsilon \lesssim n^{1/2}, \quad \text{where }x=L_n+\lambda J(k-2).
\end{equation}
In particular, the smallest $L_n$ satisfying~\eqref{eq:equation} yields the best upper bound of the error rate. Here $\tH_{[\ ]}(\varepsilon, \Phi^{k}, \vnormSize{}{\cdot})$ denotes the $L_2$-norm, $\varepsilon$-bracketing number (c.f. Definition~\ref{pro:inftynorm}) for function family $\Phi^{k}$, and, we have denoted $\Phi^{k}=\{\phi\in\Phi(r,s_1,s_2)\colon \FnormSize{}{\phi}^2\leq k\}$, i.e., the subset of functions in $\Phi(r,s_1,s_2)$ with magnitudes bounded by $k$, for $k\in\mathbb{N}_{+}$.

It remains to solve for the smallest possible $L_n$ in~\eqref{eq:equation}. Based on Lemma~\ref{lem:metric}, the inequality~\eqref{eq:equation} is satisfied with 
\begin{equation}\label{eq:delta}
L_n\asymp  t_n^{(\alpha+1)/(\alpha+2)}+{1\over \rho} t_n, 
\end{equation}
where
\[
t_n=\begin{cases}
{rd_{\max}\over n}, \quad \text{low-rank model $\phi\in\Phi(r)$,}\\
{r(s_1+s_2)\log d_{\max}\over n}, \quad \text{low-rank and two-way sparse model $\phi\in\Phi(r,s_1,s_2)$.}\\
\end{cases}
\]
Notice that we have the condition on $\lambda$ in  Theorem~\ref{thm:unified}  as $\lambda\asymp  t_n^{(\alpha+1)/(\alpha+2)}+{1\over \rho} t_n$. From this $\lambda$,  we can always choose $L_n$ with suitable constant factor such that $\lambda J\leq L_n/2$. Remember this condition guarantees $M(s,k)>0$ for all $s,k\in\mathbb{N}.$


Plugging~\eqref{eq:delta} into~\eqref{eq:gamma} gives that, 
\begin{align}\label{eq:tail}
\Gamma&=\mathbb{P}\left[\riskF(\hat \phi_{\pi,F}) - \riskF(\bayespif)  \geq L_n \right]\\&\leq{e^{-n\rho L_n/2}\over (1-e^{-n\rho L_n})(1-e^{-n\rho\lambda J})}\\
&\lesssim e^{-c n\rho L_n}\\
&\leq e^{-c n t_n}
,
\end{align}
for some constant $c>0$.
Notice that  the second inequality comes from  $\lambda \asymp L_n$ and  the last inequality uses the fact that $\rho L_n\geq t_n$. The proof is then complete by bounding the 0-1 risk by $F$-risk. 

For unbounded loss (hinge loss), we seek to bound the $F'$-risk of $\hat \phi_{\pi,F}$, where $F'$ is $T$-truncated version of $F$. The general strategy is to evaluate $\hat \phi_{\pi,F}$'s error using $F'$-risk.  Note that the estimate $\hat \phi_{\pi,F}$~\eqref{eq:def2} is defined under unbounded loss $F$. Therefore, the inclusion~\eqref{eq:outer} changes to

\begin{align}
&\left\{(\mX_i,Y_i)_{i\in[n]}\colon \riskF(\hat\phi_{\pi,F})-\riskF(\bayespif)\geq 2L_n \right\}\notag \\
= &
\left\{(\mX_i,Y_i)_{i\in[n]}\colon \textup{Risk}_{\pi,F'}(\hat\phi_{\pi,F})-\textup{Risk}_{\pi,F'}(\bayespif)\geq 2L_n \right\}\notag \\
\subset &
 \bigg\{(\mX_i,Y_i)_{i\in[n]}\colon \exists \phi\in\Phi(r,s_1,s_2), \text{s.t.}\  \textup{Risk}_{\pi,F'}(\phi; (\mX,Y))-\textup{Risk}_{\pi,F'}(\bayespif)\geq 2L_n, \notag \\&\hspace*{4.6cm} \text{and}\  \eriskF(\hat\phi_{\pi,F})+\lambda\FnormSize{}{\hat \phi_{\pi,F}}^2\leq \eriskF(\phi_{\pi}^{(n)})+\lambda J \bigg\}\notag \\
\subset &
 \bigg\{(\mX_i,Y_i)_{i\in[n]}\colon \exists \phi\in\Phi(r,s_1,s_2), \text{s.t.}\  \textup{Risk}_{\pi,F'}(\phi; (\mX,Y))-\textup{Risk}_{\pi,F'}(\bayespif)\geq 2L_n, \notag \\&\hspace*{4.6cm} \text{and}\  
 \widehat{\textup{Risk}}_{\pi,F'}(\hat\phi_{\pi,F})+\lambda\FnormSize{}{\hat \phi_{\pi,F}}^2\leq  \widehat{\textup{Risk}}_{\pi,F'}(\phi_{\pi}^{(n)})+\lambda J \bigg\}
.
\end{align}



where the  first equality uses the fact that the truncation $T= \max(2,J)>\max(2,\sup_n\|{\phi_\pi^{(n)}}\|_{\infty}, \|{\hat\phi_\pi^{(n)}}\|_{\infty})$ is a constant and has no effects to $\riskF(\bayespif)$ or $\riskF(\hat\phi_{\pi,F})$. The last line comes from 
\[
\widehat{\textup{Risk}}_{\pi,F'}(\phi)\leq \widehat{\textup{Risk}}_{\pi,F}(\phi) \text{ for all } \phi\in \Phi(r,s_1,s_2)\quad\text{ and } \quad \widehat{\textup{Risk}}_{\pi,F'}(\phi_\pi^{(n)})= \widehat{\textup{Risk}}_{\pi,F}(\phi_\pi^{(n)}).
\]
The remaining proof follows a similar line of argument as the bounded case. In particular, we invoke the mean-to-variance relationship for bounded $F'$-loss in~\eqref{eq:second}. The final conclusion follows from the excess bound inequality for $T$-truncated risk (c.f. Lemma~\ref{lem:prepare}).
\end{proof}



\begin{proof}[Proof of Theorem~\ref{thm:unified}]
Write $\rho=\rho(\pi,\tN)$. Combining Theorem~\ref{thm:identifiability} and Lemma~\ref{lem:risk} gives
\begin{align}
\onenormSize{}{\sign \phi-\bayespif} &\lesssim \left[\risk(\hat \phi_{\pi,F})-\risk(\bayespif)\right]^{\alpha/(\alpha+1)}+{1\over \rho}\left[\risk(\hat \phi_{F,\pi})-\risk(\bayespif)\right]\\
&\lesssim t_n^{\alpha/(\alpha+2)}+{1\over \rho^{\alpha/\alpha+1}}t_n^{\alpha/(\alpha+1)}+{1\over \rho}t_n^{(\alpha+1)/(\alpha+2)}+{1\over \rho^2}t_n\notag \\
&\leq 4t_n^{\alpha/(\alpha+2)}+{4\over \rho^2}t_n,
\end{align}
where the last line follows from the fact that $a(b^2+b^{(\alpha+2)/(\alpha+1)}+b+1) \leq 4 a (b^2+1)$ with $a={t_n \over \rho^2}$ and $b=\rho t_n^{-1/(\alpha+2)}$. The proof is complete by specializing $t_n$ in each context. 
\end{proof}

\subsection{Proofs of Theorem~\ref{thm:regression},~\ref{thm:sparse} and Part (b) in  Theorem~\ref{thm:extension}}\label{sec:regression}
\begin{proof}[Proof of Theorem~\ref{thm:regression}]
Define the event
\[
E=\left\{\onenormSize{}{\sign \hat \phi_\pi- \sign (f-\pi)} \leq t_n^{\alpha/(2+\alpha)}+{t_n\over \rho^2(\pi,\tN)} \text{ for all }\pi\in\tH\right\}.
\]
%We first bound the probability of the event $E$, and then show $E$ implies~\eqref{eq:pfmain2}. Based on Theorem~\ref{thm:unified} and union bound over $\pi\in \tH$, we have that, 
%\begin{align}\label{eq:joint}
%\mathbb{P}(E) &\geq 1-\sum_{\pi\in\tH}\mathbb{P}\left( \onenormSize{}{\sign \hat \phi_\pi- \sign (f-\pi)} \geq t_n^{\alpha/(2+\alpha)}+{t_n\over \rho^2(\pi,\tN)} \text{ for a given }\pi \right)\\
%&\gtrsim 1-(2H+1) \exp(-cnt_n) \gtrsim 1-\exp(-cnt_n+\log H).
%\end{align}
%%Setting $t=t_n\log H$ in the above inequality shows that, with probability at least $1-\exp(-nt_n)$,
%%\[
%%\onenormSize{}{\sign \hat \phi_\pi- \sign (f-\pi)} \leq \log H\left(t_n^{\alpha/(2+\alpha)}+{1\over \rho^2(\pi,\tN)}t_n\right) \text{ for all $\pi\in\tH$}.
%%\]
%One the other hand, 
We first show that the $E$ implies
\begin{align}\label{eq:mb}
\onenormSize{}{\hat f-f}\lesssim t_n^{\alpha/(\alpha+2)}+{1\over H}+t_nH.
\end{align} 
It follows from the definition of $\hat f$ that
\begin{align}\label{eq:pfmain}
\onenormSize{}{\hat f-f}&=\mathbb{E}\left|{1\over 2H+1} \sum_{\pi\in \tH}\sign \hat \phi_\pi - f\right|\notag\\
&\leq \mathbb{E}\left|{1\over 2H+1}\sum_{\pi \in \tH} (\sign \hat \phi_\pi - \sign (f-\pi)) \right| +\mathbb{E}\left|{1\over 2H+1}\sum_{\pi \in \tH}\sign(f-\pi)-f\right|\notag\\
& \leq{1\over 2H+1}\sum_{\pi \in \tH}\onenormSize{}{\sign \hat \phi_\pi - \sign (f-\pi)}+{1\over H},
\end{align}
where the last line comes from the triangle inequality and the inequality
\[
\left|{1\over 2H+1}\sum_{\pi \in \tH}\sign(f(\mX)-\pi)-f(\mX)\right|\leq {1\over H}, \quad \text{for all } \mX\in\tX.
\]
It suffices to bound the first term in~\eqref{eq:pfmain}. 
%We shall prove that, with probability at least $1-\exp(-cnt_n+\log H)$,  
%\begin{equation}\label{eq:pfmain2}
%{1\over 2H+1}\sum_{\pi \in \tH}\onenormSize{}{\sign \hat \phi_\pi - \sign (f-\pi)} \leq t_n^{\alpha/(\alpha+2)}+{1\over H}+Ht_n,
%\end{equation}
% where $t_n$ is specified in Theorem~\ref{thm:unified} and $c>0$ is a constant. 



Theorem~\ref{thm:unified} shows that the sign function accuracy depends on the closeness of $\pi\in \tH$ to the mass points in $\tH$. Therefore, we partition the level set $\pi \in \tH$ based on their closeness to $\tH$. Specifically, let $\tN_H \stackrel{\text{def}}{=}\bigcup_{\pi'\in\tN}\left(\pi'-\frac{1}{H},\pi'+\frac{1}{H}\right)$ denote the set of levels at least $1\over H$-close to the mass points. We expand left hand side of~\eqref{eq:pfmain} by
\begin{align}\label{eq:twobounds}
&{1\over 2H+1}\sum_{\pi \in \tH}\onenormSize{}{\sign \hat \phi_\pi - \sign (f-\pi)} \notag \\
=& \ {1\over 2H+1}\sum_{\pi \in \tH \cap \tN_H} \onenormSize{}{\sign \hat \phi_\pi - \sign (f-\pi)} +{1\over 2H+1}\sum_{\pi \in \tH \cap \tN_H^c} \onenormSize{}{\sign \hat \phi_\pi - \sign (f-\pi)} .
\end{align}
By assumption, the first term of~\eqref{eq:twobounds} involves only finite number of summands and thus can be bounded by $4C/(2H + 1)$ where $C > 0$ is a constant such that $|\tN|\leq C$. We bound the second term using the explicit forms of $\rho(\pi, \tN)$ in the sequence $\pi \in\Pi\cap \tN_H^c$. 
\begin{align}
{1\over 2H+1}\sum_{\pi \in \tH\cap \tN_H^c} \onenormSize{}{\sign \hat \phi_\pi- \sign (f-\pi)} &\lesssim  {1\over 2H+1}\sum_{\pi\in \tH\cap \tN_H^c} t_n^{\alpha/(2+\alpha)}+{t_n\over 2H+1}\sum_{\pi \in \tH\cap \tN_H^c}{1\over \rho^2(\pi, \tN)}\\
&\leq t_n^{\alpha/(2+\alpha)}+{t_n\over 2H+1} \sum_{\pi \in \tH\cap \tN_H^c} \sum_{\pi' \in \tN}{1\over |\pi-\pi'|^2}\\
&\leq  t_n^{\alpha/(2+\alpha)}+{t_n\over 2H+1} \sum_{\pi'\in \tN} \sum_{\pi \in \tH\cap \tN_H^c}{1\over |\pi-\pi'|^2}\\
&\leq t_n^{\alpha/(2+\alpha)}+ 2CHt_n,
\end{align}
where the first inequality uses the property of $E$, and the last inequality follows from Lemma~\ref{lem:H}.  Combining the bounds for the last two terms in \eqref{eq:twobounds} and plugging it into \eqref{eq:pfmain} shows that the event $E$ implies \eqref{eq:mb}.  Finally, we have
\[
\mathbb{P}\left(\onenormSize{}{\hat f-f}\gtrsim t_n^{\alpha/(\alpha+2)}+{1\over H}+t_nH \right)\leq \mathbb{P	}(E^c)\leq \exp(-cnt_n).
\]
Setting $H\asymp t^{-1/2}_n$ yields the desired upper bound. 

Poofs of Theorem~\ref{thm:sparse} and Part (b) of Theorem~\ref{thm:extension} follow the same argument with $t_n$ specified in Theorem~\ref{thm:unified}.
 \end{proof}

\begin{lem}\label{lem:H}
Fix $\pi'\in\tN$ and a sequence $\tH=\{-1,\ldots,-1/H,0,1/H,\ldots,1\}$ with $H\geq 2$. Then, 
\[
\sum_{\pi \in \tH\cap \tN_H^c}{1\over 
|\pi-\pi'|^2}\leq 4H^2. 
\]
\end{lem}
\begin{proof}[Proof of Lemma~\ref{lem:H}]
Notice that all points $\pi\in\tH\cap\tN_H^c$ satisfy $|\pi-\pi'|>{1\over H}$ for all $\pi'\in\tN$. We use this fact to compute the sum
\begin{align}
   \sum_{\pi \in \Pi\cap \tN_H^c}{1\over |\pi-\pi'|^2}&= \sum_{\frac{h}{H}\in\tH\cap \tN_H^c } {1\over |\frac{h}{H}-\pi'|^2}\\
   &\leq 2H^2\sum_{h=1}^{H}{1 \over h^2}\\
 &\leq 2H^2\left\{ 1+\int_{1}^2{1\over x^2}dx+ \int_{2}^3{1\over x^2}dx+\cdots + \int_{H-1}^H{1\over x^2}dx\right\}\\
&= 2H^2\left(1+\int^{H}_{1}{1\over x^2}dx\right) \leq 4H^2,
\end{align}
 where the third line uses the monotonicity of ${1\over x^2}$ for $x\geq 1$. 
 \end{proof}


\subsection{Proofs of Theorem~\ref{thm:estimation} and Theorem~\ref{thm:extension_gaussian}}\label{sec:sub-Gaussian}
\begin{proof}[Proof of Theorem~\ref{thm:estimation}]
Theorem~\ref{thm:estimation} follows from the same line of proof as in Theorem~\ref{thm:regression}, with slight modification to account for discrete measure space. For any matrix $\mZ\in\mathbb{R}^{d\times d}$, we use $f_{\mZ}\colon[d]^2 \to \mathbb{R}$ to denote the function induced by matrix $\mZ$ such that $f_{\mZ}(\omega)=\mZ(\omega)$ for $\omega\in[d]^2$. Set $\tX=\{\me_i\otimes\me_j\colon(i,j)\in[d]^2\}$ be the discrete feature space, and $n=|\Omega|$ the sample size. Under this set up, $\onenormSize{}{\hat f - f}=\mathbb{E}_{\mX}|\hat f (\mX)- f(\mX)| = \mathbb{E}_{\omega}|\hat \mTheta(\omega)-\mTheta(\omega)|=\textup{MAE}(\hat \mTheta-\mTheta)$. Notice that the small tolerance $\Delta s=1/d^2$ in the pseudo density is dominated by the derived convergence rate. Applying Theorem~\ref{thm:regression} to this setting finishes the proof. 
\end{proof}


\begin{proof}[Proof of Theorem~\ref{thm:extension_gaussian}]
By setting $s=\log(d_{\max})$ in Lemma~\ref{lem:subg}, we have
\[
\mathbb{P}(\mnormSize{}{\mE}\geq \sqrt{4\sigma^2\log d_{\max}} )\leq 2d_{\max}^{-2}.
\]
We divide the sample space into two exclusive events:
\begin{itemize}
\item Event I: $\mnormSize{}{\mE}\geq \sqrt{4\sigma^2\log d_{\max}}$;
\item Event II: $\mnormSize{}{\mE}< \sqrt{4\sigma^2\log d_{\max}}$.
\end{itemize}
Because the Event I occurs with probability tending to zero, we restrict ourselves to the Event II only by following the proof of Theorem~\ref{thm:main}.   We summarize the key difference compared to Section~\ref{thm:regression}. Let $\ell_\omega(\cdot)$ denote the 0-1 loss evaluated at the $\omega$-th value of matrix. 
For ease of notation,  define $\bar \mY = \mY-\pi$ and $\bar\mTheta = \mTheta-\pi$.
We expand the variance by 
\begin{align}
    \label{eq:variance2}
    \text{Var}\left[\ell_\omega\left(\mZ,\bar \mY_\Omega\right)-\ell_\omega\left(\bar\mTheta,\bar\mY_\Omega\right)\right]&\leq \mathbb{E}|\ell_\omega(\mZ(\omega),\bar\mY(\omega))-\ell_\omega(\bar\mTheta(\omega),\bar\mY(\omega))|^2\notag \\
    &= \mathbb{E}|\bar \mY(\omega)-\bar \mTheta(\omega)+\bar\mTheta(\omega)|^2|\text{sgn}\mZ(\omega)-\text{sgn}\bar\mTheta(\omega)| \notag \\
    &\leq 2\left(4 \sigma^2\log d_{\max}+2\right) \mathbb{E}|\text{sgn}\mZ-\text{sgn}\bar\mTheta| \notag \\
    & \lesssim (\sigma^2 \log d_{\max}) \text{MAE}(\sign \mZ, \sign \bar \mTheta),
    \end{align}
where the third line uses the facts $\mnormSize{}{\bar \mTheta}\leq 2$ and $\mnormSize{}{\bar \mY-\bar \mTheta}^2=\mnormSize{}{\mE}^2<4 \sigma^2\log d_{\max}$ within the Event II; the last line comes from the definition of MAE and the asymptotic $\sigma^2\log d_{\max}\gg 1$ provided that $\sigma>0$ with $d_{\max}$ sufficiently large. 

Based on \eqref{eq:variance2}, the $(\alpha,\pi)$-smoothness of $\mTheta$ implies that for all measurable functions $f_{\mZ}$, we have
\begin{align}\label{eq:vartomean}
\text{Var}\Delta_i(f_{\mZ},\bar \mY)\lesssim \left(\sigma^2\log d_{\max}\right) \left\{\left[\mathbb{E}\Delta_i(f_{\mZ},\bar\mY)\right]^{\alpha\over1+\alpha}+\frac{1}{\rho}\mathbb{E}\Delta_i(f_{\mZ},\bar\mY)\right\}.
\end{align}
The empirical process with variance-to-mean relationship \eqref{eq:vartomean} gives that
\begin{align}\label{eq:empriskbd}
\mathbb{P	}\left(\text{Risk}(\hat\mZ)-\text{Risk}(\bar\mTheta)\geq L_n\right)\lesssim \exp(-nt_n),
\end{align}
where the convergence rate $L_n$ is obtained by the same way in the proof of Lemma~\ref{lem:metric} to make sure the conditions hold in Theorem~\ref{thm:refer},  
\begin{align}\label{eq:subgbd}
L_n\asymp t_n^{(\alpha+1)/(\alpha+2)}+\frac{1}{\rho}t_n,\quad\text{ with } t_n =  {r \sigma^2 d_{\max}\log d_{\max}  \over n}.
\end{align}
Combining \eqref{eq:empriskbd} and \eqref{eq:subgbd}, we obtain that, with high probability, 
\begin{align}\label{eq:riskunbd}
   \text{Risk}(\hat\mZ)-\text{Risk}(\bar\mTheta)\lesssim \left( {r \sigma^2  d_{\max}\log d_{\max}  \over |\Omega|}\right)^{(\alpha+1)/(\alpha+2)}+\frac{1}{\rho(\pi,\tN)} \left({r \sigma^2  d_{\max} \log d_{\max} \over |\Omega|} \right),
\end{align} 
where constants have been absorbed into the $\lesssim$ relationship. Therefore, combining \eqref{eq:riskunbd} and the proof of Theorem~\ref{thm:unified} completes the proof for sign matrix estimation error in~\eqref{eq:matrix_sign}. The signal estimation error follows the same proof of Theorem~\ref{thm:regression}.
\end{proof}

\clearpage
\section{Auxiliary lemmas}

\begin{lem}[Hinge loss and $L$-1 distance]\label{lem:hingeL1} Consider the same set-up as in Theorem~\ref{thm:extension}. Let $F(z)=(1-z)_{+}$ be the hinge loss. Then, the $L$-1 distance between $\phi$ and $\bayespif$ is bounded by their excess risk; i.e,
\begin{equation}\label{eq:L}
\onenormSize{}{\phi-\bayespif}
\lesssim
\left[\riskF(\phi)-\riskF(\bayespif)\right]^{\alpha\over 1+\alpha}+
 {1\over \rho(\pi, \tN)}\left[\riskF(\phi)-\riskF(\bayespif)\right],
\end{equation}
for all functions $\phi\colon \tX\to\mathbb{R}$.
\end{lem}


\begin{rmk}\label{rmk:truncate}With little modification in the proof, similar inequality also holds for $T$-truncated hinge loss $F(z)=\min(T,(1-z)_{+})$ with $T\geq 2$. Specifically, 
\[
\onenormSize{}{\phi^T-\bayespif}\leq \left[\riskF(\phi)-\riskF(\bayespif)\right]^{\alpha\over 1+\alpha}+
 {1\over \rho(\pi, \tN)}\left[\riskF(\phi)-\riskF(\bayespif)\right],
\]
where $\phi^T\colon\tX\to[-(T-1),\ (T-1)]$ is the truncated $\phi$ defined in~\eqref{eq:Tphi}.
\end{rmk}


\begin{proof}[Proof of Lemma~\ref{lem:hingeL1}] For ease of notation, we drop the random variable $\mX$ in the function expression, and simply use $\phi, \bayespif$, $f$, to represent the trace function, Bayes rule, and the regression function, respectively. The meaning should be clear given the contexts. 

We expand the excess risk using the definition of hinge loss,
\begin{align}\label{eq:function}
&\riskF(\phi)-\riskF(\bayespif)\notag\\
=&\  \mathbb{E}[|\shift|(1-\phi\sign\shift)_{+}]-\mathbb{E}[|\shift|(1-\bayespif\sign\shift)_{+}] \notag\\
= &\ \int_{\mX} (1-\phi)_{+} \int_{y>\pi}(y-\pi)dy d\mathbb{P}_{\mX}+\int_{\mX}(1+\phi)_{+}  \int_{y\leq \pi}(\pi-y)dy d\mathbb{P}_{\mX} \notag \\
&\ -\int_{\mX} (1-\bayespif)_{+} \int_{y>\pi}(y-\pi)dy d\mathbb{P}_{\mX}-\int_{\mX}(1+\bayespif)_{+}  \int_{y\leq \pi}(\pi-y)dy d\mathbb{P}_{\mX}.
\end{align}

In order to evaluate the integral, we divide the domain $\mX$ into four exclusive regions:
\begin{itemize}
\item Region I $= \{\mX\colon f<\pi \text{ and }\phi\geq -1\}$. In this region, $\bayespif=-1$, and the integrant in~\eqref{eq:function} reduces to
\begin{align}
\Phi_{\textup{I}}&:=\left[(1-\phi)_{+}-2\right]\mathbb{E}_{Y|\mX}(Y- \pi)\mathds{1}(Y> \pi)+(\phi+1)_{+}\mathbb{E}_{Y|\mX}(\pi-Y)\mathds{1}(Y\leq \pi)\\
&\geq -(\phi+1)\mathbb{E}_{Y|\mX}(Y-\pi)\mathds{1}(Y>\pi)-(\phi+1)\mathbb{E}_{Y|\mX}(Y-\pi)\mathds{1}(Y\leq \pi)\\
&=(\phi+1)(\pi-f)=|\phi-\bayespif||f-\pi|.
\end{align}
\item Region II $= \{\mX\colon f < \pi \text{ and }\phi<-1\}$. In this region, $\bayespif=-1$, and the integrant in~\eqref{eq:function} reduces to
\[
\Phi_{\textup{II}}:=-(\phi+1)\mathds{E}_{Y|\mX}(Y-\pi)\mathds{1}(Y>\pi)\geq-|\phi+1|(f-\pi) =|\phi-\bayespif||f-\pi|.
\]
\item Region III $=\{ \mX\colon f\geq \pi \text{ and }\phi\leq 1\}$. In this region, $\bayespif=1$, and the integrant in~\eqref{eq:function} reduces to
\begin{align}
\Phi_{\textup{III}}&:=(1-\phi)_{+}\mathbb{E}_{Y|\mX}(Y-\pi)\mathds{1}(Y>\pi)+\left[(1+\phi)_{+}-2\right]\mathbb{E}_{Y|\mX}(\pi-Y)\mathds{1}(Y\leq \pi)\\
&\geq (1-\phi)\mathbb{E}_{Y|\mX}(Y-\pi)\mathds{1}(Y>\pi)+(\phi-1)\mathbb{E}_{Y|\mX}(\pi-Y)\mathds{1}(Y\leq \pi)\\
&=(1-\phi)(f-\pi)=|\phi-\bayespif||f-\pi|.
\end{align}
\item Region IV $=\{\mX\colon f\geq \pi \text{ and }\phi> 1\}$. In this region, $\bayespif=1$, and the integrant in~\eqref{eq:function} reduces to
\[
\Phi_{\textup{IV}}:=(\phi-1)\mathds{E}_{Y|\mX}(\pi-Y)\mathds{1}(Y\leq \pi)\geq (\phi-1)(f-\pi) = |\phi-\bayespif||f-\pi|.
\]
\end{itemize}
Therefore, the integral is evaluated as
\begin{align}\label{eq:integral}
\riskF(\phi)-\riskF(\bayespif) &= \int_{\textup{I}}\Phi_{\textup{I}} d\mathbb{P}_{\mX}+\int_{\textup{II}}\Phi_{\textup{II}} d\mathbb{P}_{\mX}+\int_{\textup{III}}\Phi_{\textup{III}} d\mathbb{P}_{\mX}+\int_{\textup{IV}}\Phi_{\textup{IV}} d\mathbb{P}_{\mX}\notag\\
&\geq \mathbb{E}|\phi-\bayespif||f-\pi|.
\end{align}
Note that the function $|f-\pi|$ is $\alpha$-smooth. Using the same techniques as in Theorem~\ref{thm:identifiability} to the last line of~\eqref{eq:integral}, we conclude
\begin{equation}
\mathbb{E}|\phi-\bayespif|
\lesssim
\left[\riskF(f)-\riskF(\bayespif)\right]^{\alpha\over 1+\alpha}+
 {1\over \rho(\pi, \tN)}\left[\riskF(f)-\riskF(\bayespif)\right].
\end{equation}
\end{proof}

\begin{defn}[Bracketing number]\label{pro:inftynorm}
Consider a function set $\Phi$, and let $\varepsilon>0$. We call $\{(f^l_m,f^u_m)\}_{m=1}^M$ an $L_2$-metric, $\varepsilon$-bracketing function set of $\Phi$, if for every $f\in \Phi$, there exists an $m\in[M]$ such that 
\[
f^l_m(\mX)\leq f(\mX)\leq f^u_m(\mX),\quad \text{for all }\mX\in\mathbb{R}^{d\times d},
\]
and
\[
\vnormSize{}{f^l_m-f^u_m}\stackrel{\text{def}}{=}\sqrt{\mathbb{E}_{\mX}|f^l_m(\mX)-f^u_m(\mX)|^2} \leq \varepsilon, \ \text{for all } m=1,\ldots,M. 
\]
The bracketing number with $L_2$-metric, $\tH_{[\ ]}(\varepsilon,\ \Phi,\ \vnormSize{}{\cdot})$, is defined as the logarithm of the smallest cardinality of the $\varepsilon$-bracketing function set of $\Phi$.  
\end{defn}


\begin{lem}[Bracketing number for bounded functions in $\Phi(r,s_1,s_2)$]\label{lem:entropy}
Let $\Phi(r,s_1,s_2)$ denote the collection of trace functions, 
\begin{align*} %\label{eq:class}
\Phi(r,s_1,s_2)=\{\phi\colon \mX\mapsto \langle \mX, \mB \rangle +b \ \big| \text{rank}(\mB)\leq r,  \text{supp}(\mB)\leq (s_1,s_2), (\mB,b)\in\mathbb{R}^{d_1\times d_2}\times \mathbb{R}\},
\end{align*}
Assume, for simplicity, that the intercept $b$ is known and that the function domain satisfies $\mathbb{P}\left(\FnormSize{}{\mX}\leq 1\right)=1$. For any given $k\in\mathbb{N}_{+}$, consider the subset of functions in $\Phi(r,s_1,s_2)$ with magnitudes bounded by $k$, denoted by $\Phi^k=\{f\in \Phi(r,s_1,s_2)\colon \FnormSize{}{f}^2\leq k\}$. Then, there exists a constant $C>0$ such that
\[
\tH_{[\ ]}(\varepsilon,\ \Phi^k,\ \vnormSize{}{\cdot}) \leq  Cr(s_1+s_2) \log {kd\over \varepsilon }.
\]
\end{lem}
\begin{proof} 
For any given $k\in\mathbb{N}_{+}$, define the matrix class
\[
\tB=\{\mB\in \mathbb{R}^{d\times d}\colon \text{rank}(\mB)\leq r,\ \text{supp}(\mB)\leq (s_1,s_2),\ \FnormSize{}{\mB}^2\leq k\}.
\]
Based on the assumption of known $b$, there is an one-to-one correspondence between functions in $\Phi^k$ and matrices in $\tB$,
\[
\Phi^k=\{f\colon\mX\mapsto\langle \mX,\mB\rangle+b\ \big| \ \mB\in \tB\}.
\]
Furthermore, every pair of two functions $f_1=\langle\mX, \mB_1\rangle,\ f_2=\langle \mX,\mB_2\rangle\in\Phi(r,s_1,s_1)$ satisfies the norm relationship
\[
\vnormSize{}{f_1-f_2}\leq \mnormSize{}{f_1-f_2} =\sup_{\FnormSize{}{\mX}\leq 1}|\langle \mX,\mB_1\rangle -\langle \mX,\mB_2\rangle  |\leq \FnormSize{}{\mB_1-\mB_2}.
\]
Based on~\citet[Theorem 9.23]{kosorok2007introduction}, the $L_2$-metric, $(2\varepsilon)$-bracketing number in $\Phi^k$ is bounded by
\[
\tH_{[\ ]}(2\varepsilon,\ \Phi^k,\ \vnormSize{}{\cdot}) \leq \tH \left(\varepsilon,\ \tB,\ \FnormSize{}{\cdot}\right),
\]
where $\tH$ denotes the log covering number for the (non-bracketing) set. Therefore, it suffices to bound $\tH(\varepsilon,\ \tB,\ \FnormSize{}{\cdot})$. Now fix two subsets $S_1,S_2\subset [d]$ with $|S_1|=s_1$ and $|S_2|=s_2$, where $|\cdot|$ denotes the cardinality of the sets. Let $\tB_{S_1,S_2}\subset \tB$ denote the subset of matrices satisfying $\mB(i,j)=0$ whenever $(i,j)\notin S_1\times S_2$. Based on~\citet[Lemma 3.1]{candes2011tight}, the log covering number for $\tB_{S_1,S_2}$ is
\[
\tH \left(\varepsilon,\ \tB_{S_1,S_2},\ \Fnorm{\cdot}\right)\leq r(s_1+s_2+1)\log\left({9\sqrt{k}\over \varepsilon}\right).
\]
In view of the construction $\tB\subset\bigcup\{\tB_{S_1,S_2}\colon S_1\times S_2\subset [d_1]\times[d_2], |S_1|=s_1, |S_2|=s_2\}$, an $\varepsilon$-covering set $\tB$ is then given by the union of $\varepsilon$-covering set of $\tB_{S_1,S_2}$. Using Stirling's bound, we derive that 
\begin{align}
\tH(\varepsilon,\ \tB,\ \FnormSize{}{\cdot})&\leq \log \left\{{d \choose s_1}{d \choose s_2}\exp\left[\tH(\varepsilon,\tB_{S_1,S_2},\FnormSize{}{\cdot})\right]\right\}
\\
&\leq s_1 \log {d\over s_1}+s_2\log {d\over s_2}+C'r(s_1+s_2+1)\log{k\over \varepsilon}\\
& \leq Cr(s_1+s_2)\log{kd\over \varepsilon},
\end{align}
where $C,C'>0$ are constants. 
\end{proof}

\begin{lem}[Local complexity of $\Phi(r,s_1,s_2)$] \label{lem:metric}
Define $\Phi^{k}=\{f\in\Phi(r,s_1,s_2)\colon \FnormSize{}{f}^2\leq k\}$ for all $k\in\mathbb{N}_{+}$; i.e., $\Phi^k$ is the subset of functions in $\Phi(r,s_1,s_2)$ with magnitudes bounded by $k$. Set 
\begin{equation}\label{eq:specification}
L_n\asymp \left({r(s_1+s_2)\log d \over n } \right)^{\alpha+1\over \alpha+2} + {1\over \rho (\pi, \tN)}\left({r(s_1+s_2)\log d \over n } \right),\quad \text{and}\ 
\lambda \asymp {L_n}.
\end{equation}
Then, the following inequality is satisfied for all $k\in\{2,3,\ldots\}$,
\begin{equation}
{1\over x}\int^{\sqrt{x^{\alpha/(\alpha+1)}+{x\over \rho (\pi, \tN)}}}_{x} \sqrt{\tH_{[\ ]}(\varepsilon,\ \Phi^{k},\ \vnormSize{}{\cdot}) }d\varepsilon \leq n^{1/2}, \quad \text{where}\ x:=L_n+\lambda J(k/2-1).
\end{equation}
\end{lem}
\begin{proof}
To simplify the notation, we write $\rho=\rho(\pi, \tN)$, and define
\begin{equation}\label{eq:complexity}
g(x, k)={1\over x}\int^{\sqrt{x^{\alpha/(\alpha+1)}+\rho^{-1}x}}_{x} \sqrt{r(s_1+s_2)\log\left({kd\over \varepsilon}\right)}d\varepsilon,\quad \text{for all }k\in\{2,3,\ldots\},
\end{equation}
where we have inserted the bracketing number based on Lemma~\ref{lem:entropy}.  Notice that
\begin{align}\label{eq:g}
g(x,k)&\leq {\sqrt{r(s_1+s_2)}\over L}\int_{x}^{\sqrt{x^{\alpha/(\alpha+1)}+\rho^{-1}x}}\sqrt{\log \left(kd \over x \right)}d\varepsilon\notag \\
&\leq \sqrt{r(s_1+s_2)(\log k+\log d - \log x)}\left({\sqrt{x^{\alpha/(2\alpha+2)}}+\sqrt{\rho^{-1}x} \over x }-1\right)\notag \\
&\leq  \sqrt{r(s_1+s_2)(\log k+\log d)}\left( {1\over x^{(\alpha+2)/(2\alpha+2)}}+{1\over \sqrt{\rho x}}\right),
\end{align}
where the second line follows from $\sqrt{a+b} \leq \sqrt{a}+\sqrt{b}$ for $a,b>0$.  It remains to verify that $g(L_n+\lambda J(k/2-1), k) \leq n^{1/2}$ for all $k\in\{2,3,\ldots,\}$. Notice that 
\[
L_n+\lambda J(k/2-1)=(k/2+C)\left\{\left( {r(s_1+s_2)\log d \over n}\right)^{(\alpha+1)/(\alpha+2)}+{1\over \rho}\left( {r(s_1+s_2)\log d \over n}\right)\right\},
\]
for some universal constant $C>0$. Plugging the above expression into the last line of~\eqref{eq:g} gives
\[
g(L_n+\lambda J(k/2-1),k)\leq n^{1/2}\sqrt{\log k+\log d\over (k/2)^{(\alpha+2)/(\alpha+1)} \log d}+n^{1/2}\sqrt{\log k+\log d \over (k/2) \log d }\leq C'n^{1/2},
\]
for all $k\in\{2,3,\ldots\}$, where $C'>0$ is a constant independent of $k$ and $d$. The proof is therefore complete. 
\end{proof}


\begin{thm}[Theorem 3 in~\cite{shen1994convergence}]~\label{thm:refer}Let $\tF$ be a class of functions defined on $\tX$ with $\sup_{f\in\tF}\norm{f}_{\infty}\leq T$. Let $(\mX_i)_{i=1}^n$ be i.i.d.\ random variables with distribution $\mathbb{P}_{\mX}$ over $\tX$. Set $\sup_{f\in\tF}\textup{Var}f(\mX)=V<\infty$. 
Define the empirical process $\mathbb{\hat E}f={1\over n}\sum_{i=1}^n f(\mX_i)$. 
Define $x_n^*$ be the solution of the equation to the following equation
\[
{1\over x}\int_x^{\sqrt{V}}\sqrt{\tH_{[\ ]}(\varepsilon,\tF,\vnormSize{}{\cdot})}d\varepsilon =\sqrt{n}.
\]
Suppose
\[
x_n^*\lesssim {V\over T},\quad \text{and}\quad \tH_{[\ ]}(\sqrt{V},\tF,\vnormSize{}{\cdot})\lesssim {n(x_n^*)^2 \over V}.
\]
Then we have
\begin{equation}\label{eq:oneside}
\mathbb{P}\left(\sup_{f\in\tF}\mathbb{\hat E}f -\mathbb{E}f\geq x^*_n\right)\lesssim  \exp\left(-{n (x^*_n)^2\over V+Tx^*_n}\right). %\lesssim \exp\left(-{n(x^*_n)^2\over V}\right).
\end{equation}
\end{thm}
%\begin{rmk}
%Both upper bounds $T$ and $V$ are allowed to depend on $n$. By applying Theorem~\ref{thm:refer} to $\tG=\{g\colon g= \mathbb{E}f-f, f\in \tF\}$, we obtain the other side inequality,
%\[
%\mathbb{P}\left\{\sup_{f\in\tF}\left(\mathbb{E}f-\mathbb{\hat E}f \right)\geq x_n^*\right\}\lesssim \exp\left(-{n (x^*_n)^2\over V+Tx^*_n}\right).
%\]
%\end{rmk}

\begin{lem}[sub-Gaussian maximum]\label{lem:subg}
Let $X_1,\ldots,X_n$ be independent sub-Gaussian zero-mean random variables with variance proxy $\sigma^2$. Then, for any $s>0,$
\[\mathbb{P}\left\{\max_{1\leq i\leq n}|X_i|\geq\sqrt{2\sigma^2(\log n +s)}\right\}\leq2 e^{-s}.\]
\end{lem}
\begin{proof}
The conclusion follows from
\begin{align}
\mathbb{P}[\max_{1\leq i\leq n}X_i\geq u] \leq \sum_{i=1}^n\mathbb{P}[X_i\geq u]\leq n e^{-{u^2\over 2\sigma^2}} = e^{-s},
\end{align}
where we set $u = \sqrt{2\sigma^2(\log n+s)}.$
\end{proof}


\bibliographystyle{plainnat} % Style BST file (imsart-number.bst or imsart-nameyear.bst)

\bibliography{ref-trace.bib}     
\end{document}



