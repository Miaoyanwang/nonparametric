\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{fan2019generalized,hamidi2019low}
\citation{zhou2014regularized,wang2014network}
\citation{wang2017generalized,fan2019generalized}
\citation{Cai2016}
\citation{Ma2016}
\gdef\hy@title{Nonparametric trace regression in high dimensions via sign series representation}
\thanksnewlabel{e1@email}{{chanwoo.lee@wisc.edu}{1}}
\thanksnewlabel{e2@email}{{lexinli@berkeley.edu}{1}}
\thanksnewlabel{e3@email}{{hzhang@math.arizona.edu}{1}}
\thanksnewlabel{e4@email}{{miaoyan.wang@wisc.edu}{1}}
\thanksnewlabel{Athanks}{{1}{1}}
\thanksnewlabel{Bthanks}{{2}{1}}
\thanksnewlabel{Cthanks}{{3}{1}}
\gdef\hy@author{Chanwoo Lee, Lexin Li, Hao Helen Zhang and Miaoyan Wang}
\gdef\hy@subject{Manuscript in preparation}
\gdef\hy@keywords{62G05, 62H30, Matrix trace model, Matrix completion, Matrix predictor regression, Nonparametric regression, Sparsity}
\gdef\author@num{4}
\newlabel{sec:intro}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{eq:linear}{{1}{1}{Introduction}{equation.1.1}{}}
\citation{caruana1997multitask,fan2019generalized}
\citation{candes2011tight}
\citation{recht2010guaranteed}
\newlabel{sec:limit}{{1.1}{2}{Inadequacy of low-rank trace regression}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Inadequacy of low-rank trace regression}{2}{subsection.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Two examples of high-rank matrix trace models. (a) The numerical rank of the matrix $g(\bm  {B})$ versus $c$ in the transformation, where the numerical rank is defined by $\textup  {rank}(g(\bm  {B}))=\qopname  \relax m{min}\{\textup  {rank}(\bm  {C})\penalty \@M \mskip 2mu\mathpunct {}\nonscript \mkern -\thinmuskip {:}\mskip 6muplus1mu\relax \delimiter 69645069 \bm  {C}-g(\bm  {B})\delimiter 86422285 _F \leq 0.01 \delimiter 69645069 g(\bm  {B})\delimiter 86422285 _F \}$. The error bar represents standard errors from 10 realizations of $\bm  {B}$. (b) Heatmap of a full-rank matrix $\bm  {B}\in \mathbb  {R}^{d\times d}$ with the $(i,j)$-th entry equal to $\qopname  \relax o{log}(1+\qopname  \relax m{max}(i,j))$. In (a), $d=50$, and in (b), $d=10$.}}{3}{figure.1}}
\newlabel{fig:limit}{{1}{3}{Two examples of high-rank matrix trace models. (a) The numerical rank of the matrix $g(\mB )$ versus $c$ in the transformation, where the numerical rank is defined by $\rank (g(\mB ))=\min \{\rank (\mC )\colon \FnormSize {}{\mC -g(\mB )} \leq 0.01 \FnormSize {}{g(\mB )} \}$. The error bar represents standard errors from 10 realizations of $\mB $. (b) Heatmap of a full-rank matrix $\mB \in \mathbb {R}^{d\times d}$ with the $(i,j)$-th entry equal to $\log (1+\max (i,j))$. In (a), $d=50$, and in (b), $d=10$}{figure.1}{}}
\newlabel{penG}{{1}{3}{Two examples of high-rank matrix trace models. (a) The numerical rank of the matrix $g(\mB )$ versus $c$ in the transformation, where the numerical rank is defined by $\rank (g(\mB ))=\min \{\rank (\mC )\colon \FnormSize {}{\mC -g(\mB )} \leq 0.01 \FnormSize {}{g(\mB )} \}$. The error bar represents standard errors from 10 realizations of $\mB $. (b) Heatmap of a full-rank matrix $\mB \in \mathbb {R}^{d\times d}$ with the $(i,j)$-th entry equal to $\log (1+\max (i,j))$. In (a), $d=50$, and in (b), $d=10$}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Our proposal and contributions}{3}{subsection.1.2}}
\citation{fan2019generalized,hamidi2019low}
\citation{goodfellow2016deep}
\citation{hao2019sparse}
\citation{zhou2020broadcasted}
\citation{tsybakov1997nonparametric}
\citation{gibou2018review}
\citation{wang2008probability}
\citation{singh2009adaptive}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Related work}{4}{subsection.1.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.4}Notation and organization}{5}{subsection.1.4}}
\newlabel{sec:idea}{{2}{5}{Nonparametric trace regression model}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Nonparametric trace regression model}{5}{section.2}}
\newlabel{eq:model}{{2}{5}{Nonparametric trace regression model}{equation.2.2}{}}
\citation{zhou2014regularized,wang2017generalized,fan2019generalized}
\citation{balabdaoui2019least,ganti2017learning}
\citation{hu2020matrix}
\citation{cohn2013fast}
\citation{de2003nondeterministic}
\newlabel{def:caliF}{{1}{6}{Rank-$r$ sign representable function}{definition.1}{}}
\newlabel{eq:sign}{{3}{6}{Rank-$r$ sign representable function}{equation.2.3}{}}
\citation{chan2014consistent}
\newlabel{prop:signbasis}{{1}{7}{Sign-representable function over basis matrices}{proposition.1}{}}
\newlabel{prop:signrank}{{2}{7}{Sign-rank vs. matrix rank}{proposition.2}{}}
\newlabel{ex:high-rank}{{5}{7}{High-rank matrix completion model}{example.5}{}}
\newlabel{sec:bridge}{{3}{8}{From classification to regression: a learning reduction approach}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}From classification to regression: a learning reduction approach}{8}{section.3}}
\newlabel{eq:proposal}{{4}{8}{From classification to regression: a learning reduction approach}{equation.3.4}{}}
\newlabel{eq:stepfunction}{{5}{8}{From classification to regression: a learning reduction approach}{equation.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Nonparametric matrix regression via sign function series estimation. We use a series of weighted classifications to estimate the sign functions, then obtain the regression function estimate via sign aggregation. Here, $\bm  {X}\in \mathcal  {X}$ denotes matrix-valued predictor, $f\penalty \@M \mskip 2mu\mathpunct {}\nonscript \mkern -\thinmuskip {:}\mskip 6muplus1mu\relax \mathcal  {X}\to \mathbb  {R}$ denotes regression function, and $\textup  {sgn}(f-\pi )\in \{-1,1\}$ is the sign function, where $\pi \in \{-1,\ldots  ,-1/H,0,1/H,\ldots  , 1\}$ is the series of levels to aggregate in our algorithm.}}{8}{figure.2}}
\newlabel{fig:method}{{2}{8}{Nonparametric matrix regression via sign function series estimation. We use a series of weighted classifications to estimate the sign functions, then obtain the regression function estimate via sign aggregation. Here, $\mX \in \tX $ denotes matrix-valued predictor, $f\colon \tX \to \mathbb {R}$ denotes regression function, and $\sign (f-\pi )\in \{-1,1\}$ is the sign function, where $\pi \in \{-1,\ldots ,-1/H,0,1/H,\ldots , 1\}$ is the series of levels to aggregate in our algorithm}{figure.2}{}}
\citation{tsybakov2004optimal,singh2009adaptive}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Statistical characterization of sign functions via weighted classification}{9}{subsection.3.1}}
\newlabel{eq:loss}{{6}{9}{Statistical characterization of sign functions via weighted classification}{equation.3.6}{}}
\newlabel{eq:constrained}{{7}{9}{Statistical characterization of sign functions via weighted classification}{equation.3.7}{}}
\newlabel{thm:oracle}{{1}{9}{Global optimum of weighted classification risk}{theorem.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Identifiability}{9}{subsection.3.2}}
\newlabel{ass:decboundary}{{2}{9}{$\alpha $-smoothness}{definition.2}{}}
\newlabel{eq:mass}{{8}{9}{$\alpha $-smoothness}{equation.3.8}{}}
\citation{singh2009adaptive,xu2020class}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Three examples of CDF, $G(\pi )=\mathbb  {P}_{\bm  {X}}(f(\bm  {X})\leq \pi )$, with local smoothness index $\alpha $ at $\pi $ depicted in dashed line. (a) and (b). Function $G(\pi )$ $\alpha =1$ because the $G(\pi )$ has finite sub-derivatives in the range of $\pi $; (c). Function $G(\pi )$ with $\alpha =\infty $ at most $\pi $ (in blue), except for a total number of $|\mathcal  {N}|=r$ jump points (in red). Here $|\mathcal  {N}|$ denotes the number of jump points.}}{10}{figure.3}}
\newlabel{fig:CDF}{{3}{10}{Three examples of CDF, $G(\pi )=\mathbb {P}_{\mX }(f(\mX )\leq \pi )$, with local smoothness index $\alpha $ at $\pi $ depicted in dashed line. (a) and (b). Function $G(\pi )$ $\alpha =1$ because the $G(\pi )$ has finite sub-derivatives in the range of $\pi $; (c). Function $G(\pi )$ with $\alpha =\infty $ at most $\pi $ (in blue), except for a total number of $|\tN |=r$ jump points (in red). Here $|\tN |$ denotes the number of jump points}{figure.3}{}}
\newlabel{thm:identifiability}{{2}{10}{Identifiability}{theorem.2}{}}
\newlabel{eq:identity}{{9}{10}{Identifiability}{equation.3.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Regression risk bound}{10}{subsection.3.3}}
\newlabel{thm:main}{{3}{11}{Sign function estimation}{theorem.3}{}}
\newlabel{eq:riskbound}{{10}{11}{Sign function estimation}{equation.3.10}{}}
\newlabel{thm:regression}{{4}{11}{Regression function estimation}{theorem.4}{}}
\newlabel{eq:bound}{{11}{11}{Regression function estimation}{equation.3.11}{}}
\newlabel{eq:final}{{12}{11}{Regression function estimation}{equation.3.12}{}}
\citation{zhou2014regularized}
\citation{Zhang2015}
\newlabel{sec:examples}{{4}{12}{Two applications of nonparametric matrix learning}{section.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Two applications of nonparametric matrix learning}{12}{section.4}}
\newlabel{sec:sparse}{{4.1}{12}{Low-rank sparse matrix predictor regression}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Low-rank sparse matrix predictor regression}{12}{subsection.4.1}}
\newlabel{thm:sparse}{{5}{12}{Nonparametric low-rank two-way sparse regression}{theorem.5}{}}
\newlabel{eq:final2}{{13}{12}{Nonparametric low-rank two-way sparse regression}{equation.4.13}{}}
\newlabel{sec:matrixcompletion}{{4.2}{12}{High-rank matrix completion}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}High-rank matrix completion}{12}{subsection.4.2}}
\newlabel{eq:modelcompletion}{{14}{12}{High-rank matrix completion}{equation.4.14}{}}
\newlabel{eq:est}{{15}{13}{High-rank matrix completion}{equation.4.15}{}}
\newlabel{eq:smooth}{{3}{13}{$\alpha $-smoothness for discrete distribution}{definition.3}{}}
\newlabel{thm:estimation}{{6}{13}{Nonparametric matrix completion}{theorem.6}{}}
\newlabel{eq:real}{{16}{13}{Nonparametric matrix completion}{equation.4.16}{}}
\citation{chi2020provable}
\citation{gao2016optimal}
\citation{ganti2015matrix}
\citation{ganti2015matrix}
\citation{yuan2016tensor,pmlr-v119-lee20i}
\citation{shen2003psi}
\citation{shen2003psi}
\newlabel{thm:sample-complexity}{{1}{14}{Sample complexity for nonparametric completion}{corollary.1}{}}
\newlabel{sec:estimation}{{5}{14}{Large-margin implementation and ADMM algorithm}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Large-margin implementation and ADMM algorithm}{14}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Large-margin learning}{14}{subsection.5.1}}
\newlabel{eq:large-margin}{{17}{14}{Large-margin learning}{equation.5.17}{}}
\citation{wang2008probability}
\citation{shen2003psi}
\citation{Ma2013}
\citation{Ma2016}
\citation{Ma2016}
\citation{parikh2014proximal}
\newlabel{eq:stepfunction-large-margin}{{18}{15}{Large-margin learning}{equation.5.18}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}ADMM optimization}{15}{subsection.5.2}}
\newlabel{eq:sampleoptim}{{19}{15}{ADMM optimization}{equation.5.19}{}}
\newlabel{eq:ADMM}{{5.2}{15}{ADMM optimization}{equation.5.19}{}}
\newlabel{eq:primal}{{5.2}{15}{ADMM optimization}{equation.5.19}{}}
\newlabel{eq:dual}{{20}{15}{ADMM optimization}{equation.5.20}{}}
\citation{scott2011surrogate}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces {\bf  Nonparametric low-rank two-way sparse matrix regression via ADMM} }}{16}{algorithm.1}}
\newlabel{alg:weighted}{{1}{16}{Hyperparameter tuning}{algorithm.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Hyperparameter tuning}{16}{subsection.5.3}}
\newlabel{sec:large-margin}{{5.4}{16}{Large-margin statistical guarantees}{subsection.5.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Large-margin statistical guarantees}{16}{subsection.5.4}}
\newlabel{eq:riskdef}{{21}{16}{Large-margin statistical guarantees}{equation.5.21}{}}
\newlabel{ass:main}{{1}{16}{Assumptions on surrogate loss}{assumption.1}{}}
\newlabel{eq:fisher}{{5.4}{17}{Large-margin statistical guarantees}{assumption.1}{}}
\newlabel{thm:extension}{{7}{17}{Large-margin estimation}{theorem.7}{}}
\newlabel{sec:simulation}{{6}{17}{Simulations}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Simulations}{17}{section.6}}
\newlabel{sec:validation}{{6.1}{17}{Impacts of sample size, matrix dimension, and model complexity}{subsection.6.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Impacts of sample size, matrix dimension, and model complexity}{17}{subsection.6.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Finite sample performance under a smooth function. (a) simulation setup; (b) prediction error with varying $n$ and $d=20$ for the continuous response; (c) for the binary response; (d) with varying $d$ and $n=200$. The dashed lines in panels (b)-(d) represent upper bounds $\mathcal  {O}(n^{-1/3})$, $\mathcal  {O}(n^{-1/3})$, and $\mathcal  {O}(\qopname  \relax o{log}d)$, respectively. The results are based on 30 data replications.}}{18}{figure.4}}
\newlabel{fig:logistic}{{4}{18}{Finite sample performance under a smooth function. (a) simulation setup; (b) prediction error with varying $n$ and $d=20$ for the continuous response; (c) for the binary response; (d) with varying $d$ and $n=200$. The dashed lines in panels (b)-(d) represent upper bounds $\tO (n^{-1/3})$, $\tO (n^{-1/3})$, and $\tO (\log d)$, respectively. The results are based on 30 data replications}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Finite sample performance under a non-smooth function. The setup is similar as Fig\nobreakspace  {}\ref  {fig:logistic}. The dashed lines in panels (b)-(d) represent upper bounds $\mathcal  {O}(n^{-1/2})$, $\mathcal  {O}(n^{-1/2})$, and $\mathcal  {O}(\qopname  \relax o{log}d)$, respectively.}}{18}{figure.5}}
\newlabel{fig:step}{{5}{18}{Finite sample performance under a non-smooth function. The setup is similar as Fig~\ref {fig:logistic}. The dashed lines in panels (b)-(d) represent upper bounds $\tO (n^{-1/2})$, $\tO (n^{-1/2})$, and $\tO (\log d)$, respectively}{figure.5}{}}
\citation{relion2019network}
\citation{Zou2005}
\citation{relion2019network}
\citation{chollet2018deep}
\newlabel{sec:comparison}{{6.2}{19}{Comparison with alternative methods}{subsection.6.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Comparison with alternative methods}{19}{subsection.6.2}}
\newlabel{eq:pattern}{{22}{19}{Comparison with alternative methods}{equation.6.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Four activation patterns in simulations. The active region is divided into four or five subregions, denoted by I, II, ..., V, each of which has its own edge connectivity signal $g_{pq}(\pi )$.}}{19}{figure.6}}
\newlabel{fig:region}{{6}{19}{Four activation patterns in simulations. The active region is divided into four or five subregions, denoted by I, II, ..., V, each of which has its own edge connectivity signal $g_{pq}(\pi )$}{figure.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Performance comparison of various methods under four different activation patterns. Reported are the prediction error $\delimiter 69645069 \mathaccentV {hat}05Ef - f\delimiter 86422285 _1$, denoted by ``regression", and the misclassification error at $\pi =1/2$, denoted by ``classification". The results are based on 30 data replications.}}{20}{figure.7}}
\newlabel{fig:compare}{{7}{20}{Performance comparison of various methods under four different activation patterns. Reported are the prediction error $\onenormSize {}{\hat f - f}$, denoted by ``regression", and the misclassification error at $\pi =1/2$, denoted by ``classification". The results are based on 30 data replications}{figure.7}{}}
\newlabel{sec:realdata}{{7}{20}{Real data applications}{section.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Real data applications}{20}{section.7}}
\citation{van2013wu}
\citation{desikan2006automated}
\citation{zhang2018mapping}
\citation{wang2019common}
\citation{hastie2015statistical}
\citation{wang2019common}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Example output returned by {\bf  \relax \fontsize  {8}{10}\selectfont  ASSIST} based on the moving average of the feature weights, and the scatter plot of the edge connectivity strength, averaged by each subregion, versus the estimated mean response. The dashed curve shows the true function. }}{21}{figure.8}}
\newlabel{fig:compare2}{{8}{21}{Example output returned by {\bf \scriptsize ASSIST} based on the moving average of the feature weights, and the scatter plot of the edge connectivity strength, averaged by each subregion, versus the estimated mean response. The dashed curve shows the true function}{figure.8}{}}
\newlabel{sec:brain}{{7.1}{21}{Brain connectivity analysis}{subsection.7.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Brain connectivity analysis}{21}{subsection.7.1}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Brain connectivity analysis. (a) Comparison of prediction accuracy measured by AUC, with standard errors over 5-fold cross validation in the parentheses. For \text  {\bf  \relax \fontsize  {9}{11pt plus .2\p@ minus .2\p@ }\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus2\p@ \belowdisplayskip \abovedisplayskip \abovedisplayshortskip \abovedisplayskip \belowdisplayshortskip \abovedisplayskip CNN }, there is no report for node selection. (b) Top edges selected by the method {\relax \fontsize  {9}{11pt plus .2\p@ minus .2\p@ }\selectfont  \abovedisplayskip 6\p@ plus2\p@ minus2\p@ \belowdisplayskip \abovedisplayskip \abovedisplayshortskip \abovedisplayskip \belowdisplayshortskip \abovedisplayskip \bf  ASSIST-p}. The letters ``r'' and ``l'' in node names indicate the right and left hemisphere, respectively. The $p$-value is calculated from the two-sample test of edge connection strength between two individual groups. }}{21}{table.1}}
\newlabel{fig:real}{{1}{21}{Brain connectivity analysis. (a) Comparison of prediction accuracy measured by AUC, with standard errors over 5-fold cross validation in the parentheses. For \CNN , there is no report for node selection. (b) Top edges selected by the method {\footnotesize \bf ASSIST-p}. The letters ``r'' and ``l'' in node names indicate the right and left hemisphere, respectively. The $p$-value is calculated from the two-sample test of edge connection strength between two individual groups}{table.1}{}}
\citation{hastie2015matrix}
\citation{mazumder2010spectral}
\citation{rennie2005fast}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Brain connectivity analysis. (a) Top edges overlaid on a brain template. (b) Edge connectivity strength versus estimated mean response. Colored curves represent the moving averages of connectivity strengths, gray bands represent one standard error, and jitter points represent the raw connectivity values (0 or 1).}}{22}{figure.9}}
\newlabel{fig:real2}{{9}{22}{Brain connectivity analysis. (a) Top edges overlaid on a brain template. (b) Edge connectivity strength versus estimated mean response. Colored curves represent the moving averages of connectivity strengths, gray bands represent one standard error, and jitter points represent the raw connectivity values (0 or 1)}{figure.9}{}}
\newlabel{sec:completion}{{7.2}{22}{Imaging matrix completion}{subsection.7.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Imaging matrix completion}{22}{subsection.7.2}}
\newlabel{sec:discussion}{{8}{22}{Discussion}{section.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Discussion}{22}{section.8}}
\bibstyle{imsart-number}
\bibdata{ref-trace}
\bibcite{balabdaoui2019least}{{1}{}{{}}{{}}}
\bibcite{Cai2016}{{2}{}{{}}{{}}}
\bibcite{candes2011tight}{{3}{}{{}}{{}}}
\bibcite{caruana1997multitask}{{4}{}{{}}{{}}}
\bibcite{chan2014consistent}{{5}{}{{}}{{}}}
\bibcite{chi2020provable}{{6}{}{{}}{{}}}
\bibcite{chollet2018deep}{{7}{}{{}}{{}}}
\bibcite{cohn2013fast}{{8}{}{{}}{{}}}
\bibcite{de2003nondeterministic}{{9}{}{{}}{{}}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Matrix completion analysis. (a)-(b) correspond to the 40\% missing rate, and (c)-(d) the 80\% missing rate. Error bars represent the standard error over 5-fold cross-validation. Numbers in the parentheses represent the selected tuning parameters for each method. In (a) and (c), we omit the worst method {\bf  \relax \fontsize  {8}{10}\selectfont  Alt} for space consideration.}}{23}{figure.10}}
\newlabel{fig:braincv}{{10}{23}{Matrix completion analysis. (a)-(b) correspond to the 40\% missing rate, and (c)-(d) the 80\% missing rate. Error bars represent the standard error over 5-fold cross-validation. Numbers in the parentheses represent the selected tuning parameters for each method. In (a) and (c), we omit the worst method {\bf \scriptsize Alt} for space consideration}{figure.10}{}}
\@writefile{toc}{\contentsline {section}{Acknowledgements}{23}{section*.2}}
\@writefile{toc}{\contentsline {section}{Supplementary Material}{23}{section*.4}}
\@writefile{toc}{\contentsline {section}{References}{23}{section*.6}}
\bibcite{desikan2006automated}{{10}{}{{}}{{}}}
\bibcite{fan2019generalized}{{11}{}{{}}{{}}}
\bibcite{ganti2017learning}{{12}{}{{}}{{}}}
\bibcite{ganti2015matrix}{{13}{}{{}}{{}}}
\bibcite{gao2016optimal}{{14}{}{{}}{{}}}
\bibcite{gibou2018review}{{15}{}{{}}{{}}}
\bibcite{goodfellow2016deep}{{16}{}{{}}{{}}}
\bibcite{hamidi2019low}{{17}{}{{}}{{}}}
\bibcite{hao2019sparse}{{18}{}{{}}{{}}}
\bibcite{hastie2015matrix}{{19}{}{{}}{{}}}
\bibcite{hastie2015statistical}{{20}{}{{}}{{}}}
\bibcite{hu2020matrix}{{21}{}{{}}{{}}}
\bibcite{pmlr-v119-lee20i}{{22}{}{{}}{{}}}
\bibcite{Ma2013}{{23}{}{{}}{{}}}
\bibcite{mazumder2010spectral}{{24}{}{{}}{{}}}
\bibcite{parikh2014proximal}{{25}{}{{}}{{}}}
\bibcite{recht2010guaranteed}{{26}{}{{}}{{}}}
\bibcite{relion2019network}{{27}{}{{}}{{}}}
\bibcite{rennie2005fast}{{28}{}{{}}{{}}}
\bibcite{scott2011surrogate}{{29}{}{{}}{{}}}
\bibcite{shen2003psi}{{30}{}{{}}{{}}}
\bibcite{singh2009adaptive}{{31}{}{{}}{{}}}
\bibcite{tsybakov1997nonparametric}{{32}{}{{}}{{}}}
\bibcite{tsybakov2004optimal}{{33}{}{{}}{{}}}
\bibcite{van2013wu}{{34}{}{{}}{{}}}
\bibcite{wang2008probability}{{35}{}{{}}{{}}}
\bibcite{wang2019common}{{36}{}{{}}{{}}}
\bibcite{wang2017generalized}{{37}{}{{}}{{}}}
\bibcite{wang2014network}{{38}{}{{}}{{}}}
\bibcite{xu2020class}{{39}{}{{}}{{}}}
\bibcite{Ma2016}{{40}{}{{}}{{}}}
\bibcite{yuan2016tensor}{{41}{}{{}}{{}}}
\bibcite{Zhang2015}{{42}{}{{}}{{}}}
\bibcite{zhang2018mapping}{{43}{}{{}}{{}}}
\bibcite{zhou2014regularized}{{44}{}{{}}{{}}}
\bibcite{zhou2020broadcasted}{{45}{}{{}}{{}}}
\bibcite{Zou2005}{{46}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
