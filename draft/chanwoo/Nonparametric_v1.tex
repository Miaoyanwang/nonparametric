 \documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}
\newcommand*{\KeepStyleUnderBrace}[1]{%f
\mathop{%
\mathchoice
{\underbrace{\displaystyle#1}}%
{\underbrace{\textstyle#1}}%
{\underbrace{\scriptstyle#1}}%
{\underbrace{\scriptscriptstyle#1}}%
}\limits
}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{amsmath,amssymb,amsthm,bm,hyperref,enumerate}
\usepackage{dsfont,listings}

\usepackage[ruled,vlined]{algorithm2e}

\usepackage{enumitem}
\newtheorem{schm}{Scheme}
\newtheorem*{schm*}{Scheme}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{assumption}{Assumption}
\newtheorem{cor}{Corollary}[section]
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}
\newtheorem{clm}{Claim}
% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%
\usepackage{xcolor}

\input macros.tex
\begin{document}
\setcounter{secnumdepth}{3}
%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}
\def\fixme#1#2{\textbf{\color{red}[FIXME (#1): #2]}}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Model free probability estimation for matrix feature}
  \author{Author 1\thanks{
    The authors gratefully acknowledge \textit{please remember to list all relevant funding sources in the unblinded version}}\hspace{.2cm}\\
    Department of YYY, University of XXX\\
    and \\
    Author 2 \\
    Department of ZZZ, University of WWW}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Nonparametric learning with matrix-valued predictors in high dimensions}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
We consider the problem of learning the relationship between a binary label response and a high-dimensional matrix-valued predictor. Such data problems arise commonly in brain imaging studies, sensor network localization, and personalized medicine. Existing regression analysis often takes a parametric procedure by imposing a pre-specified relationship between variables. However, parametric models are insufficient in capturing complex regression surfaces defined over high-dimensional matrix space. Here, we propose a flexible nonparametric framework for various learning tasks, including classification, level set estimation, and regression, that specifically accounts for the matrix structure in the predictors. Unlike classical approaches, our method adapts to the possibly non-smooth, non-linear pattern in the regression function of interest. The proposal achieves prediction and interpretability simultaneously via a joint optimization of prediction rules and dimension reduction in the matrix space. Generalization bounds, estimation consistency, and convergence rate are established. We demonstrate the advantage of our method over previous approaches through simulations and applications to  {\color{red}XXX} data analyses. 
\end{abstract}

\noindent%
{\it Keywords:} Nonparametric learning, matrix-valued predictors, high dimension, classification, level-set estimation, regression.
\vfill


\newpage
\spacingset{1.5} % DON'T change the spacing!
\section{Introduction}
\label{sec:intro}

\section{Methods}
\label{sec:meth}
\setcounter{subsection}{-1}

Consider a statistical learning problem where we would like to model the relationship between a feature $\mX \in \tX$ and a response $Y\in\tY$. Suppose that we observe a sample of of $n$ data points, $(\mX_1,Y_1),\ldots,(\mX_n,Y_n)$, identically and independently distributed (i.i.d.) according to a unknown distribution $\mathbb{P}(\mX,Y)$ over $\tX\times \tY$. We are interested in predicting a new response $Y_{n+1}$ from a new feature value $\mX_{n+1}$. The observations $\{(\mX_i,Y_i)\}_{i=1}^n$ are called the training data and $(\mX_{n+1},Y_{n+1})$ the test point. When no confusion arises, we often omit the subscript $(n+1)$ and simply write $(\mX,Y)$ for the prototypical test point. The test point is assumed independent of the training data and is drawn from the same unknown distribution $\mathbb{P}$. Our goal is to make accurate prediction under a wide range of distributions. In particular, we consider a non-parametric, distribution-free setting with no strong assumptions on the data generative distribution other than i.i.d. 


We focus on the scenario with matrix-valued predictors and binary label response; that is, $\tX=\mathbb{R}^{d_1\times d_2}$ and $\tY=\{-1,1\}$. Matrix-valued predictors ubiquitously arise in modern applications. One example is from electroencephalography studies of alcoholism. The data set records voltage value measured from 64 channels of electrodes on 256 subjects for 256 time points~\citep{zhou2014regularized}. Each feature is a $256\times 64$ matrix and the response is a binary indicator of subject being alcoholic or control. Another example is pedestrian detection from image data. Each image is divided into 9 regions where local orientation statistics are generated with a total of 22 numbers per region. This yields a $22 \times 9$ matrix-valued feature and a binary label response indicating whether the image is pedestrian~\citep{Shashua2004PedestrianDF}. 

In the above two examples and many other studies, researchers are interested in \emph{interpretable prediction}, where the goal is to not only make accurate prediction but also identify features that are informative to the prediction. While classical learning algorithms have been successful in prediction with vector-valued predictors, the key challenge with matrix-valued predictors is the complex structure in the feature space. 
A naive approach is to transform the feature matrices to vectors and apply classical methods based on vectors to solve the problem. However, this vectorization would destroy the structural information of the data matrices. Moreover, the reshaping matrices to vectors results in high dimensionality which leads to overfitting. Notably, the ambient dimension with matrix-valued feature, $d_1d_2$, is often comparable to, or even larger than the number of sample, $n$. 
%Modern applications are often in the high dimensional regime where $d_1d_2$ is comparable to, or even larger, than the sample size $n$. 
Our method exploits the structural information in the data matrix to overcome these challenges. 


\subsection{Three main problems}
%Assume that we are given i.i.d. training samples $\{(\mX_1,y_1),\ldots,(\mX_n,y_n)\}$ drawn according to some unknown probability density function $\mathbb{P}_{\mX,y}(\mX,y)$ where  data matrix $\mX_i\in\mathbb{R}^{d_1\times d_2}$ and class labels $y_i\in\{+1,-1\}$. Define the regression function $p(\mX) = \mathbb{E}\left(\frac{y+1}{2}|\mX\right)$. 
Before we proceed with our proposal for matrix-valued features, we present the concrete learning problems of our interest. We consider three major supervised learning problems: classification, level set estimation, and regression estimation.

\begin{enumerate}[label={2.\arabic*}]
\setcounter{enumi}{1}
\item {\it The problem of classification}: Classification is the problem of identifying to which of a set of categories a new observation belongs, based on training samples. We aim to find a decision rule $g(\mX)$ that has small error
\begin{align}
    \mathbb{P}_{\mX,y}\left(y\neq g(\mX)\right),
\end{align}
where $g(\mX) = \text{sign}(f(\mX))$ and $f:\mathbb{R}^{d_1\times d_2}\rightarrow \mathbb{R}$ is a decision function. The classification problem has long been interested. Many attempts have been developed and performed well for example, decision tree, nearest neighbor, neural network and support vector machine to name a few. However, most of methods have focused on vector valued features. In many classification problems, the input features are naturally represented as matrices or tensors rather than vectors. 
%One example is a study of an electroencephalography data set of alcoholism. The data records voltage value measured from 64 channels of electrodes on 256 subjects for 256 time points, so each feature data is $256\times 64$ matrix and the response is binary indicator of subject being alcoholic or control \citep{zhou2014regularized}. Another example is pedestrian detection from image data. Each image was divided into 9 regions where local orientation statistics were generated with a total of 22 numbers per region, so each feature data is $22 \times 9$ matrix and the response is whether the image is pedestrian \citep{Shashua2004PedestrianDF}. 
We want to tackle matrix valued classification preserving the matrix structure.


\item {\it The problem of level set estimation}: The $\pi$-level set of $p$ given a fixed $\pi\in[0,1]$ is the set 
\begin{align}
S(\pi) = \{\mX\in\mathbb{R}^{d_1\times d_2}: p(\mX)>\pi\}.
\end{align}
 Accurate and efficient level set estimation plays an important role in many applications.
 One example can be found in medical decision making. In Osteosarcoma treatment, the degree of tumor necrosis is used to guide the choice of postoperative chemotherapy \citep{man2005expression}. Patients with $\geq 90 \%$ necrosis is labeled as 1, which is response variable $y$. Suppose that $\mX$ is a feature matrix collected from the patient such as gene expression levels on each tissue. Knowledge of the regression level set is needed to allow effective postoperative chemotherapy without a biopsy. 
 We consider a nonparametric way to estimate the $\pi$-level set of the regression function based on classification problem.
%Level set estimation plays crucial roles in many applications such as digital elevation maps, medical imaging and pattern recognition.
\item {\it The problem of regression estimation}: Regression function calculates expectation of $y$ given a feature matrix $\mX$ on the basis of a training set of data. In our setting, the regression $ \mathbb{E}(y|\mX)$  is equivalent to  the conditional probability $\mathbb{P}(y = 1|\mX)$ because the class label $y$ is binary. Knowledge about the class probability itself is of significant interest and can tell us the confidence of the outcome of classification. Traditionally, the regression problem is addressed 
through distribution assumption like logistic regression or linear discriminant analysis (LDA). In many applications, however, it is often difficult to  justify the assumptions made in logistic regression or satisfy the Gaussian assumption in LDA. These issues become more challenging for matrix features because of high dimensionality.
We establish distribution free method for estimating the regression function $p(\mX)$ based on level set estimation.  

\end{enumerate}
The three problems represent common learning tasks with increasing difficulties. 
Classification problem can be completed from level set $S(\frac{1}{2})$ utilizing Bayes rule. The level set estimation problem becomes trivial when we have all information about regression function.  
Accordingly, classical approach for the three problems is to find a solution for regression first, and address the other two based on the estimation. This is why the regression problem is also called soft classification. However, our approach finds classification rule first and address the level set estimation and regression problem in order. Through the sequence of solving the problems, we successfully solve the problems without assuming probability distribution.


\subsection{Choice of decision function space}\label{sec:fcn class}

Here, we consider a set of linear predictors as decision function class and extend to non-linear case.
\subsubsection{Linear predictors}\label{subsec:linear class}
 We impose low-rankness on a linear predictor of the form $f(\mX) = \langle \mB,\mX\rangle,$
where $\mB,\mX\in\mathbb{R}^{d_1\times d_2}$ and $\langle \mX,\mX'\rangle = \text{Tr}(\mX^T\mX')$.
Specifically,  the coefficient matrix $\mB$ has low-rank $r$  usually much smaller than the matrix size  $\min(d_1,d_2)$,
\begin{align}\label{eq:lowrank}
\mB = \mU\mV^T \text{ where } \mU \in\mathbb{R}^{d_1\times r} ,\mV\in\mathbb{R}^{d_2\times r}\text{ and } r\leq\min(d_1,d_2).
\end{align}
The condition determines trade-off between model complexity and flexibility.  This low-rankness makes distinction from classical classification problem for feature vectors and preserves structural information of feature matrices.

\subsubsection{Nonlinear predictors}\label{subsec:nonlinear class}
We generalize classical kernel method for vector case to matrix case. Before proposing a new matrix feature mapping and kernel, we introduce notations and operations used later. Let $\phi_i\colon\mathbb{R}^{d_i}\rightarrow \tH_i$ be feature mappings with a classical kernel $K_i\colon\mathbb{R}^{d_i}\times \mathbb{R}^{d_i}\rightarrow \mathbb{R}$ for $i = 1,2.$ $\tH_i$ denotes image space of $\phi_i$ and a possibly infinite dimensional Hilbert space. Let  Let $\tH^{d_1\times d_2}$ = $\{\mX\colon\mX = \entry{x_{ij}},x_{ij}\in\tH\}$ denote the collection of $d_1$ by $d_2$ matrices with each entry taking value in a Hilbert space $\tH$.  Matrix algebraic operations are carried over from $\mathbb{R}^{d_1\times d_2}$ to $\tH^{d_1\times d_2}$ as follows,
\begin{prop}
    Let $\mA = \entry{a_{ij}}$ and $\mA' = \entry{a'_{ij}}$ be two matrices in $\tH^{d_1\times d_2}$ and $\mB = \entry{b_{ij}}$ be a matrix in $\tH^{d_2\times d_3}$.  Let $\mP\in\mathbb{R}^{r\times d_1}$ and $\mP'\in\mathbb{R}^{d_2\times r}$ be real valued matrices. Then, we have well defined operations.
    \begin{itemize}
    \item Inner product: $\langle \mA,\mA' \rangle = \sum_{ij}\langle a_{ij},a'_{ij}\rangle\in\mathbb{R}.$
    \item Linear combination: $\mP\mA = \entry{c_{ij}}\in\tH^{r\times d_2}$ where $c_{ij} = \sum_{k\in[d_1]}p_{ik}a_{kj}$ for all $(i,j)\in[r]\times[d_2]$ and  $\mA\mP' = \entry{c'_{ij}}\in\tH^{d_1\times r}$ where $c'_{ij} = \sum_{k\in[d_1]}a_{ik}p'_{kj}$ for all $(i,j)\in[d_1]\times[r]$.
    \item Summation:  $\mA+\mA' = \entry{a_{ij}+a'_{ij}}\in\tH^{d_1\times d_2}.$
    \item Matrix product: $\mA\mB' = \entry{c_{ij}}\in\mathbb{R}^{d_1\times d_3}$, where $c_{ij} = \sum_{k\in[d_2]}\langle a_{ik},b_{kj}\rangle,$ for all $(i,j)\in[d_1]\times[d_3].$ 
\end{itemize}
\end{prop}

Now we present matrix the matrix kernel and associated feature mapping.
We define matrix valued feature mapping over  the $d_1$-by-$d_2$ matrix space.
\begin{defn}\label{def:map}
Let $\phi_1\colon\mathbb{R}^{d_1}\rightarrow \tH_1 $ and $\phi_2\colon\mathbb{R}^{d_2}\rightarrow \tH_2 $  be classical feature mappings defined on vector space. Then $\Phi$ is matrix feature mappings defined on
\begin{align}\label{eq:featuremap}
    \Phi(\mX)\colon \mathbb{R}^{d_1\times d_2} &\rightarrow (\tH_1\times \tH_2)^{d_1\times d_2}\\
    \mX &\mapsto \Phi(\mX) = \entry{\Phi(\mX)_{ij}}\text{ where } \Phi(\mX)_{ij}\stackrel{\text{def}}{=}\left(\phi_2(\mX_{i:}),\phi_1(\mX_{:j})\right).
\end{align}
We can define a linear function  $f\colon \mathbb{R}^{d_1\times d_2}\rightarrow \mathbb{R}$ with respect to $\Phi(\mX)\in(\tH_1\times \tH_2)^{d_1\times d_2}$,
\begin{align}\label{eq:linearfcn}
    f(\mX) \stackrel{\text{def}}{=}\langle \mB,\Phi(\mX) \rangle ,\text{ where }& \mB=\entry{(\mb^{\text{row}}_i,\mb^{\text{col}}_j)}\in(\tH_1\times \tH_2)^{d_1\times d_2},\\\text{ with }& \mb^{\text{row}}_i\in\tH_1\text{ and } \mb^{\text{col}}_j\in\tH_2 \text{ for all } (i,j)\in[d_1]\times[d_2].
\end{align}
\end{defn}

Notice we impose this structure on $\mB$ for identifiability issue.
Matrix valued feature map \eqref{eq:featuremap} and corresponding linear function \eqref{eq:linearfcn} is reduced down to existing vector version  and can be generalized to tensor case (see Supplement for the details).
 Suppose $\mB$ in \eqref{eq:linearfcn} admits low rank decomposition, 
 \begin{align}\label{eq:lowrk}
     \mB = \mP_1\mC\mP_2^T,\text{ where }& \mP_1\in\mathbb{R}^{d_1\times r},\mP_2\in\mathbb{R}^{d_2\times r} \text{ and } \mC = \entry{(\mc_i^{\text{row}},\mc_j^{\text{col}})}\in(\tH_1\times \tH_2)^{r\times r},\\
     \text{ with }&\mc^{\text{row}}_i\in\tH_1\text{ and } \mc^{\text{col}}_j\in\tH_2 \text{ for all } i,j\in[r].
 \end{align}
 Again, we have the structured $\mC$ for identifiability. 
   When classical kernels $K_i$  are linear kernels whose corresponding feature maps $\phi_i$ are identify  $i=1,2$, One can check that considered linear functions with low-rank $r$ in \eqref{eq:lowrk} is equivalent to linear functions with low-rank $2r$ in Section \ref{subsec:linear class}.
   
   Now we define matrix kernel associated with the matrix feature mapping.
\begin{defn}\label{def:kernel}
Let $K_i(\cdot,\cdot)$ be classical kernels which can be represented as $K_i(\cdot,\cdot) = \langle \phi_i(\cdot),\phi_i(\cdot)\rangle$ for $i=1,2$. Let weight matrices $\mW_i = \entry{w^{(i)}_{jk}}\in\mathbb{R}^{d_i\times d_i}$ be  rank-$r$ semi-positive definite matrices for $i = 1,2$. Then $\{\mW_i,K_i\}_{i=1,2}$  induce matrix kernel defined by
\begin{align}
    \mK\colon\mathbb{R}^{d_1\times d_2}\times \mathbb{R}^{d_1\times d_2}&\rightarrow \mathbb{R}\\
    (\mX,\mX')&\mapsto \mK(\mX,\mX')  = \sum_{j,k\in[d_1]}w^{(1)}_{jk}K_1(\mX_{j:},\mX'_{k:})+\sum_{j,k\in[d_2]}w^{(2)}_{jk}K_2(\mX_{:j},\mX'_{:k}).
\end{align}
\end{defn}
Notice that this kernel definition is generalization from classical kernel based on vector valued features. We can associate the feature mapping in Definition \ref{def:map} with the matrix kernel like classical case which based on vectors. Given $\{\mW_i,K_i\}_{i=1,2}$, we have
\begin{align}\mK(\mX,\mX')& = \sum_{j,k\in[d_1]}w^{(1)}_{jk}K_1(\mX_{j:},\mX'_{k:})+\sum_{j,k\in[d_2]}w^{(2)}_{jk}K_2(\mX_{:j},\mX'_{:k}) \\
&=\langle (\mW_1,\mW_2)\Phi(\mX),(\mW_1,\mW_2)\Phi(\mX')\rangle.
\end{align}
By Definition \ref{def:kernel}, we can successfully learn weight matrices thereby nonlinear function given pre-specified row-wise and column-wise kernels $\{K_i\}_{i=1,2}$ avoiding feature mapping in Definition \ref{def:map}. In Section \ref{subsec:pb1}, we specify learning problem and show how we learn nonlinear function using the matrix kernel.



\subsection{Classification}
\label{subsec:pb1}

We consider a large margin classifier that minimizes a cost function in $f$ over a decision function class $\tF$
\begin{align}
\label{eq:large-margin}
\min_{f\in\tF}n^{-1}\sum_{i=1}^n L\left(y_i f(\mX_i)\right)+\lambda J(f),
\end{align}
where $J(f)$ is a regularization term for model complexity and $L(z)$ is a margin loss that is a function of the functional margin $yf(\mX)$. Examples of such loss functions are the hinge loss function $L(z) = (1-z)_+$ and the logistic loss function $L(z) =\log(1+e^{-z})$.  For demonstration, we focus on the hinge loss case in Equation \eqref{eq:large-margin}. However, our estimation schemes and theorems are applicable to general large-margin classifiers.

Here we present the solution to~\eqref{eq:large-margin} with nonlinear kernels; linear case is a special case of nonlinear case. Based on the decision function class in Section \ref{subsec:nonlinear class}, we solve the following optimization problem.
\begin{align}
\label{eq:opt}
(\hat \mP_1,\hat\mC,\hat\mP_2) = \argmin_{\{(\mP_1,\mP_2,\mC)\in\mathbb{R}^{d_1\times r}\times \mathbb{R}^{d_2\times r}\times (\tH_1\times \tH_2)^{r\times r}\}}n^{-1}\sum_{i=1}^n \left(1-y_i\langle \mP_1\mC\mP_2^T,\mX_i\rangle\right)_++\lambda \FnormSize{}{\mP_1\mC\mP_2^T}^2.
\end{align}
Notice that the optimization problem \eqref{eq:opt} degenerates to the conventional SVM with vectorized feature matrices when the coefficient is full rank. From the solution to \eqref{eq:opt}, our estimated decision rule is \begin{align}\hat g(\mX) = \text{sign}(\hat f(\mX)) =  \text{sign}\left(\langle \hat\mP_1\hat\mC\hat\mP_2^T,\mX\rangle\right)\end{align}


We make a remark on the implication of our formulation \eqref{eq:opt}.
The formulation \eqref{eq:opt} implies a joint learning of dimension reduction and classification risk minimization. This is one of our contribution to combine two different processes into one. To check this, we see the a dual representation of the solution to \eqref{eq:opt}.
\begin{align}\label{eq:dualrep}
f(\mX) &= 
&= \sum_{i=1}^n\alpha_iy_i\left(\sum_{j,k\in[d_1]} [\mH_{\mP_1}]_{jk},K_1(\mX^{(i)}_{j:},\mX_{k:})+\sum_{j,k\in[d_1]} [\mH_{\mP_2}]_{jk},K_2(\mX^{(i)}_{:j},\mX_{:k})\right).
\end{align}
where $\{\alpha_i\}_{i=1}^n$ are  (sparse) dual solution of \eqref{eq:opt}, $\mX^{(i)}_{jk}$ denotes (j,k)-entry of $\mX_i$ and $\mH_{\mA} = \mA(\mA^T\mA)^{-1}\mA^T$. Notice that the last representation of \eqref{eq:dualrep} is a linear combination of matrix kernel induced by weight matrices $\{\mH_{\mP_i}\}_{i=1,2}$ and row and column-wise kernels $\{K_i\}_{i=1,2}$. 
Therefore, our considered function space can be written as 
\begin{align}
    \tF &= \{f\colon\mX\mapsto\langle\mP_1\mC\mP_2^T,\Phi(\mX)\rangle|\mC\in(\tH_1\times \tH_2)^{d_1\times d_2} \text{ and }\mP_i \in\mathbb{R}^{d_i\times r} \text{ for } i=1,2\}\\
    &=\{f\in \text{RKHS induced by matrix kernel } \mK \text{ with } \{\mH_{\mP_i},K_i\}_{i=1,2}\}.
\end{align}
The projection matrices $\{\mH_{\mP_i}\}_{i=1,2}$ play role in reducing the feature dimension and at the same time, with coefficients $\malpha$ find the best function that minimizes the classification risk. The procedure is summarized as the optimization over the union or RKHS induced by low rank weight matrices $\{\mW_i\}_{i=1,2,}$
\begin{align}
    \max_{f\in\tF}L(f)  = \max_{\substack{\text{rank}(\mW_i)\leq r,\\ \mW_i\succeq 0,i = 1,2}}\max_{\substack{f\in\text{RKHS} (\mK)\\ |\{\mW_i,K_i\}_{i=1,2}}}L(f).
\end{align}

\subsection{Level set estimation}
\label{subsec:pb2}
We propose weighted loss function from \eqref{eq:large-margin} to estimate the level set. 
\begin{align}
\label{eq:weighted}
\min_{f\in\tF}n^{-1}\sum_{i=1}^n \omega_{\pi}(y_i)L\left(y_if(\mX_i)\right)+ \lambda J(f),
\end{align}
where $\omega_\pi(y) = 1-\pi $ if $y = 1$ and $\pi$ if $y = -1$. The weighted loss accepts unequal costs for positive and negative misclassifications in margin classifier, where $\pi$ is the known cost for the negative and $1-\pi$ is for the positive classes. Notice that equal cost $\pi = \frac{1}{2}$ make \eqref{eq:weighted} reduce to \eqref{eq:large-margin}. 
The optimizer to Equation \eqref{eq:weighted} with respect to all measurable function class yields an consistent estimate of the Bayes rule $g_\pi(\mX) = \text{sign}\left(f_\pi(\mX)\right)$ with $f_\pi(\mX) = p(\mX) - \pi$ \citep{lin2002support,wang2008probability}. 
Therefore, under the considered decision function class as in Section \ref{sec:fcn class}, we obtain a minimizer $\hat f_\pi$ to \eqref{eq:weighted} and estimate the level set as
\begin{align}\label{eq:levelset}\hat S(\pi) = \{\mX:\mathbb{R}^{d_1\times d_2} : \text{sign}(\hat f_\pi(\mX)) = 1\}
.\end{align}


\subsection{Regression function estimation}
\label{subsec:pb3}
We propose a method to estimate  the regression function $p(\mX)\stackrel{\text{def}}{=}\mathbb{E}(y = 1|\mX)$ at any $\mX$ which does not necessarily belong to the observed training data set. Linearity in the candidate function space does not rule out nonlinear regression functions. In fact, non-smooth, non-continuous regression functions are allowed in our framework. Consider the following two steps of approximation to the target function.
\begin{align}
\label{eq:approx}
p(\mX) &\stackrel{\text{step1}}{\approx} \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX: p(\mX)\leq\frac{h}{H}\right\}\\&\hspace{.2cm}=\hspace{.2cm} \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX\notin S\left(\frac{h}{H}\right)\right\}
\\&\stackrel{\text{step2}}{\approx} \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX\notin\hat S\left(\frac{h}{H}\right)\right\}.
\end{align}
Step 1 approximates the target probability by linear combination of step functions where $H$ is a smooth parameter. In step 2, we plug in the level set estimation \eqref{eq:levelset} in Section \ref{subsec:pb2} given $\pi = h/H$. Here we use consistency of level set estimation.
Therefore, we estimate the regression function,\begin{align}
\hat p(\mX) = \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX\notin\hat S\left(\frac{h}{H}\right)\right\},
\end{align}
by repeatedly estimating the level sets in \eqref{eq:levelset} with different $\pi$ values, say  $\pi = \frac{h}{H}$ for $h = 1,\ldots, H$. 



\section{Algorithm}
\label{sec:alg}
In this Section, we describe the algorithm to seek the optimizer of Equation  \eqref{eq:large-margin} in the case of hinge loss function $L(z) = (1-z)_+$ and linear function class $\tF = \{f:f(\cdot)= \langle \mU\mV^T,\cdot\rangle, \text{ where }\mU\in\mathbb{R}^{d_1\times r}\mV\in\mathbb{R}^{d_2\times r}\}$.
Equation \eqref{eq:large-margin} is written as 
\begin{align}
\min_{\{(\mU,\mV)\in\mathbb{R}^{d_1\times r}\times \mathbb{R}^{d_2\times r}\}}n^{-1}\sum_{i=1}^n \left(1-y_i\langle \mU\mV^T,\mX_i\rangle\right)_++\lambda \FnormSize{}{\mU\mV^T}^2
\end{align}
We optimize Equation \eqref{eq:opt} with a coordinate descent algorithm that solves one block holding the other block fixed.  Each step is a convex optimization and can be solved with quadratic programming.
To be specific, when we fix $\mV$ and update $\mU$ we have the following equivalent dual problem 
\begin{align}
 \hspace{1.5cm}\max_{\malpha\in\mathbb{R}^n:\malpha\geq0}& \left(\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\langle \mX_i,\mX_j \mV(\mV^T\mV)^{-1}\mV^T\rangle\right)\\
    \text{subject to}&\quad\sum_{i=1}^Ny_i\alpha_i = 0,\quad0\leq\alpha_i\leq\frac{1}{2\lambda n},\quad i=1.\cdots,n,
    \end{align}
   We use quadratic programming to solve this dual problem and update $\mU = \sum_{i=1}^n\alpha_iy_i\mX_i\mV(\mV^T\mV)^{-1}.$
Similar approach is applied to update $\mV$ fixing $\mU$.  The Algorithm \ref{alg:linear} gives the full description.

 \begin{algorithm}[h]
 \label{alg:linear}
\KwIn{$(\mX_1,y_1),\cdots,(\mX_n,y_m)$, rank $r$}
{\bf Parameter:} U,V\\
{\bf Initizlize:} $\mU^{(0)}, \mV^{(0)}$\\
{\bf Do until converges}\\
\hspace*{.5cm}{\bf Update} $\mU$ fixing $\mV$ :\\[.1cm]
\hspace*{.4cm} Solve $ \max_{\malpha}\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n \sum_{j=1}^n\alpha_i\alpha_jy_iy_j\langle \mX_i,\mX_j\mV(\mV^T\mV)^{-1}\mV^T\rangle$.\\
\hspace{.5cm} $\mU = \sum_{i=1}^n\alpha_iy_i \mX_i\mV(\mV^T\mV)^{-1}$.\\[.1cm]
\hspace*{.5cm}{\bf Update} $\mV$ fixing $\mU$ :\\[.1cm]
\hspace*{.4cm} Solve  $ \max_{\malpha}\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\langle \mX_i,\mU(\mU^T\mU)^{-1}\mU^T\mX_j\rangle$.\\
\hspace{.5cm} $\mV = \sum_{i=1}^n\alpha_iy_i \mX_i^T\mU(\mU^T\mU)^{-1}$.\\[.1cm]
\KwOut{ $\mB = \mU\mV^T$}
    \caption{{\bf Linear classification algorithm} }
\label{alg:smm}
\end{algorithm}
\fixme{Miaoyan}{Add Algorithm 2 for Regression Estimation. Put it in an algorithm environment}

\section{Extension to nonlinear case}
\fixme{Miaoyan}{Possible to move this section to Sec 2.1?}

\label{sec:nonlinear}
We extend linear function class to non-linear class with kernel trick. We enlarge feature space through feature mapping $\mh:\mathbb{R}^{d_1\times d_2}\rightarrow \mathbb{R	}^{d_1\times d_2'}$. Once this mapping fixed, the procedure is the same as before.
We fit the linear classifier using pair of input feature and label $\{\mh(\mX_i),y_i\}_{i=1}^n$.
Define a nonlinear low rank kernel in similar way to linear case.
\begin{align}\label{eq:nonlinear kernel}
\langle \mX,\mX'\rangle_{\mP_r,h} &\stackrel{\text{def}}{=} \langle \mP_r h(\mX),\mP_r h(\mX')\rangle = \text{trace}\left[\mK(\mX,\mX')\mP_r^T\mP_r\right]\quad\text{ for all } \mX,\mX'\in\mathbb{R}^{d_1\times d_2},
\end{align}
where $\mK(\mX,\mX')\stackrel{\text{def}}{=} h(\mX)h^T(\mX')
\in \mathbb{R}^{d_1\times d_1}$ denotes the matrix product of mapped features.
The solution function $ f(\cdot)$ of \eqref{eq:large-margin} on enlarged feature can be written 
\begin{align}
f(\cdot) = \sum_{i=1}^n\alpha _i y_i \langle \mP_r h(\mX_i), \mP_r h(\cdot)\rangle =  \sum_{i=1}^n  \alpha _i y_i \langle \mX_i, \cdot\rangle_{ \mP_r,h}  =  \sum_{i=1}^n  \alpha _i y_i \text{trace}\left[\mK(\mX_i,\cdot)\ \mP_r^T \mP_r\right],
\end{align}
which involves feature mapping $h(\mX)$ only thorough inner products. In fact, we need not specify the the transformation $h(\mX)$ at all but only requires knowledge of the $
\mK(\mX,\mX')$. A sufficient condition and a necessary condition for $\mK$ being reasonable appear in Supplement.
Three popular choices for $\mK$ are
\begin{itemize}
\item Linear kernel: $\mK(\mX,\mX')=\mX\mX'^T$.
\item Polynomial kernel with degree $m$: $\mK(\mX,\mX')=(\mX\mX'^T+\lambda\mI)^{\circ m}$.
\item Gaussian kernel: the $(i,j)$-th entry of $\mK(\mX,\mX')$ is 
\[
\left[\mK(\mX,\mX')\right]_{(i,j)}=\exp\left\{-{1\over 2\sigma^2} \|\mX[i,\colon]-\mX'[j,\colon]\|_2^2\right\}
\]
for all $(i,j)\in[d_1]\times[d_1]$.
\end{itemize}
One can check detailed description for non-linear case algorithm in Supplement.
\section{Theory}
\label{sec:thm}
\section{Conclusion}
\label{sec:conc}


\bigskip
\begin{center}
{\large\bf SUPPLEMENTARY MATERIAL}
\end{center}


\bibliographystyle{chicago}
\bibliography{tensor_wang}

\end{document}
