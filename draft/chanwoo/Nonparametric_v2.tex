 \documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}
\newcommand*{\KeepStyleUnderBrace}[1]{%f
\mathop{%
\mathchoice
{\underbrace{\displaystyle#1}}%
{\underbrace{\textstyle#1}}%
{\underbrace{\scriptstyle#1}}%
{\underbrace{\scriptscriptstyle#1}}%
}\limits
}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{amsmath,amssymb,amsthm,bm,hyperref,enumerate}
\usepackage{dsfont,listings}

\usepackage[ruled,vlined]{algorithm2e}

\usepackage{enumitem}
\newtheorem{schm}{Scheme}
\newtheorem*{schm*}{Scheme}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{assumption}{Assumption}
\newtheorem{cor}{Corollary}[section]
\newtheorem{defn}{Definition}
\newtheorem{example}{Example}
\newtheorem{rmk}{Remark}
\newtheorem{clm}{Claim}
% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%
\usepackage{xcolor}

\input macros.tex
\begin{document}
\setcounter{secnumdepth}{3}
%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}
\def\fixme#1#2{\textbf{\color{red}[FIXME (#1): #2]}}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Model free probability estimation for matrix feature}
  \author{Author 1\thanks{
    The authors gratefully acknowledge \textit{please remember to list all relevant funding sources in the unblinded version}}\hspace{.2cm}\\
    Department of YYY, University of XXX\\
    and \\
    Author 2 \\
    Department of ZZZ, University of WWW}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Nonparametric learning with matrix-valued predictors in high dimensions}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
We consider the problem of learning the relationship between a binary label response and a high-dimensional matrix-valued predictor. Such data problems arise commonly in brain imaging studies, sensor network localization, and personalized medicine. Existing regression analysis often takes a parametric procedure by imposing a pre-specified relationship between variables. However, parametric models are insufficient in capturing complex regression surfaces defined over high-dimensional matrix space. Here, we propose a flexible nonparametric framework for various learning tasks, including classification, level set estimation, and regression, that specifically accounts for the matrix structure in the predictors. Unlike classical approaches, our method adapts to the possibly non-smooth, non-linear pattern in the regression function of interest. The proposal achieves prediction and interpretability simultaneously via a joint optimization of prediction rules and dimension reduction in the matrix space. Generalization bounds, estimation consistency, and convergence rate are established. We demonstrate the advantage of our method over previous approaches through simulations and applications to  {\color{red}XXX} data analyses. 
\end{abstract}

\noindent%
{\it Keywords:} Nonparametric learning, matrix-valued predictors, high dimension, classification, level-set estimation, regression.
\vfill


\newpage
\spacingset{1.5} % DON'T change the spacing!
\section{Introduction}
\label{sec:intro}

\section{Methods}
\label{sec:meth}
\setcounter{subsection}{-1}

Consider a statistical learning problem where we would like to model the relationship between a feature $\mX \in \tX$ and a response $Y\in\tY$. Suppose that we observe a sample of of $n$ data points, $(\mX_1,Y_1),\ldots,(\mX_n,Y_n)$, identically and independently distributed (i.i.d.) according to a unknown distribution $\mathbb{P}(\mX,Y)$ over $\tX\times \tY$. We are interested in predicting a new response $Y_{n+1}$ from a new feature value $\mX_{n+1}$. The observations $\{(\mX_i,Y_i)\}_{i=1}^n$ are called the training data and $(\mX_{n+1},Y_{n+1})$ the test point. When no confusion arises, we often omit the subscript $(n+1)$ and simply write $(\mX,Y)$ for the prototypical test point. The test point is assumed independent of the training data and is drawn from the same unknown distribution $\mathbb{P}$. Our goal is to make accurate prediction under a wide range of distributions. In particular, we consider a non-parametric, distribution-free setting with no strong assumptions on the data generative distribution other than i.i.d. 


We focus on the scenario with matrix-valued predictors and binary label response; that is, $\tX=\mathbb{R}^{d_1\times d_2}$ and $\tY=\{-1,1\}$. Matrix-valued predictors ubiquitously arise in modern applications. One example is from electroencephalography studies of alcoholism. The data set records voltage value measured from 64 channels of electrodes on 256 subjects for 256 time points~\citep{zhou2014regularized}. Each feature is a $256\times 64$ matrix and the response is a binary indicator of subject being alcoholic or control. Another example is pedestrian detection from image data. Each image is divided into 9 regions where local orientation statistics are generated with a total of 22 numbers per region. This yields a $22 \times 9$ matrix-valued feature and a binary label response indicating whether the image is pedestrian~\citep{Shashua2004PedestrianDF}. 

In the above two examples and many other studies, researchers are interested in \emph{interpretable prediction}, where the goal is to not only make accurate prediction but also identify features that are informative to the prediction. While classical learning algorithms have been successful in prediction with vector-valued predictors, the key challenge with matrix-valued predictors is the complex structure in the feature space. 
A naive approach is to transform the feature matrices to vectors and apply classical methods based on vectors to solve the problem. However, this vectorization would destroy the structural information of the data matrices. Moreover, the reshaping matrices to vectors results in high dimensionality which leads to overfitting. Notably, the ambient dimension with matrix-valued feature, $d_1d_2$, is often comparable to, or even larger than the number of sample, $n$. 
%Modern applications are often in the high dimensional regime where $d_1d_2$ is comparable to, or even larger, than the sample size $n$. 
Our method exploits the structural information in the data matrix to overcome these challenges. 


\subsection{Three main problems}
%Assume that we are given i.i.d. training samples $\{(\mX_1,y_1),\ldots,(\mX_n,y_n)\}$ drawn according to some unknown probability density function $\mathbb{P}_{\mX,y}(\mX,y)$ where  data matrix $\mX_i\in\mathbb{R}^{d_1\times d_2}$ and class labels $y_i\in\{+1,-1\}$. Define the regression function $p(\mX) = \mathbb{E}\left(\frac{y+1}{2}|\mX\right)$. 
Before we proceed with our proposal for matrix-valued features, we present the concrete learning problems of our interest. We consider three major supervised learning problems: classification, level set estimation, and regression estimation.

\begin{enumerate}[label={2.\arabic*}]
\setcounter{enumi}{1}
\item {\it The problem of classification}: Classification is the problem of identifying to which of a set of categories a new observation belongs, based on training samples. We aim to estimate a decision function $f:\mathbb{R}^{d_1\times d_2}\rightarrow \mathbb{R}$ that has small error
\begin{align}
    \mathbb{P}_{\mX,y}\left(y\neq g(\mX)\right),
\end{align}
where $g(\mX) = \text{sign}(f(\mX))$ is a decision rule. The classification problem has long been interested. Many attempts have been developed and performed well for example, decision tree, nearest neighbor, neural network and support vector machine to name a few. However, most of methods have focused on vector valued features. In many classification problems, the input features are naturally represented as matrices or tensors rather than vectors. 
%One example is a study of an electroencephalography data set of alcoholism. The data records voltage value measured from 64 channels of electrodes on 256 subjects for 256 time points, so each feature data is $256\times 64$ matrix and the response is binary indicator of subject being alcoholic or control \citep{zhou2014regularized}. Another example is pedestrian detection from image data. Each image was divided into 9 regions where local orientation statistics were generated with a total of 22 numbers per region, so each feature data is $22 \times 9$ matrix and the response is whether the image is pedestrian \citep{Shashua2004PedestrianDF}. 
We want to tackle matrix valued classification preserving the matrix structure.


\item {\it The problem of level set estimation}: The $\pi$-level set of $p$ given a fixed $\pi\in[0,1]$ is the set 
\begin{align}
S(\pi) = \{\mX\in\mathbb{R}^{d_1\times d_2}: p(\mX)>\pi\}.
\end{align}
 Accurate and efficient level set estimation plays an important role in many applications.
 One example can be found in medical decision making. In Osteosarcoma treatment, the degree of tumor necrosis is used to guide the choice of postoperative chemotherapy \citep{man2005expression}. Patients with $\geq 90 \%$ necrosis is labeled as 1, which is response variable $y$. Suppose that $\mX$ is a feature matrix collected from the patient such as gene expression levels on each tissue. Knowledge of the regression level set is needed to allow effective postoperative chemotherapy without a biopsy. 
 We consider a nonparametric way to estimate the $\pi$-level set of the regression function based on classification problem.
%Level set estimation plays crucial roles in many applications such as digital elevation maps, medical imaging and pattern recognition.
\item {\it The problem of regression estimation}: Regression function calculates expectation of $y$ given a feature matrix $\mX$ on the basis of a training set of data. In our setting, the regression $ \mathbb{E}(y|\mX)$  is equivalent to  the conditional probability $\mathbb{P}(y = 1|\mX)$ because the class label $y$ is binary. Knowledge about the class probability itself is of significant interest and can tell us the confidence of the outcome of classification. Traditionally, the regression problem is addressed 
through distribution assumption like logistic regression or linear discriminant analysis (LDA). In many applications, however, it is often difficult to  justify the assumptions made in logistic regression or satisfy the Gaussian assumption in LDA. These issues become more challenging for matrix features because of high dimensionality.
We establish distribution free method for estimating the regression function $p(\mX)$ based on level set estimation.  

\end{enumerate}
The three problems represent common learning tasks with increasing difficulties. 
Classification problem can be completed from level set $S(\frac{1}{2})$ utilizing Bayes rule. The level set estimation problem becomes trivial when we have all information about regression function.  
Accordingly, classical approach for the three problems is to find a solution for regression first, and address the other two based on the estimation. This is why the regression problem is also called soft classification. However, our approach finds classification rule first and address the level set estimation and regression problem in order. Through the sequence of solving the problems, we successfully solve the problems without assuming probability distribution.


\subsection{Choice of decision function space}\label{sec:fcn class}
The aforementioned three problems are formulated as empirical structure minimization over functions with matrices as inputs.
In this section, we introduce function classes we consider for the minimization problem. we propose a family of matrix kernels, which are building blocks for defining functions in matrix space. Kernel methods defined on non-vector objects have recently evolved into a rapidly developing branch of learning on structured data. Informally, a matrix kernel is a distance measure between two matrices with the same size using proper notion of similarity. Unlike vectors, matrix-valued inputs represent two-way relationship across rows and columns at a time. Taking into account of this two-way relationship is essential in the kernel development.
Our proposed kernel uses concept from latent factor models and incorporates the two-way similarities via low rank regularization. We start introducing linear function class in matrix space and generalize nonlinear one based on our proposed matrix kernel. 

\subsubsection{Linear function class}\label{subsec:linear class}
We propose the linear functions as a decision function class, $f(\mX) = \langle \mB,\mX\rangle,$
where $\mB,\mX\in\mathbb{R}^{d_1\times d_2}$ and $\langle \mX,\mX'\rangle = \text{Tr}(\mX^T\mX')$. The inner product is called Frobenius inner product in the space of matrices and is a generalization of the dot product from vector spaces.
We impose low-rankness on the linear predictor to consider the degeneracy of the coefficient matrix $\mB$ and leverage the structure information within the predictor $\mX$.
Specifically,  the coefficient matrix has low-rank $r$  usually much smaller than the matrix size  $\min(d_1,d_2)$,
\begin{align}\label{eq:lowrank}
\mB = \mC\mP^T \text{ where } \mC \in\mathbb{R}^{d_1\times r} ,\mP\in\mathbb{R}^{d_2\times r}\text{ and } r\leq\min(d_1,d_2).
\end{align}
The low-rankness makes distinction from classical classification problem in vector spaces and preserves structural information of feature matrices. The low rank constraint on coefficient matrix has been proposed in Support Vector Machine (SVM) classification problem \citep{pirsiavash2009bilinear,luo2015support} in matrix spaces. However these papers only focus on application to the classification problem. Our paper is not confined in classification but further apply the linear function class to level set estimation and regression estimations. Furthermore, we extend linear case to nonlinear case based on matrix kernel concept in the next section. There are some prior work on matrix/tensor kernels. Most of these methods, however, are inadaptive and do not use label information to guide the kernel construction. We propose a new matrix kernel that solves the limitation.

\subsubsection{Nonlinear function class}\label{subsec:nonlinear class}
We generalize classical kernel method in vector spaces to matrix spaces. We briefly summarize kernel method for vector features and introduce new notations and operations which will be used later.

It has been popular and successful to extend linear classifiers in vector space to nonlinear classifiers using kernel method. Classical linear classifier finds linear boundaries in the input vector feature space. By introducing feature mapping which maps input feature space to enlarged dimension space, learning nonlinear classifier becomes possible. In fact, we need not specify the feature mapping at all to obtain an optimal function that minimizes pre-determined loss function. Instead, the learning process only requires knowledge of the kernel function that computes inner products in the transformed enlarged space, thereby avoiding heavy computation. We generalize this kernel approach to the case when input feature is matrix-valued.


Before proposing a new matrix feature mapping and kernel, we introduce notations and operations needed later. Let $\phi_i\colon\mathbb{R}^{d_i}\rightarrow \tH_i$ be feature mappings with a classical kernel defined on vectors $K_i\colon\mathbb{R}^{d_i}\times \mathbb{R}^{d_i}\rightarrow \mathbb{R}$ for $i = 1,2.$ $\tH_i$ denotes enlarged feature space by $\phi_i$ and a possibly infinite dimensional Hilbert space. Let   $\tH^d = \tH^{1\times d}$ = $\{(\mx_1,\ldots,\mx_d)\colon x_{i}\in\tH,\text{ for } i = 1,\ldots, d.\}$ denote the collection of row vectors with each entry taking value in a Hilbert space $\tH$.  Matrix algebraic operations are carried over from operations on real valued matrices. One can check exact definitions of operations in Supplement.


Now we present a feature mapping and matrix kernel.
Matrix feature mapping is defined as follows.
\begin{defn}\label{def:map}
Let $\phi_1\colon\mathbb{R}^{d_1}\rightarrow \tH_1 $ and $\phi_2\colon\mathbb{R}^{d_2}\rightarrow \tH_2 $  be classical feature mappings defined on vector space. Then $\Phi$ is matrix feature mappings defined on $d_1$-by-$d_2$ matrices: 
\begin{align}\label{eq:featuremap}
    \Phi\colon\mathbb{R}^{d_1\times d_2}&\rightarrow \tH_1^{d_1}\times \tH_2^{d_2}\\
    \mX&\mapsto (\Phi_1(\mX),\Phi_2(\mX)) \stackrel{\text{def}}{=}\left(\left(\phi_1(\mX_{1:}),\ldots,\phi_1(\mX_{d_1:})\right),\left(\phi_2(\mX_{:1})\ldots,\phi(\mX_{:d_2})\right)\right).
\end{align}
\end{defn}
Notice that the matrix feature mapping considers both row-wise and column-wise enlarged features.
From the feature mapping, the linear function $f\colon \mathbb{R}^{d_1\times d_2}\rightarrow \mathbb{R}$ with respect to enlarged space $\Phi(\mX)\in\tH_1^{d_1}\times \tH_2^{d_2}$ is defined as,
\begin{align}\label{eq:linearfcn}
    f(\mX) &\stackrel{\text{def}}{=}\langle \mB,\Phi(\mX) \rangle ,\text{ where } \mB=(\mB_1,\mB_2)\in\tH_1^{d_1}\times \tH_2^{d_2}\\
    &= \langle \mB_1,\Phi_1(\mX)\rangle + \langle \mB_2,\Phi_2(\mX)\rangle.
\end{align}
These matrix valued feature mapping \eqref{eq:featuremap} and corresponding linear function \eqref{eq:linearfcn} are generalization from existing classical kernel method in vector spaces and can be extended naturally to tensor case (see Supplement for the details).
 We assume that the coefficient $\mB$ in \eqref{eq:linearfcn} admits low rank decomposition as in Section \ref{subsec:linear class}, 
 \begin{align}\label{eq:lowrk}
     \mB = \mC\mP^T, \text{ where } \mC = (\mC_1,\mC_2)\in \tH_1^{r}\times \tH_2^r \text{ and } \mP = (\mP_1,\mP_2)\in\mathbb{R}^{d_1\times r}\times \mathbb{R}^{d_2\times r}.
 \end{align}
  When feature mapping $\phi_i$ is identity for $i=1,2$ implying the linear case in Section \ref{subsec:linear class}, we show that considered linear functions \eqref{eq:linearfcn} with low-rank $r$ defined by \eqref{eq:lowrk} are equivalent to the linear functions in Section \ref{subsec:linear class} with the low-rank $r$ constraint \eqref{eq:lowrank}.
  Therefore, our matrix feature mapping is generalization of classical feature mapping on vector spaces and extension to nonlinear case from linear functions on matrix features.
  
Now we define matrix kernel associated with the matrix feature mapping.
\begin{defn}\label{def:kernel}
Let $K_i(\cdot,\cdot)$ be classical kernels which can be represented as $K_i(\cdot,\cdot) = \langle \phi_i(\cdot),\phi_i(\cdot)\rangle$ for $i=1,2$. Let weight matrices $\mW_i = \entry{w^{(i)}_{jk}}\in\mathbb{R}^{d_i\times d_i}$ be  rank-$r$ semi-positive definite matrices for $i = 1,2$. Then $\{\mW_i,K_i\}_{i=1,2}$  induce matrix kernel defined by
\begin{align}
    \mK\colon\mathbb{R}^{d_1\times d_2}\times \mathbb{R}^{d_1\times d_2}&\rightarrow \mathbb{R}\\
    (\mX,\mX')&\mapsto \mK(\mX,\mX')  = \sum_{j,k\in[d_1]}w^{(1)}_{jk}K_1(\mX_{j:},\mX'_{k:})+\sum_{j,k\in[d_2]}w^{(2)}_{jk}K_2(\mX_{:j},\mX'_{:k}).
\end{align}
\end{defn}
The matrix kernel incorporates classical kernel in vector spaces. Like classical kernel, we can associate the feature mapping in Definition \ref{def:map} with the matrix kernel. Given $\{\mW_i,K_i\}_{i=1,2}$, we have
\begin{align}\mK(\mX,\mX')& = \sum_{j,k\in[d_1]}w^{(1)}_{jk}K_1(\mX_{j:},\mX'_{k:})+\sum_{j,k\in[d_2]}w^{(2)}_{jk}K_2(\mX_{:j},\mX'_{:k}) \\
&=\langle \mW_1,\Phi_1(\mX)^T\Phi_1(\mX') \rangle + \langle \mW_2,\Phi_2(\mX)^T\Phi_2(\mX')\rangle\\
&= \langle \mW,\Phi(\mX)^T\Phi(\mX')\rangle, \text{ where } \mW = (\mW_1,\mW_2).
\end{align}
We can view the matrix kernel as weighted inner product of the feature mappings. From the kernel representation, we learn nonlinear function successfully  avoiding specification of feature mapping $\Phi(\mX)$ as in classical vector case given pre-specified row and column-wise kernels $K_1,K_2$. 

\subsection{Classification}
\label{subsec:pb1}
We estimate a decision function $f\colon\mathbb{R}^{d_1\times d_2}\rightarrow \mathbb{R}$ over function class introduced in Section \ref{sec:fcn class}. The decision function class denoted as $\tF$ can be either linear or nonlinear. 
We propose a large margin classifier that minimizes a cost function in $f$ over the function class $\tF$.
\begin{align}
\label{eq:large-margin}
\min_{f\in\tF}n^{-1}\sum_{i=1}^n L\left(y_i f(\mX_i)\right)+\lambda J(f),
\end{align}
where $J(f)$ is a regularization term for model complexity and $L(z)$ is a margin loss that is a function of the functional margin $yf(\mX)$. Examples of such loss functions are the hinge loss function $L(z) = (1-z)_+$ and the logistic loss function $L(z) =\log(1+e^{-z})$.  For demonstration, we focus on the hinge loss case in Equation \eqref{eq:large-margin}. However, our estimation schemes and theorems are applicable to general large-margin classifiers.

We present the solution to~\eqref{eq:large-margin} with nonlinear kernels which incorporate linear case. Based on the considered decision function class, we solve the following optimization problem.
\begin{align}
\label{eq:opteq}
(\hat \mC,\hat\mP) = \argmin_{\{\mC\in\tH_1^{r}\times \tH_1^{r} ,\mP\in \mathbb{R}^{d_1\times r}\times \mathbb{R}^{d_2\times r}\}}n^{-1}\sum_{i=1}^n \left(1-y_i\langle \mC\mP^T,\Phi(\mX_i)\rangle\right)_++\lambda \FnormSize{}{\mC\mP^T}^2.
\end{align}
Notice that the optimization problem \eqref{eq:opteq} degenerates to the conventional SVM with vectorized feature matrices when feature mapping is identity and the coefficient is full rank. From the solution to \eqref{eq:opteq}, our estimated classifier is written 
\begin{align}\label{eq:formulation}\hat g(\mX) = \text{sign}(\hat f(\mX)) = \text{sign}\left(\langle \hat\mC \hat\mP^T,\Phi(\mX)\rangle\right).\end{align}

We make a remark on the implication of the formulation \eqref{eq:formulation}.
The solution \eqref{eq:formulation} implies a joint learning of dimension reduction and classification risk minimization. This is one of our contribution to combine two different processes into one. To check this, we see the a dual representation of the solution to \eqref{eq:opteq},
\begin{align}\label{eq:dualrep}
f(\mX) &= \sum_{i=1}^n \alpha_i y_i\langle \Phi(\mX_i)\mH_{\mP} ,\Phi(\mX)\mH_{\mP}  \rangle,\text{ where } \mH_{\mP} = \mP(\mP^T\mP)^{-1}\mP^T\\
&=\sum_{i=1}^n\alpha_iy_i\left(\langle \Phi_1(\mX_i)\mH_{\mP_1},\Phi_1(\mX)\mH_{\mP_1}\rangle+\langle \Phi_2(\mX_i)\mH_{\mP_2},\Phi_2(\mX)\mH_{\mP_2}\rangle\right)\\
&= \sum_{i=1}^n\alpha_iy_i\left(\sum_{j,k\in[d_1]} [\mH_{\mP_1}]_{jk},K_1(\mX^{(i)}_{j:},\mX_{k:})+\sum_{j,k\in[d_1]} [\mH_{\mP_2}]_{jk},K_2(\mX^{(i)}_{:j},\mX_{:k})\right),
\end{align}
where $\{\alpha_i\}_{i=1}^n$ are  (sparse) dual solution to \eqref{eq:opteq} and $\mX^{(i)}_{jk}$ denotes (j,k)-entry of $\mX_i$. Notice that the last representation of \eqref{eq:dualrep} can be viewed as an element of reproducing kernel Hilbert space (RKHS) induced by matrix kernel with weight matrices $\{\mH_{\mP_i}\}_{i=1,2}$ and row and column-wise kernels $K_1,K_2$.
Therefore, our considered function space $\tF$ can be written as 
\begin{align}
    \tF &= \{f\colon\mX\mapsto\langle\mC\mP^T,\Phi(\mX)\rangle|\mC\in\tH_1^{d_1}\times \tH_2^{d_2} \text{ and }\mP = (\mP_1,\mP_2)\in\mathbb{R}^{d_1\times r} \times \mathbb{R}^{d_2\times r}\}\\
    &=\{f\in \text{RKHS induced by matrix kernel } \mK \text{ with } \{\mH_{\mP_i},K_i\}_{i=1,2}\}.
\end{align}
Given row and column-wise kernels, we estimate weight matrices and an element of RKHS induced by the estimated weight matrices.
The projection matrices $\{\mH_{\mP_i}\}_{i=1,2}$ play role in reducing  the feature dimension and at the same time, we find the best element of RKHS that minimizes the classification risk by estimating coefficients $\malpha$ in \eqref{eq:dualrep}. The procedure is summarized as the following optimization
\begin{align}
    \max_{f\in\tF}L(f)  = \max_{\substack{\text{rank}(\mW_i)\leq r,\\ \mW_i\succeq 0,i = 1,2}}\max_{\substack{f\in\text{RKHS} (\mK)\\ |\{\mW_i,K_i\}_{i=1,2}}}L(f).
\end{align}


\subsection{Level set estimation}
\label{subsec:pb2}
We propose weighted loss function from \eqref{eq:large-margin} to estimate the level set, 
\begin{align}
\label{eq:weighted}
\min_{f\in\tF}n^{-1}\sum_{i=1}^n \omega_{\pi}(y_i)L\left(y_if(\mX_i)\right)+ \lambda J(f),
\end{align}
where $\omega_\pi(y) = 1-\pi $ if $y = 1$ and $\pi$ if $y = -1$. The weighted loss accepts unequal costs for positive and negative misclassifications in margin classifier, where $\pi$ is the known cost for the negative and $1-\pi$ is for the positive classes. Notice that equal cost $\pi = \frac{1}{2}$ make \eqref{eq:weighted} reduce to \eqref{eq:large-margin}. 
The optimizer to Equation \eqref{eq:weighted} with respect to all measurable function class yields an consistent estimate of the Bayes rule $g_\pi(\mX) = \text{sign}\left(f_\pi(\mX)\right)$ where $f_\pi(\mX) = p(\mX) - \pi$ \citep{lin2002support,wang2008probability}. 
Therefore, under the considered decision function class, we obtain a minimizer $\hat f_\pi$ to \eqref{eq:weighted} and estimate the level set as
\begin{align}\label{eq:levelset}\hat S(\pi) = \{\mX\in\mathbb{R}^{d_1\times d_2} : \text{sign}(\hat f_\pi(\mX)) = 1\}
.\end{align}


\subsection{Regression function estimation}
\label{subsec:pb3}
We propose a method to estimate  the regression function $p(\mX)\stackrel{\text{def}}{=}\mathbb{E}(y = 1|\mX)$ at any $\mX$ which does not necessarily belong to the observed training data set. Non-smooth, non-continuous regression functions are allowed in our framework. Consider the following two steps of approximation to the target function.
\begin{align}
\label{eq:approx}
p(\mX) &\stackrel{\text{step1}}{\approx} \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX: p(\mX)\leq\frac{h}{H}\right\}\\&\hspace{.2cm}=\hspace{.2cm} \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX\notin S\left(\frac{h}{H}\right)\right\}
\\&\stackrel{\text{step2}}{\approx} \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX\notin\hat S\left(\frac{h}{H}\right)\right\}.
\end{align}
Step 1 approximates the target probability by linear combination of step functions where $H$ is a smooth parameter. In step 2, we plug in the level set estimation defined in \eqref{eq:levelset} given $\pi = h/H$. Here we use consistency of level set estimation.
Therefore, we estimate the regression function as,\begin{align}
\hat p(\mX) = \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX\notin\hat S\left(\frac{h}{H}\right)\right\},
\end{align}
by repeatedly estimating the level sets as \eqref{eq:levelset} with different $\pi$ values, say  $\pi = \frac{h}{H}$ for $h = 1,\ldots, H$. 



\section{Algorithm}
\label{sec:alg}
In this Section, we describe an algorithm to seek the optimizer of Equation  \eqref{eq:large-margin} in the case of hinge loss function $L(z) = (1-z)_+$. We consider nonlinear decision function class $\tF = \{f\colon\mX\mapsto\langle\mC \mP^T,\Phi(\mX)\rangle|\mC = (\mC_1,\mC_2)\in\tH_1^{d_1}\times \tH_2^{d_2} \text{ and }\mP = (\mP_1,\mP_2)\in\mathbb{R}^{d_1\times r} \times \mathbb{R}^{d_2\times r}\}$ given row and columnwise kernels $K_1,K_2$.
Notice Equation \eqref{eq:opteq} is written as 
\begin{align}
    \label{eq:opt}
    &\min_{\substack{\mC\in\tH_1^{d_1}\times \tH_2^{d_2} ,\\\mP\in\mathbb{R}^{d_1\times r} \times \mathbb{R}^{d_2\times r}}}\frac{1}{2}\FnormSize{}{\mC\mP^T}^2+ C\sum_{i=1}^n \xi_i,\\
    &\text{subject to } y_i\langle\mC\mP^T,\Phi(\mX_i)\rangle\leq 1-\xi_i\text{ and } \xi_i\geq 0, i=1,\ldots,n.
\end{align}
Optimization problem \eqref{eq:opt} is non-convex problem because low-rank constraint makes feasible set non-convex.
We propose to utilize coordinate descent algorithm that solves one block holding the other block fixed. From this approach, we can solve a convex problem in each step.
To be specific, first we update $\mC$ holding $\mP$ fixed.
The dual problem of Equation \eqref{eq:opt} with fixed $\mP$ is 
\begin{align}
    \label{eq:dual1}
    &\max_{\malpha = (\alpha_1,\ldots,\alpha_n)}-\sum_{i=1}^n\alpha_i + \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_j y_iy_j \langle \Phi(\mX_i)\mP(\mP^T\mP)^{-1}\mP^T,\Phi(\mX_j)\mP(\mP^T\mP)^{-1}\mP^T\rangle\\
    &\text{ subject to }  0\leq \alpha_i\leq C, i=1,\ldots,n.
\end{align}
We use quadratic programming to solve the dual problem and update $\mC$ as
\begin{align}\label{eq:C}
    \mC = \sum_{i=1}^n \alpha_iy_i \Phi(\mX_i) \mP(\mP^T\mP)^{-1}\in\tH_r^r\times \tH_c^r.
\end{align}
We use the formula \eqref{eq:C} without information about feature mapping $\Phi(\cdot)$.
Second, we assume that $\mC$ is fixed and update $\mP$. The dual problem of Equation \eqref{eq:opt} with fixed $\mC$ is 
\begin{align}
    \label{eq:dual2}
    &\max_{\malpha = (\alpha_1,\ldots,\alpha_n)}-\sum_{i=1}^n\alpha_i + \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_j y_iy_j\langle 
    \mC\left((\mC^T\mC)^{-1}\mC^T\Phi(\mX_i)\right),\mC\left((\mC^T\mC)^{-1}\mC^T\Phi(\mX_j)\right)\rangle,\\
    &\text{ subject to }  0\leq \alpha_i\leq C, i=1,\ldots,n,
\end{align}
We can find an optimizer of \eqref{eq:dual2} based on kernel information only. We obtain the following formula by plugging \eqref{eq:C} into components of \eqref{eq:dual2}. 
\begin{align}\label{eq:helpeq}
 &\mC^T\mC = \sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_jy_iy_j(\mP^T\mP)^{-1}\mP^T\mK(i,j) \mP(\mP^T\mP)^{-1}\in\mathbb{R}^{r\times r}\times \mathbb{R}^{r\times r},\\
 &\mC^T\Phi(\mX_i) = \sum_{j=1}^n \alpha_iy_i(\mP^T\mP)^{-1}\mP^T\mK(i,j)\in\mathbb{R}^{r\times d_1}\times \mathbb{R}^{r\times d_2},
\end{align}
where $\mK(i,j) \stackrel{\text{def}}{=} \left( \Phi_1(\mX_i)^T\Phi_1(\mX_j),\Phi_2(\mX_i)^T\Phi_2(\mX_j)\right)\in \mathbb{R}^{d_1\times d_1}\times \mathbb{R}^{d_2\times d_2}$. Notice that $[\Phi_1(\mX_i)^T
\Phi_1(\mX_j)]_{ss'} = K_1(\mX^{(i)}_{s:},\mX^{(j)}_{s':})$ and vice versa for $\Phi_2(\cdot)$.
Therefore, we update $\mP$  from an optimal coefficient $\malpha$ to \eqref{eq:dual2} without specifying feature mapping.
\begin{align}
    \mP = \sum_{i=1}^n\alpha_iy_i(\mC^T\mC)^{-1}\mC^T\Phi(\mX_i).
\end{align}
We end up obtaining nonlinear function output of the form,
\begin{align}\label{eq:output}
    \hat f(\mX) = \sum_{k=1}^n\hat\alpha_ky_k&\bigg(\sum_{i=1}^{d_1}\sum_{j=1}^{d_1}[\hat\mP_1(\hat\mP_1^T\hat\mP_1)^{-1}\hat\mP_1^T]_{ij}K_r\left([\mX_k]_{i:},[\mX]_{j:}\right)\\ &+\sum_{i=1}^{d_2}\sum_{j=1}^{d_2}[\hat\mP_2(\hat\mP_2^T\hat\mP_2)^{-1}\hat\mP_2^T]_{ij}K_c\left([\mX_k]_{:i},[\mX]_{:j}\right)\bigg).
\end{align}
Algorithm \ref{alg:svm} gives the full description for classification. 

By the similar way with little modification, we can obtain an algorithm for weighted margin classifier \eqref{eq:weighted}. From the explanation in Section \ref{subsec:pb2} and \ref{subsec:pb3}, we summarize level set and regression estimation procedure in Algorithm \ref{alg:regest}.
 \begin{algorithm}[h]
 \label{alg:svm}
\KwIn{$(\mX_1,y_1),\cdots,(\mX_n,y_m)$, rank $r$, and pre-specified kernels $K_1,K_2$}
{\bf Initizlize:} $\mP^{(0)}\in\mathbb{R}^{d_1\times r}\times \mathbb{R}^{d_2\times r}$\\
{\bf Do until converges}\\
\hspace*{.5cm}{\bf Update} $\mC$ fixing $\mP$ :\\[.1cm]
\hspace*{.4cm} Solve $\max_{\malpha}-\sum_{i=1}^n\alpha_i + \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_j y_iy_j \langle \Phi(\mX_i),\Phi(\mX_j)\mP(\mP^T\mP)^{-1}\mP^T\rangle$\\
\hspace{.5cm} $\mC = \sum_{i=1}^n \alpha_iy_i \Phi(\mX_i) \mP(\mP^T\mP)^{-1}$.\\[.1cm]
\hspace*{.5cm}{\bf Update} $\mP$ fixing $\mC$ :\\[.1cm]
\hspace*{.4cm} Solve  $ \max_{\malpha}-\sum_{i=1}^n\alpha_i + \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_j y_iy_j\langle 
    \Phi(\mX_i),\mC\left((\mC^T\mC)^{-1}\mC^T\Phi(\mX_j)\right)\rangle$.\\
\hspace{.5cm} $\mP = \sum_{i=1}^n\alpha_iy_i(\mC^T\mC)^{-1}\mC^T\Phi(\mX_i)$.\\[.1cm]
\KwOut{ $\hat f$ of the form \eqref{eq:output}}
    \caption{{\bf Classification algorithm} }
\end{algorithm}





 \begin{algorithm}[h]
 \label{alg:regest}
\KwIn{$(\mX_1,y_1),\cdots,(\mX_n,y_m)$, rank $r$,pre-specified kernels $K_1,K_2$, and smooth parameter $H$.}
{\bf Initialize:} $\pi_h = (h-1)/H$ for $h = 1, \ldots, H+1$\\
{\bf For $h = 1,\ldots, H+1$:}\\
\hspace*{.5cm}{\bf Level set $\hat S(\pi_h)$ estimation:}\\
\hspace*{1cm}{\bf Train} weighted margin classifier $\hat f_{\pi_h}$ from \eqref{eq:weighted} based on Algorithm \ref{alg:svm}.
\\[.1cm]
\hspace*{.9cm} $\hat S(\pi_h) = \{\mX\in\mathbb{R}^{d_1\times d_2}:\text{sign}(\hat f_{\pi_h}(\mX)) = 1\}.$
\\[.1cm]
{\bf Regression $\hat p(\mX)$ estimation:} \\[.1cm]
\hspace*{.4cm} $\hat p(\mX) = \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX\notin\hat S\left(\pi_h\right)\right\}.$\\[.1cm]
\KwOut{ Level sets $\hat S(\pi_h)$ for $h =1,\ldots H$ and regression function $\hat p(\mX)$.}
    \caption{{\bf Level set \& Regression Algorithm} }
\end{algorithm}



\section{Theory}
\label{sec:thm}
\section{Conclusion}
\label{sec:conc}


\bigskip
\appendix
\begin{center}
{\large\bf SUPPLEMENTARY MATERIAL}
\end{center}
\section{Hilbert-space valued matrix operations}
\begin{prop}
    Let $\mA = \entry{a_{i}}$ and $\mB = \entry{b_{i}}$ be two vectors in $\tH_1^{d_1}$ and $\mA' = \entry{a'_{i}}$ be a vector in $\tH_2^{d_2}$.  Let $\mP\in\mathbb{R}^{d_1\times r}$ and $\mP'\in\mathbb{R}^{d_2\times r}$ be real valued matrices. Then, we have well defined operations.
    \begin{itemize}
    \item Inner product: $ \langle \mA,\mB \rangle = \sum_{i}\langle a_{i},b_{i}\rangle\in\mathbb{R}.$
    \item Linear combination: $\mA\mP = \entry{c_{i}}\in\tH_1^{r}$ where $c_{i} = \sum_{k\in[d_1]}a_{k}p_{ki}$ for all $i\in[r]$.
    \item Summation:  $\mA+\mB = \entry{a_{i}+b_{i}}\in\tH_1^{d_1}.$
    \item Matrix product: $\mA^T\mB = \entry{c_{ij}}\in\mathbb{R}^{r\times r}$, where $c_{ij} = \langle a_{i},b_{j}\rangle,$ for all $i,j\in[r].$ 
    \item Tuple operation: $(\mA,\mA')(\mP,\mP') =(\mA\mP,\mA'\mP')\in \tH_1^r\times \tH_2^r. $
\end{itemize}
\end{prop}



\bibliographystyle{chicago}
\bibliography{tensor_wang}

\end{document}
