\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}
\newcommand*{\KeepStyleUnderBrace}[1]{%f
\mathop{%
\mathchoice
{\underbrace{\displaystyle#1}}%
{\underbrace{\textstyle#1}}%
{\underbrace{\scriptstyle#1}}%
{\underbrace{\scriptscriptstyle#1}}%
}\limits
}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{amsmath,amssymb,amsthm,bm,hyperref,enumerate}
\usepackage{dsfont,listings}

\usepackage[ruled,vlined]{algorithm2e}

\usepackage{enumitem}
\newtheorem{schm}{Scheme}
\newtheorem*{schm*}{Scheme}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%

\input macros.tex
\begin{document}
\setcounter{secnumdepth}{3}
%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Model free probability estimation for matrix feature}
  \author{Author 1\thanks{
    The authors gratefully acknowledge \textit{please remember to list all relevant funding sources in the unblinded version}}\hspace{.2cm}\\
    Department of YYY, University of XXX\\
    and \\
    Author 2 \\
    Department of ZZZ, University of WWW}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Large margin based regression for matrix feature}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
The text of your abstract. 200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:}  3 to 6 keywords, that do not appear in the title
\vfill

\newpage
\spacingset{1.5} % DON'T change the spacing!
\section{Introduction}
\label{sec:intro}

\section{Methods}
\label{sec:meth}
\setcounter{subsection}{-1}
\subsection{Three main problems}
Assume that we are given a set of training data $\{(\mX_1,y_1),\ldots,(\mX_n,y_n)\}$ with data matrix $\mX_i\in\mathbb{R}^{d_1\times d_2}$ and class labels $y_i\in\{+1,-1\}$ drawn according to some unknown probability density function $p_{\mX,y}(\mX,y)$. Define $p(\mX) = \mathbb{E}(y|\mX)$. 
We consider three major problems on this setting: classification, level set estimation, and regression estimation.

\begin{enumerate}[label={2.\arabic*}]
\item {\it The problem of classification}: We aim to address learning a model with a low error on the training data. One successful approach is large-margin classifier. We consider  a large margin classifier that minimizes a cost function in $f$ over a decision function class $\tF$
\begin{align}
\label{eq:large-margin}
\min_{f\in\tF}n^{-1}\sum_{i=1}^n L\left(y_i f(\mX_i)\right)+\lambda J(f),
\end{align}
where $J(f)$ is a regularization term for model complexity and $L(z)$ is a margin loss that is a function of the functional margin $yf(\mX)$. Examples of such loss functions are the hinge loss function $L(z) = (1-z)_+$ and the logistic loss function $L(z) =\log(1+e^{-z})$.  


\item {\it The problem of level set estimation}: The $\pi$-level set of $p$ given a fixed $\pi\in[0,1]$ is the set 
\begin{align}
S^*= S(p,\pi) = \{\mX\in\mathbb{R}^{d_1\times d_2}: p(\mX)>\pi\}.
\end{align}
We consider the problem of estimating the $\pi$-level set 
of the regression function without assuming any certain models but utilizing the large-margin classifier. 
%Level set estimation plays crucial roles in many applications such as digital elevation maps, medical imaging and pattern recognition.
\item {\it The problem of regression estimation}: Based on level set estimation problem, we aim to establish a model-free method for  estimating $p(\mX) = \mathbb{E}(y|\mX)$.  In our setting, the regression $ \mathbb{E}(y|\mX)$  is equivalent to  the conditional probability $\mathbb{P}(y = 1|\mX)$ because the class label $y$ is binary.  This problem is also known as soft classification, since the estimated probability can be used to determine the classification boundary.
\end{enumerate}

When utilizing classical methods based on feature vectors to solve the above problems, we have to transform the feature matrices to vectors. However, this vectorization would destroy the structural information of the data matrices. Moreover, the reshaping matrices to vectors results in very high dimensionality which leads to overfitting problem.

We propose a methodology for those problems. Our method exploits the structural information of the data matrix. We take advantage of low-rank assumption to describe the correlation within a matrix and overcome overfitting problem.
 
\subsection{Classification}
\label{subsec:pb1}
It is well known that the hinge loss enjoys sparseness and robustness, which are two desirable properties for a good classifiers. For demonstration, we focus on the hinge loss case in Equation \eqref{eq:large-margin}. However, our estimation schemes and theorems are applicable to general large-margin classifiers.
We propose a linear predictor with low-rank coefficient matrix  of the form $f_{\mB}(\mX) = \langle \mB,\mX\rangle,$
where $\mB,\mX\in\mathbb{R}^{d_1\times d_2}$ and $\langle \mX,\mX'\rangle = \text{Tr}(\mX^T\mX')$. We can extend the linear model to non-linear case in Section \ref{sec:nonlinear}.
We impose low-rank structure on the coefficient matrix $\mB$,
\begin{align}
\mB = \mU\mV^T \text{ where } \mU \in\mathbb{R}^{d_1\times r} ,\mV\in\mathbb{R}^{d_2\times r}\text{ and } r\leq\min(d_1,d_2)
\end{align}
 We  see that the condition provides a reasonable trade-off between model complexity and model flexibility. This low-rankness makes distinction from classical classification problem for feature vectors and considers structural information of feature matrices. Based on the considered function class, we present the following formulation from Equation \eqref{eq:large-margin}:
\begin{align}
\label{eq:opt}
\min_{\{(\mU,\mV)\in\mathbb{R}^{d_1\times r}\times \mathbb{R}^{d_2\times r}\}}n^{-1}\sum_{i=1}^n \left(1-y_i\langle \mU\mV^T,\mX_i\rangle\right)_++\lambda \FnormSize{}{\mU\mV^T}^2
\end{align}
Notice that when the rank of the coefficient matrix is full rank, the optimization problem \eqref{eq:opt} degenerates to the conventional linear SVM with vectorized feature matrices.

We show that the solution of \eqref{eq:opt} has the form  (one can check Supplement for the details)
\begin{align}\label{eq:form}
f(\mX) = \sum_{i=1}^n \alpha_i y_i\langle \mP_r\mX_i, \mP_r \mX\rangle,
\end{align}
where $\{\alpha_i\}_{i=1}^n$ are  solution  (spare)  of the dual problem in \eqref{eq:opt} and $\mP_r\in\mathbb{R}^{r\times d_1}$ is the projection matrix induced by low rank coefficient $\mU,\mV$. Let $\langle \cdot,\cdot\rangle_{\mP_r}$ denote the low rank linear kernel for a pair of matrices:
\begin{align}\label{eq:linear kernel}
\langle \mX,\mX'\rangle_{\mP_r} \stackrel{\text{def}}{=}\langle \mP_r\mX,\mP_r\mX'\rangle, \quad\text{ for all } \mX,\mX'\in\mathbb{R}^{d_1\times d_2}.
\end{align}
The solution function $f(\cdot)$ can be written $f(\cdot) = \sum_{i=1}^n \alpha_iy_i\langle\mX_i,\cdot\rangle_{\mP_r},$ which can be viewed as an element in the reproducing kernel Hilbert space induced by rank-$r$ linear kernels $\{\langle \cdot,\cdot\rangle_{\mP}: \mP\in\mathbb{R}^{r\times d_1}\text{ is a projection matrix} \}$.  
Therefore, we estimate the optimal hyperplane that separate the training data the best on projected feature matrices.  The classification rule $\hat G(\mX)$ can be written as \begin{align}
\hat G(\mX) = \text{sign}\left(\sum_{i=1}^n \hat \alpha_i y_i\langle \mX_i,\mX\rangle_{\hat \mP_r}\right),
\end{align}
where  $\{\hat \alpha_i\}_{i=1}^n$ are estimated coefficients and $\hat \mP_r$ is an estimated projection matrix.
The detailed algorithm for the estimation will appear in Section \ref{sec:alg}. 

\subsection{Level set estimation}
\label{subsec:pb2}
Since the large-margin classifier showed good performance in classification, one might expect to extract any information about level set $S(p,\pi)$ from the large-margin classifier. \citet{lin2002support} showed that the solution $\hat f$ to \eqref{eq:large-margin} targets directly at $\text{sign}\left(p(\mX)-\frac{1}{2}\right)$. Moreover, \citet{wang2008probability} proved that the minimizer $\hat f_\pi$ to $\pi$-weighted loss function is a consistent estimate of $\text{sign}(p(\mX)-\pi)$. 
Therefore, we solve the regularization problem with weighed loss function from  \eqref{eq:large-margin},
\begin{align}
\label{eq:weighted}
\min_{f\in\tF}n^{-1}\sum_{i=1}^n \omega_{\pi}(y_i)L\left(y_if(\mX_i)\right)+ \lambda J(f),
\end{align}
where $\omega_\pi(y) = 1-\pi $ if $y = 1$ and $\pi$ if $y = -1$.  From the solution $\hat f_\pi$ to \eqref{eq:weighted}, we estimate the level set $\hat S(p,\pi) = \{\mX:\mathbb{R}^{d_1\times d_2} : \text{sign}(\hat f_\pi(\mX)) = 1\}$.


\subsection{Regression function estimation}
\label{subsec:pb3}
Our proposed method is designed to estimate  the regression function $p(\mX)\stackrel{\text{def}}{=}\mathbb{E}(y = 1|\mX)$ at any $\mX$ which does not necessarily belong to the observed training data set. Consider the following two steps of approximation to the target function.
\begin{align}
\label{eq:approx}
p(\mX) &\stackrel{\text{step1}}{\approx} \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX: p(\mX)\leq\frac{h}{H}\right\}\\&\stackrel{\text{step2}}{\approx} \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX\in\hat S^c(p,h/H)\right\},
\end{align}
where $\hat S(p,\pi) = \{\mX:\mathbb{R}^{d_1\times d_2} : \text{sign}(\hat f_\pi(\mX)) = 1\}$.
Step 1 approximates the target probability by linear combination of step functions and Step 2 uses the fact that estimated level set in Section \ref{subsec:pb2} is consistent.
The probability estimation scheme can be summarized as follows.\\
{\bf Shceme}\vspace{-.4cm}
\begin{enumerate}[label={S.\arabic*}]
\item Choose a sequence of weight $\pi_h = \frac{h}{H}$, for $h = 1,\ldots, H$.
\item For each weight $\pi_h\in[0,1]$, solve Equation \eqref{eq:weighted} with $\omega_{\pi_h}(y)$
\item Denote the sequence of solutions and estimated level sets
\begin{align}
\{\hat f_h\}_{h=1}^H \text{ and } \{\hat S(p,h/H)\}_{h=1}^H.
\end{align}
\item Estimate the target probability function by 
\begin{align}
\hat p(\mX) = \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX\in\hat S^c(p,h/H)\right\}.
\end{align}
\end{enumerate}



\section{Algorithm}
\label{sec:alg}
In this Section, we describe the algorithm to seek the optimizer of Equation  \eqref{eq:large-margin} in the case of hinge loss function $L(z) = (1-z)_+$ and linear function class $\tF = \{f:f(\cdot)= \langle \mU\mV^T,\cdot\rangle, \text{ where }\mU\in\mathbb{R}^{d_1\times r}\mV\in\mathbb{R}^{d_2\times r}\}$.
Equation \eqref{eq:large-margin} is written as 
\begin{align}
\min_{\{(\mU,\mV)\in\mathbb{R}^{d_1\times r}\times \mathbb{R}^{d_2\times r}\}}n^{-1}\sum_{i=1}^n \left(1-y_i\langle \mU\mV^T,\mX_i\rangle\right)_++\lambda \FnormSize{}{\mU\mV^T}^2
\end{align}
We optimize Equation \eqref{eq:opt} with a coordinate descent algorithm that solves one block holding the other block fixed.  Each step is a convex optimization and can be solved with quadratic programming.
To be specific, when we fix $\mV$ and update $\mU$ we have the following equivalent dual problem 
\begin{align}
 \hspace{1.5cm}\max_{\malpha\in\mathbb{R}^n:\malpha\geq0}& \left(\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\langle \mX_i,\mX_j \mV(\mV^T\mV)^{-1}\mV^T\rangle\right)\\
    \text{subject to}&\quad\sum_{i=1}^Ny_i\alpha_i = 0,\quad0\leq\alpha_i\leq\frac{1}{2\lambda n},\quad i=1.\cdots,n,
    \end{align}
   We use quadratic programming to solve this dual problem and update $\mU = \sum_{i=1}^n\alpha_iy_i\mX_i\mV(\mV^T\mV)^{-1}.$
Similar approach is applied to update $\mV$ fixing $\mU$.  The Algorithm \ref{alg:linear} gives the full description.

 \begin{algorithm}[h]
 \label{alg:linear}
\KwIn{$(\mX_1,y_1),\cdots,(\mX_n,y_m)$, rank $r$}
{\bf Parameter:} U,V\\
{\bf Initizlize:} $\mU^{(0)}, \mV^{(0)}$\\
{\bf Do until converges}\\
\hspace*{.5cm}{\bf Update} $\mU$ fixing $\mV$ :\\[.1cm]
\hspace*{.4cm} Solve $ \max_{\malpha}\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n \sum_{j=1}^n\alpha_i\alpha_jy_iy_j\langle \mX_i,\mX_j\mV(\mV^T\mV)^{-1}\mV^T\rangle$.\\
\hspace{.5cm} $\mU = \sum_{i=1}^n\alpha_iy_i \mX_i\mV(\mV^T\mV)^{-1}$.\\[.1cm]
\hspace*{.5cm}{\bf Update} $\mV$ fixing $\mU$ :\\[.1cm]
\hspace*{.4cm} Solve  $ \max_{\malpha}\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\langle \mX_i,\mU(\mU^T\mU)^{-1}\mU^T\mX_j\rangle$.\\
\hspace{.5cm} $\mV = \sum_{i=1}^n\alpha_iy_i \mX_i^T\mU(\mU^T\mU)^{-1}$.\\[.1cm]
\KwOut{ $\mB = \mU\mV^T$}
    \caption{{\bf Linear classification algorithm} }
\label{alg:smm}
\end{algorithm}

\section{Extension to nonlinear case}
\label{sec:nonlinear}
We extend linear function class to non-linear class with kernel trick. We enlarge feature space through feature mapping $\mh:\mathbb{R}^{d_1\times d_2}\rightarrow \mathbb{R	}^{d_1\times d_2'}$. Once this mapping fixed, the procedure is the same as before.
We fit the linear classifier using pair of input feature and label $\{\mh(\mX_i),y_i\}_{i=1}^n$.
Define a nonlinear low rank kernel in similar way to linear case.
\begin{align}\label{eq:nonlinear kernel}
\langle \mX,\mX'\rangle_{\mP_r,h} &\stackrel{\text{def}}{=} \langle \mP_r h(\mX),\mP_r h(\mX')\rangle = \text{trace}\left[\mK(\mX,\mX')\mP_r^T\mP_r\right]\quad\text{ for all } \mX,\mX'\in\mathbb{R}^{d_1\times d_2},
\end{align}
where $\mK(\mX,\mX')\stackrel{\text{def}}{=} h(\mX)h^T(\mX')
\in \mathbb{R}^{d_1\times d_1}$ denotes the matrix product of mapped features.
The solution function $ f(\cdot)$ of \eqref{eq:large-margin} on enlarged feature can be written 
\begin{align}
f(\cdot) = \sum_{i=1}^n\alpha _i y_i \langle \mP_r h(\mX_i), \mP_r h(\cdot)\rangle =  \sum_{i=1}^n  \alpha _i y_i \langle \mX_i, \cdot\rangle_{ \mP_r,h}  =  \sum_{i=1}^n  \alpha _i y_i \text{trace}\left[\mK(\mX_i,\cdot)\ \mP_r^T \mP_r\right],
\end{align}
which involves feature mapping $h(\mX)$ only thorough inner products. In fact, we need not specify the the transformation $h(\mX)$ at all but only requires knowledge of the $
\mK(\mX,\mX')$. A sufficient condition and a necessary condition for $\mK$ being reasonable appear in Supplement.
Three popular choices for $\mK$ are
\begin{itemize}
\item Linear kernel: $\mK(\mX,\mX')=\mX\mX'^T$.
\item Polynomial kernel with degree $m$: $\mK(\mX,\mX')=(\mX\mX'^T+\lambda\mI)^{\circ m}$.
\item Gaussian kernel: the $(i,j)$-th entry of $\mK(\mX,\mX')$ is 
\[
\left[\mK(\mX,\mX')\right]_{(i,j)}=\exp\left\{-{1\over 2\sigma^2} \|\mX[i,\colon]-\mX'[j,\colon]\|_2^2\right\}
\]
for all $(i,j)\in[d_1]\times[d_1]$.
\end{itemize}
One can check detailed description for non-linear case algorithm in Supplement.
\section{Theory}
\label{sec:thm}
\section{Conclusion}
\label{sec:conc}


\bigskip
\begin{center}
{\large\bf SUPPLEMENTARY MATERIAL}
\end{center}


\bibliographystyle{chicago}
\bibliography{tensor_wang}

\end{document}
