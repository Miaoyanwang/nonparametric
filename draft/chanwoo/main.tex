%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% A template for Wiley article submissions.
% Developed by Overleaf. 
%
% Please note that whilst this template provides a 
% preview of the typeset manuscript for submission, it 
% will not necessarily be the final publication layout.
%
% Usage notes:
% The "blind" option will make anonymous all author, affiliation, correspondence and funding information.
% Use "num-refs" option for numerical citation and references style.
% Use "alpha-refs" option for author-year citation and references style.

\documentclass[mathptm]{statsoc}
\usepackage[a4paper]{geometry}

\usepackage[utf8]{inputenc}
\usepackage{lipsum}

\usepackage{amsmath,amssymb}
\usepackage{natbib}



\let\proof\relax
\let\endproof\relax 

\usepackage{amsmath}
\usepackage{graphicx}
\newcommand{\blind}{0}
\newcommand*{\KeepStyleUnderBrace}[1]{%f
\mathop{%
\mathchoice
{\underbrace{\displaystyle#1}}%
{\underbrace{\textstyle#1}}%
{\underbrace{\scriptstyle#1}}%
{\underbrace{\scriptscriptstyle#1}}%
}\limits
}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{amsmath,amssymb,amsthm,bm,hyperref}
\usepackage{dsfont,listings}


\usepackage[ruled,vlined]{algorithm2e}

\usepackage{enumitem}
\newtheorem{schm}{Scheme}
\newtheorem*{schm*}{Scheme}
\newtheorem{thm}{Theorem}[section]
\newtheorem{lem}{Lemma}
\newtheorem{prop}{Proposition}
\newtheorem{pro}{Property}
\newtheorem{assumption}{Assumption}
\newtheorem{cor}{Corollary}[section]
\newtheorem{defn}{Definition}
\newtheorem{exam}{Example}
\newtheorem{rmk}{Remark}
\newtheorem{clm}{Claim}

\usepackage{xcolor}

\input macros.tex

\setcounter{secnumdepth}{3}
%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}
\def\fixme#1#2{\textbf{\color{red}[FIXME (#1): #2]}}


\usepackage{hyperref}
\usepackage[parfill]{parskip}
\usepackage{bm}


\newcommand{\Hnorm}[1]{\left\lVert#1\right\rVert_{\tH_\alpha}}
\newcommand{\nullnorm}[1]{\left\lVert#1\right\rVert}
\def\trueB{\mB^{\text{true}}}
\def\newX{\mX_{\textup{new}}}
\def\newy{y_{\textup{new}}}
\def\sign{\textup{sign}}
\def\bayesf{f_{\textup{bayes}}}


\title[Nonparametric learning with matrix predictors]{Nonparametric learning with matrix-valued predictors in high dimensions}


\author[Lee {\it et al.}]{Chanwoo Lee$^{1}$, Lexin Li$^2$, Hao Helen Zhang$^3$, and Miaoyan Wang$^1$}
\address{$^1$Department of Statistics, University of Wisconsin-Madison}
\address{$^2$Department of Biostatistics and Epidemiology, University of California at Berkeley}
\address{$^3$Department of Mathematics, University of Arizona}


\begin{document}

\maketitle

\begin{abstract}
We consider the problem of learning the relationship between a binary label response and a high-dimensional matrix-valued predictor. Such data problems are important in brain imaging studies, sensor network localization, and personalized medicine. Existing regression analysis typically takes a parametric procedure by imposing a pre-specified relationship between variables. Parametric models, however, often perform poorly for misspecified models and for high-dimensional problems, especially in the high dimension, low sample size data. Here, we propose a flexible nonparametric framework for various learning tasks, including classification, level set estimation, and regression, that specifically accounts for the matrix-valued predictors. Unlike classical approaches, our method is distribution-free, allows exponential growth of feature dimension with sample size, and adapts to the possibly non-smooth, non-linear regression function of interest. The proposal achieves \emph{predictive prediction} via a structural risk minimization within a low-rank, sparse model space. Generalization bounds, estimation consistency, and convergence rate are established. We demonstrate the advantage of our method over previous approaches through simulations and applications to human brain connectome data analyses. 
\end{abstract}

\keywords{Nonparametric learning, high-dimensional matrix-valued predictors, sparse and low-rank models, classification, regression, feature selection}


\section{Introduction}
Consider a predictive learning setting where we would like to model the relationship between a matrix-valued predictor $\mX \in \mathbb{R}^{d\times d}$ and a binary label response $Y\in\{0,1\}$. Matrix-valued predictors ubiquitously arise in modern applications. One example is from electroencephalography studies of alcoholism. The data set records voltage value measured from 64 channels of electrodes on 256 subjects for 256 time points~\citep{zhou2014regularized}. Each feature is a $256\times 64$ matrix and the response is a binary indicator of subject being alcoholic or control. Another example is pedestrian detection from image data. Each image is divided into 9 regions where local orientation statistics are generated with a total of 22 numbers per region. This yields a $22 \times 9$ matrix feature and a binary label response indicating whether the image is pedestrian~\citep{Shashua2004PedestrianDF}. 


Label prediction based on networks is a particularly active problem in neuroscience. In this setting, each individual in the sample is represented by their own brain network, and the nodes (brain regions of interest) shared across all networks through a registration process that maps all individual brains onto a common atlas. In our analysis we use the parcellation of Power et al. (2011) (see Figure 1), which consists of 264 ROIs divided into 14 functional brain systems. A connectivity is then computed for every pair of nodes, resulting in an adjacency matrix of size 264 $\times$ 264 (see Figure 2). Two main approaches have been followed for the brain network analysis. One approach is to reduce the network to its global summary measures such as the average degree, clustering coefficient, or average path length (Bullmore and Sporns (2009)), and use those measures as features for training a classification method. An alternative approach to classification of large networks is to treat edge weights as a ``bag of features,'' vectorizing the unique elements of the adjacency matrix and ignoring the network nature of the data. The number of individual ~100.... 

In the above two examples and many other studies, researchers are interested in {\it interpretable prediction}, where the goal is to not only make accurate prediction but also identify features that are informative to the prediction. While classical learning algorithms have been successful in prediction with vector-valued predictors, the key challenge with matrix-valued predictors is the high-dimensional, complex structure in the feature space. 
A naive approach is to transform the feature matrices to vectors and apply classical methods based on vectors to solve the problem. However, this vectorization would destroy the structural information of the data matrices. Moreover, the reshaping matrices to vectors results in high dimensionality which leads to overfitting. 
Notably, the ambient dimension with matrix feature, $d_1d_2$, is often comparable to, or even exponentially larger than the number of sample, $n$. 
%Modern applications are often in the high dimensional regime where $d_1d_2$ is comparable to, or even larger, than the sample size $n$. 
{\color{red} feature selection...}
Our method exploits the structural information in the data matrix to overcome these challenges. 

Our goal in this paper is to develop a distribution-free prediction method that respects the matrix structure of the predictors and produces more interpretable results. To achieve this goal, we use structured sparsity penalties to incorporate the network information by penalizing both the number of submodules and the number of nodes selected. 


\section{Setting of the learning problem}
\subsection{Three learning problems}
In this section we present the main learning goals of our interest. Let $\mX\in\mathbb{R}^{d_1\times d_2}$ denote the matrix-valued predictor, $y\in\{-1,1\}$ denote the binary label response, and $\mathbb{P}_{\mX,Y}$ denote the unknown joint probability distribution over the pair $(\mX,y)$. Suppose that we observe a sample of $n$ training data points, $(\mX_1,y_1),\ldots,(\mX_n,y_n)$, identically and independently distributed (i.i.d.) according to $\mathbb{P}_{\mX,y}$. Let $(\mX_{n+1},y_{n+1})$ be a new test point drawn independently from the same distribution. Our goal is to make reliable prediction about $y_{n+1}$ given the new feature value $\mX_{n+1}$, with no strong distributional assumptions other than i.i.d.\ data. When no confusion arises, we often omit the subscript $(n+1)$ and simply write $(\mX,y)$ for the prototypical test point. 

 
The problem of learning is to find a decision function $f\colon \mathbb{R}^{d_1\times d_2}\to \mathbb{R}$ that minimizes certain expected risk $R(f)$ under the unknown distribution $\mathbb{P}_{\mX,y}$. The expected risk $R(f)$ is an functional over a given set of candidate functions $f\in \tF$; the form of risk depends on the specific learning goals which are detailed in the next paragraph. We focus on the challenging setting where the feature matrix dimension $d_1d_2$ may grow sub-exponentially with the sample size $n$, and no assumptions are made about the distributional family of $\mathbb{P}_{\mX,y}$. The only available information is in the training data $\{(\mX_i,y_i)\}_{i=1}^n$. We consider three main supervised learning problems: classification, level set estimation, and regression. 

\begin{enumerate}[wide, labelwidth=!, labelindent=0pt]
\item {\it Classification}: Classification is the problem of predicting to which of the label $y\in \{-1,1\}$ a new observation $\mX$ belongs. We formulate the learning problem as finding a binary-valued function (also called a classifier) from $\mathbb{R}^{d_1\times d_2}$ to $\{-1,1\}$ that minimizes the expected risk. In classification, the expected risk $R(f)$ is defined as the (expected) classification error; i.e., the probability when the true label and the predicted label differs,
\[
R(f) = \mathbb{P}_{\mX,y}\left(y\neq \sign f(\mX)\right),
\]
where the probability is taken with respect to both the predictor and response from the unknown joint distribution $\mathbb{P}_{\mX,y}$.  
Note that we have used $\sign f$ to denote the binary-valued classifier, in order to distinguish from the real-valued decision function $f$. In general, it is common, but not necessary, to estimate $\sign f$ through $f$, for the purpose of classification. 

%The classification problem has long been interested. Many attempts have been developed and performed well for example, decision tree, nearest neighbor, neural network and support vector machine to name a few. However, most of methods have focused on vector valued features. In many classification problems, the input features are naturally represented as matrices or tensors rather than vectors. 
%One example is a study of an electroencephalography data set of alcoholism. The data records voltage value measured from 64 channels of electrodes on 256 subjects for 256 time points, so each feature data is $256\times 64$ matrix and the response is binary indicator of subject being alcoholic or control \citep{zhou2014regularized}. Another example is pedestrian detection from image data. Each image was divided into 9 regions where local orientation statistics were generated with a total of 22 numbers per region, so each feature data is $22 \times 9$ matrix and the response is whether the image is pedestrian \citep{Shashua2004PedestrianDF}. 
%We want to tackle matrix valued classification preserving the matrix structure.


\item {\it Level set estimation}: The $\pi$-level set of $p$ given a fixed $\pi\in[0,1]$ is the set 
\begin{align}
S(\pi) = \{\mX\in\mathbb{R}^{d_1\times d_2}: p(\mX)>\pi\}.
\end{align}
 Accurate and efficient level set estimation plays an important role in many applications.
 %One example can be found in medical decision making. In Osteosarcoma treatment, the degree of tumor necrosis is used to guide the choice of postoperative chemotherapy \citep{man2005expression}. Patients with $\geq 90 \%$ necrosis is labeled as 1, which is response variable $y$. Suppose that $\mX$ is a feature matrix collected from the patient such as gene expression levels on each tissue. Knowledge of the regression level set is needed to allow effective postoperative chemotherapy without a biopsy. 
 We consider a nonparametric way to estimate the $\pi$-level set of the regression function based on classification problem.
%Level set estimation plays crucial roles in many applications such as digital elevation maps, medical imaging and pattern recognition.

\item {\it Regression function estimation}: Regression function calculates expectation of $y$ given a feature matrix $\mX$ on the basis of a training set of data. In our setting, the regression $ \mathbb{E}(y|\mX)$  is equivalent to  the conditional probability $\mathbb{P}(y = 1|\mX)$ because the class label $y$ is binary. Knowledge about the class probability itself is of significant interest and can tell us the confidence of the outcome of classification. 

%Traditionally, the regression problem is addressed through distribution assumption like logistic regression or linear discriminant analysis (LDA). In many applications, however, it is often difficult to  justify the assumptions made in logistic regression or satisfy the Gaussian assumption in LDA. These issues become more challenging for matrix features because of high dimensionality.
%We establish distribution free method for estimating the regression function $p(\mX)$ based on level set estimation.  
\end{enumerate}

The three problems represent our learning tasks with increasing difficulties. 
Classification problem can be completed from level set $S(\frac{1}{2})$ utilizing Bayes rule. The level set estimation problem becomes trivial when we have all information about regression function.  
Accordingly, classical approach for the three problems is to find a solution for regression first, and address the other two based on the estimation. This is why the regression problem is also called soft classification. However, our approach finds classification rule first and address the level set estimation and regression problem in order. Through the sequence of solving the problems, we successfully solve the problems without assuming probability distribution.

A common challenge is \emph{distribution-free}. 


\section{From classifications to regression: a level-set approach}

\subsection{Cost-weighted classification decision boundaries estimate the level sets}
The main approach our method is to estimate regression function by estimating a set of level-set ...estimation from a penalized convex optimization problem. This equivalence allows us to use the rich set of methods developed in classification and derive conditional densities where the conditioning events are expressed as solutions to convex optimization problem... draw a picture here...

Geometric boundary?? Insert two figures...Connects two worlds. level-set is widely used in optimization communities,... Bayes boundary in statistical communities. 

\subsection{Sparse and low-rank decision boundaries}\label{sec:fcn class}
The aforementioned three problems are formulated as empirical structure minimization over functions with matrices as inputs.
In this section, we introduce function classes we consider for the minimization problem. We start introducing linear function class in matrix space and generalize nonlinear one based on our proposed matrix kernel. 
One approach to finding a set of such nodes was proposed by Vogelstein et al. (2013), who called it a signal-subgraph, and proposed finding the minimal set of nodes (called signal vertices) which together are incident to all selected edges (but not every node connected to a selected edge is a signal vertex).

%\subsection{sparse and low-rank function class}\label{subsec:linear class}
We propose the linear functions as a decision function class, $f(\mX) = \langle \mB,\mX\rangle,$
where $\mB,\mX\in\mathbb{R}^{d_1\times d_2}$ and $\langle \mX,\mX'\rangle = \text{Tr}(\mX^T\mX')$. The inner product is called Frobenius inner product in the space of matrices and is a generalization of the dot product from vector spaces.
We impose low-rankness on the linear predictor to consider the degeneracy of the coefficient matrix $\mB$ and leverage the structure information within the predictor $\mX$.
Specifically,  the coefficient matrix has low-rank $r$  usually much smaller than the matrix size  $\min(d_1,d_2)$,
\begin{align}\label{eq:lowrank}
\mB = \mC\mP^T \text{ where } \mC \in\mathbb{R}^{d_1\times r} ,\mP\in\mathbb{R}^{d_2\times r}\text{ and } r\leq\min(d_1,d_2).
\end{align}
The low-rankness makes distinction from classical classification problem in vector spaces and preserves structural information of feature matrices. The low rank constraint on coefficient matrix has been proposed in Support Vector Machine (SVM) classification problem \citep{pirsiavash2009bilinear,luo2015support} in matrix spaces. However these papers only focus on application to the classification problem. Our paper is not confined in classification but further apply the linear function class to level set estimation and regression estimations. Furthermore, we extend linear case to nonlinear case based on matrix kernel concept in the next section. There are some prior work on matrix/tensor kernels. Most of these methods, however, are inadaptive and do not use label information to guide the kernel construction. We propose a new matrix kernel that solves the limitation.

\begin{prop}[Rademacher complexity]. 

\end{prop}


\section{Nonparametric learning with ultra-high dimensional matrices}


\subsection{Main Result I: Classification with high-dimensional matrices} 
We consider a single boundary..., 
 
 (why large-margin classification??)
 
 three examples: large margin, logistic, neural network.

\label{subsec:pb1}
We estimate a decision function $f\colon\mathbb{R}^{d_1\times d_2}\rightarrow \mathbb{R}$ over function class introduced in Section \ref{sec:fcn class}. The decision function class denoted as $\tF$ can be either linear or nonlinear. 
We propose a large margin classifier that minimizes a cost function in $f$ over the function class $\tF$.
\begin{align}
\label{eq:large-margin}
\min_{f\in\tF}n^{-1}\sum_{i=1}^n L\left(y_i f(\mX_i)\right)+\lambda J(f),
\end{align}
where $J(f)$ is a regularization term for model complexity and $L(z)$ is a margin loss that is a function of the functional margin $yf(\mX)$. Examples of such loss functions are the hinge loss function $L(z) = (1-z)_+$ and the logistic loss function $L(z) =\log(1+e^{-z})$.  For demonstration, we focus on the hinge loss case in Equation \eqref{eq:large-margin}. However, our estimation schemes and theorems are applicable to general large-margin classifiers.

We present the solution to~\eqref{eq:large-margin} with nonlinear kernels which incorporate linear case. Based on the considered decision function class, we solve the following optimization problem,
\begin{align}
\label{eq:opteq}
(\hat \mP_1,\hat\mC,\hat\mP_2) = \argmin_{\{(\mP_1,\mP_2,\mC)\in\mathbb{R}^{d_1\times r}\times \mathbb{R}^{d_2\times r}\times (\tH_1\times \tH_2)^{r\times r}\}}n^{-1}\sum_{i=1}^n \left(1-y_i\langle \mP_1\mC\mP_2^T,\mX_i\rangle\right)_++\lambda \FnormSize{}{\mP_1\mC\mP_2^T}^2.
\end{align}
Notice that the optimization problem \eqref{eq:opteq} degenerates to the conventional SVM with vectorized feature matrices when feature mapping is identity and the coefficient is full rank. From the solution to \eqref{eq:opteq}, our estimated classifier is written 
\begin{align}\label{eq:formulation}\hat g(\mX) = \text{sign}(\hat f(\mX)) =  \text{sign}\left(\langle \hat\mP_1\hat\mC\hat\mP_2^T,\mX\rangle\right).\end{align}


We make a remark on the implication of the formulation \eqref{eq:formulation}.
The solution \eqref{eq:formulation} implies a joint learning of dimension reduction and classification risk minimization. This is one of our contribution to combine two different processes into one. To check this, we see a dual representation of the solution to \eqref{eq:opteq},
\begin{align}\label{eq:dualrep}
f(\mX) &= \sum_{i=1}^n\alpha_iy_i\left(\sum_{j,k\in[d_1]} [\mH_{\mP_1}]_{jk},K_1(\mX^{(i)}_{j:},\mX_{k:})+\sum_{j,k\in[d_1]} [\mH_{\mP_2}]_{jk},K_2(\mX^{(i)}_{:j},\mX_{:k})\right),
\end{align}
where $\{\alpha_i\}_{i=1}^n$ are  (sparse) dual solution to \eqref{eq:opteq} and $\mX^{(i)}_{jk}$ denotes (j,k)-entry of $\mX_i$. Notice that the representation  \eqref{eq:dualrep} can be viewed as an element of reproducing kernel Hilbert space (RKHS) induced by matrix kernel with weight matrices $\{\mH_{\mP_i}\}_{i=1,2}$ and row and column-wise kernels $K_1,K_2$.
Therefore, our considered function space $\tF$ can be written as  
\begin{align}
    \tF &= \{f\colon\mX\mapsto\langle\mP_1\mC\mP_2^T,\Phi(\mX)\rangle|\mC\in(\tH_1\times \tH_2)^{d_1\times d_2} \text{ and }\mP_i \in\mathbb{R}^{d_i\times r} \text{ for } i=1,2\}\\
    &=\{f\in \text{RKHS induced by matrix kernel } \mK \text{ with } \{\mH_{\mP_i},K_i\}_{i=1,2}\}.
\end{align}
Given row and column-wise kernels, we estimate weight matrices and an element of RKHS induced by the estimated weight matrices.
The projection matrices $\{\mH_{\mP_i}\}_{i=1,2}$ play role in reducing  the feature dimension and at the same time, we find the best element of RKHS that minimizes the classification risk by estimating coefficients $\malpha$ in \eqref{eq:dualrep}. The procedure is summarized as the following optimization
\begin{align}
    \max_{f\in\tF}L(f)  = \max_{\substack{\text{rank}(\mW_i)\leq r,\\ \mW_i\succeq 0,i = 1,2}}\max_{\substack{f\in\text{RKHS} (\mK)\\ |\{\mW_i,K_i\}_{i=1,2}}}L(f).
\end{align}


\subsection{Main result II: Level set estimation in matrix space}\label{subsec:pb2}

We now consider multiple boundaries, for any $\pi\in[0,1]$. We propose weighted loss function from \eqref{eq:large-margin} to estimate the level set, 
\begin{align}
\label{eq:weighted}
\min_{f\in\tF}n^{-1}\sum_{i=1}^n \omega_{\pi}(y_i)L\left(y_if(\mX_i)\right)+ \lambda J(f),
\end{align}
where $\omega_\pi(y) = 1-\pi $ if $y = 1$ and $\pi$ if $y = -1$. The weighted loss accepts unequal costs for positive and negative misclassifications in margin classifier, where $\pi$ is the known cost for the negative and $1-\pi$ is for the positive classes. Notice that equal cost $\pi = \frac{1}{2}$ make \eqref{eq:weighted} reduce to \eqref{eq:large-margin}. 
The optimizer to Equation \eqref{eq:weighted} with respect to all measurable function class yields an consistent estimate of the Bayes rule $g_\pi(\mX) = \text{sign}\left(f_\pi(\mX)\right)$ where $f_\pi(\mX) = p(\mX) - \pi$ \citep{lin2002support,wang2008probability}. 
Therefore, under the considered decision function class, we obtain a minimizer $\hat f_\pi$ to \eqref{eq:weighted} and estimate the level set as
\begin{align}\label{eq:levelset}\hat S(\pi) = \{\mX\in\mathbb{R}^{d_1\times d_2} : \text{sign}(\hat f_\pi(\mX)) = 1\}
\end{align}

\begin{assumption}[Identifiability]~\label{ass:main}
There exist constants $b>0$ and $\alpha\geq 0$, such that, for any sufficient small $\delta>0$,
\[
\mathbb{E} \big|\sign  f (\mX)- \sign \bayesf(\mX)\big| \leq b\left[R_\ell(f)-R_\ell(\bayesf)\right]^\alpha
\]
holds for all $f\in \{ f\in\tF(d,r,s)\colon R_\ell(f)-R_\ell(\bayesf) \leq \delta\}$ in a $\delta$-neighborhood of $\bayesf$.
\end{assumption}

\begin{thm}
Assumption~\ref{ass:main} holds with $\rho=\alpha\wedge 1$, and the estimation error for level sets satisfies
\[
\mathbb{P}(\hat \mX\Delta\mX_{\text{bayes}}) \leq C\left({rs\log d \over n}+a_n\right)^{\alpha/(2-\alpha\wedge 1)},
\]
where $\hat \mX=\{\mX\colon \hat f(\mX)\geq 0\}$ and $\mX_{\text{bayes}}=\{\mX\colon \bayesf(\mX)\geq 0\}$ are estimated and true level sets, respectively. 
\end{thm}

More comments on the assumptions....

\subsection{Main results III: Nonparametric regression function estimation}
\label{subsec:pb3}
We propose a method to estimate  the regression function $p(\mX)\stackrel{\text{def}}{=}\mathbb{E}(y = 1|\mX)$ at any $\mX$ which does not necessarily belong to the observed training data set. Non-smooth, non-continuous regression functions are allowed in our framework. Consider the following two steps of approximation to the target function.
\begin{align}
\label{eq:approx}
p(\mX) &\stackrel{\text{step1}}{\approx} \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX: p(\mX)\leq\frac{h}{H}\right\}\\&\hspace{.2cm}=\hspace{.2cm} \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX\notin S\left(\frac{h}{H}\right)\right\}
\\&\stackrel{\text{step2}}{\approx} \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX\notin\hat S\left(\frac{h}{H}\right)\right\}.
\end{align}
Step 1 approximates the target probability by linear combination of step functions where $H$ is a smooth parameter. In step 2, we plug in the level set estimation defined in \eqref{eq:levelset} given $\pi = h/H$. Here we use consistency of level set estimation.
Therefore, we estimate the regression function as,\begin{align}
\hat p(\mX) = \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX\notin\hat S\left(\frac{h}{H}\right)\right\},
\end{align}
by repeatedly estimating the level sets as \eqref{eq:levelset} with different $\pi$ values, say  $\pi = \frac{h}{H}$ for $h = 1,\ldots, H$. 



\begin{thm}[Main result for probability estimation] Consider the same assumptions of Theorem~\ref{thm:main}. For the estimator $\hat p\colon \mathbb{R}^{d\times d}\to[0,1]$ obtained from level-set estimation,
\[
\hat p(\mX)={1\over H}\sum_{h=1}^H \mathds{1}\{\mX\colon \hat f_h(\mX)\geq 0\},\quad \text{for all }\mX\in\mathbb{R}^{d\times d}.
\]
With probability at least $1-C\exp(-an\lambda^{2-\alpha})$,
\[
\mathbb{E}|\hat p(\mX)- p(\mX)| \geq \KeepStyleUnderBrace{{1\over 2H}}_{\text{discretization error}} +\ {a(H+1)\over 2} \tO\left(\KeepStyleUnderBrace{{rs\log d\over n}}_{\text{statistical error}} +\KeepStyleUnderBrace{a_n}_{\text{approximation error}}\right)^{\alpha^2}.
\]
\end{thm}

\begin{cor} Assume $a_n \leq {rs\log d \over n}$. Choosing $H\asymp ({rs\log d \over n})^{-\alpha^2/2}$ gives the estimation error,
\[
\mathbb{E}|\hat p(\mX)- p(\mX)| \leq C\left({rs\log d \over n}\right)^{\rho/2}.
\]
\end{cor}

\section{Two examples}
The tensor regression provides a very general framework. By choosing a different distribution over predictor matrix, label ..., ...., we then obtain a different model. Several important examples are given below.
\subsection{1-bit matrix denoising}

\subsection{Multi-task learning}

\section{An ADMM algorithm for structural risk minimization}
\[
\min_{f\in \tF}n^{-1}\sum_{i=1}^nL(y_if(\mX-i))+\lambda J(f)
\]

\[
\min_{\mB\in\tB(r,s), \mc\in\mathbb{R}^p} n^{-1}\sum_{i=1}^nL\left(y_i\left[\mw^T_i\mc+\langle \mX_i,\mB\rangle \right]\right) + \lambda \FnormSize{}{\mB}^2
\]
We introduce ADMM argument $\mB=\mP\mQ^T$, where $\mP,\mQ\in\mathbb{R}^{d\times r}$ and penalization $\rho$. 
Given $\rho$ and $\lambda$, we solve
\[
\tL(\mB,\mS, \mc, \mLambda; \rho, \lambda)=n^{-1}\sum_{i=1}^nL\left(y_i\left[\mw^T_i\mc+\langle \mX_i,\mB\rangle \right]\right) + \lambda \FnormSize{}{\mB}^2+\rho\FnormSize{}{\mB-\mS}^2 + \langle \mLambda, \mB-\mS\rangle.
\]
\begin{enumerate}
\item Update $\mB$:
\[
\tL(\mB, \mc; \mS, \Lambda, \rho,\lambda) = n^{-1}\sum_{i=1}^nL\left(y_i\left[\mw^T_i\mc+\langle \mX_i,\mB\rangle \right]\right)  + (\lambda+\rho) \Fnorm{\mB-{1\over 2(\lambda+\rho)}\left(2\rho\mS-\mLambda\right)}^2.
\]
Equivalently, define $\check\mB=\mB-{1\over 2(\lambda+\rho)}\left(2\rho\mS-\mLambda\right)$, we have
\[
\tL(\check \mB, \mc; \mS, \Lambda, \rho,\lambda) = n^{-1}\sum_{i=1}^nL\left(y_i\left[\Big\langle \mX_i, {2\rho\mS-\mLambda\over 2(\lambda+\rho)}\Big\rangle+\mw^T_i\mc+\langle \mX_i,\check\mB\rangle \right]\right)  + (\lambda+\rho)\FnormSize{}{\check \mB}^2.
\]
\item Update $\mc$:
\[
\tL(\mc;\mB, \mS, \Lambda, \rho,\lambda) = n^{-1}\sum_{i=1}^nL\left(y_i\left[\mw^T_i\mc+\langle \mX_i,\mB\rangle \right]\right)
\]
\item Update $\mS$:
\[
\tL(\mS; \mc, \mB, \Lambda, \rho,\lambda) = \Fnorm{\mS-{2\rho\mB+\mLambda\over 2\rho}}^2, \quad \text{where}\quad \mS\in\tB(r,s).
\]
\item Update $\mLambda$:
\[
\mLambda^{(t+1)} = \mLambda^{(t)}+2\rho(\mB-\mS).
\]
Ideally, small $\rho$ = $\rho$.
\end{enumerate}



\label{sec:alg}
In this Section, we describe an algorithm to seek the optimizer of Equation  \eqref{eq:large-margin} in the case of hinge loss function $L(z) = (1-z)_+$. We consider nonlinear decision function class $\tF = \{f\colon\mX\mapsto\langle\mC \mP^T,\Phi(\mX)\rangle|\mC = (\mC_1,\mC_2)\in\tH_1^{d_1}\times \tH_2^{d_2} \text{ and }\mP = (\mP_1,\mP_2)\in\mathbb{R}^{d_1\times r} \times \mathbb{R}^{d_2\times r}\}$ given row and columnwise kernels $K_1,K_2$.
Notice Equation \eqref{eq:opteq} is written as 
\begin{align}
    \label{eq:opt}
    &\min_{\substack{\mC\in\tH_1^{d_1}\times \tH_2^{d_2} ,\\\mP\in\mathbb{R}^{d_1\times r} \times \mathbb{R}^{d_2\times r}}}\frac{1}{2}\FnormSize{}{\mC\mP^T}^2+ C\sum_{i=1}^n \xi_i,\\
    &\text{subject to } y_i\langle\mC\mP^T,\Phi(\mX_i)\rangle\leq 1-\xi_i\text{ and } \xi_i\geq 0, i=1,\ldots,n.
\end{align}
Optimization problem \eqref{eq:opt} is non-convex problem because low-rank constraint makes feasible set non-convex.
We propose to utilize coordinate descent algorithm that solves one block holding the other block fixed. From this approach, we can solve a convex problem in each step.
To be specific, first we update $\mC$ holding $\mP$ fixed.
The dual problem of Equation \eqref{eq:opt} with fixed $\mP$ is 
\begin{align}
    \label{eq:dual1}
    &\max_{\malpha = (\alpha_1,\ldots,\alpha_n)}-\sum_{i=1}^n\alpha_i + \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_j y_iy_j \langle \Phi(\mX_i)\mP(\mP^T\mP)^{-1}\mP^T,\Phi(\mX_j)\mP(\mP^T\mP)^{-1}\mP^T\rangle\\
    &\text{ subject to }  0\leq \alpha_i\leq C, i=1,\ldots,n.
\end{align}
We use quadratic programming to solve the dual problem and update $\mC$ as
\begin{align}\label{eq:C}
    \mC = \sum_{i=1}^n \alpha_iy_i \Phi(\mX_i) \mP(\mP^T\mP)^{-1}\in\tH_r^r\times \tH_c^r.
\end{align}
We use the formula \eqref{eq:C} without information about feature mapping $\Phi(\cdot)$.
Second, we assume that $\mC$ is fixed and update $\mP$. The dual problem of Equation \eqref{eq:opt} with fixed $\mC$ is 
\begin{align}
    \label{eq:dual2}
    &\max_{\malpha = (\alpha_1,\ldots,\alpha_n)}-\sum_{i=1}^n\alpha_i + \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_j y_iy_j\langle 
    \mC\left((\mC^T\mC)^{-1}\mC^T\Phi(\mX_i)\right),\mC\left((\mC^T\mC)^{-1}\mC^T\Phi(\mX_j)\right)\rangle,\\
    &\text{ subject to }  0\leq \alpha_i\leq C, i=1,\ldots,n,
\end{align}
We can find an optimizer of \eqref{eq:dual2} based on kernel information only. We obtain the following formula by plugging \eqref{eq:C} into components of \eqref{eq:dual2}. 
\begin{align}\label{eq:helpeq}
 &\mC^T\mC = \sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_jy_iy_j(\mP^T\mP)^{-1}\mP^T\mK(i,j) \mP(\mP^T\mP)^{-1}\in\mathbb{R}^{r\times r}\times \mathbb{R}^{r\times r},\\
 &\mC^T\Phi(\mX_i) = \sum_{j=1}^n \alpha_iy_i(\mP^T\mP)^{-1}\mP^T\mK(i,j)\in\mathbb{R}^{r\times d_1}\times \mathbb{R}^{r\times d_2},
\end{align}
where $\mK(i,j) \stackrel{\text{def}}{=} \left( \Phi_1(\mX_i)^T\Phi_1(\mX_j),\Phi_2(\mX_i)^T\Phi_2(\mX_j)\right)\in \mathbb{R}^{d_1\times d_1}\times \mathbb{R}^{d_2\times d_2}$. Notice that $[\Phi_1(\mX_i)^T
\Phi_1(\mX_j)]_{ss'} = K_1(\mX^{(i)}_{s:},\mX^{(j)}_{s':})$ and vice versa for $\Phi_2(\cdot)$.
Therefore, we update $\mP$  from an optimal coefficient $\malpha$ to \eqref{eq:dual2} without specifying feature mapping.
\begin{align}
    \mP = \sum_{i=1}^n\alpha_iy_i(\mC^T\mC)^{-1}\mC^T\Phi(\mX_i).
\end{align}
We end up obtaining nonlinear function output of the form,
\begin{align}\label{eq:output}
    \hat f(\mX) = \sum_{k=1}^n\hat\alpha_ky_k&\bigg(\sum_{i=1}^{d_1}\sum_{j=1}^{d_1}[\hat\mP_1(\hat\mP_1^T\hat\mP_1)^{-1}\hat\mP_1^T]_{ij}K_r\left([\mX_k]_{i:},[\mX]_{j:}\right)\\ &+\sum_{i=1}^{d_2}\sum_{j=1}^{d_2}[\hat\mP_2(\hat\mP_2^T\hat\mP_2)^{-1}\hat\mP_2^T]_{ij}K_c\left([\mX_k]_{:i},[\mX]_{:j}\right)\bigg).
\end{align}
Algorithm \ref{alg:svm} gives the full description for classification. 

By the similar way with little modification, we can obtain an algorithm for weighted margin classifier \eqref{eq:weighted}. From the explanation in Section \ref{subsec:pb2} and \ref{subsec:pb3}, we summarize level set and regression estimation procedure in Algorithm \ref{alg:regest}.
 \begin{algorithm}[h]
 \label{alg:svm}
\KwIn{$(\mX_1,y_1),\cdots,(\mX_n,y_m)$, rank $r$, and pre-specified kernels $K_1,K_2$}
{\bf Initizlize:} $\mP^{(0)}\in\mathbb{R}^{d_1\times r}\times \mathbb{R}^{d_2\times r}$\\
{\bf Do until converges}\\
\hspace*{.5cm}{\bf Update} $\mC$ fixing $\mP$ :\\[.1cm]
\hspace*{.4cm} Solve $\max_{\malpha}-\sum_{i=1}^n\alpha_i + \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_j y_iy_j \langle \Phi(\mX_i),\Phi(\mX_j)\mP(\mP^T\mP)^{-1}\mP^T\rangle$\\
\hspace{.5cm} $\mC = \sum_{i=1}^n \alpha_iy_i \Phi(\mX_i) \mP(\mP^T\mP)^{-1}$.\\[.1cm]
\hspace*{.5cm}{\bf Update} $\mP$ fixing $\mC$ :\\[.1cm]
\hspace*{.4cm} Solve  $ \max_{\malpha}-\sum_{i=1}^n\alpha_i + \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n \alpha_i\alpha_j y_iy_j\langle 
    \Phi(\mX_i),\mC\left((\mC^T\mC)^{-1}\mC^T\Phi(\mX_j)\right)\rangle$.\\
\hspace{.5cm} $\mP = \sum_{i=1}^n\alpha_iy_i(\mC^T\mC)^{-1}\mC^T\Phi(\mX_i)$.\\[.1cm]
\KwOut{ $\hat f$ of the form \eqref{eq:output}}
    \caption{{\bf Classification algorithm} }
\end{algorithm}





 \begin{algorithm}[h]
 \label{alg:regest}
\KwIn{$(\mX_1,y_1),\cdots,(\mX_n,y_m)$, rank $r$,pre-specified kernels $K_1,K_2$, and smooth parameter $H$.}
{\bf Initialize:} $\pi_h = (h-1)/H$ for $h = 1, \ldots, H+1$\\
{\bf For $h = 1,\ldots, H+1$:}\\
\hspace*{.5cm}{\bf Level set $\hat S(\pi_h)$ estimation:}\\
\hspace*{1cm}{\bf Train} weighted margin classifier $\hat f_{\pi_h}$ from \eqref{eq:weighted} based on Algorithm \ref{alg:svm}.
\\[.1cm]
\hspace*{.9cm} $\hat S(\pi_h) = \{\mX\in\mathbb{R}^{d_1\times d_2}:\text{sign}(\hat f_{\pi_h}(\mX)) = 1\}.$
\\[.1cm]
{\bf Regression $\hat p(\mX)$ estimation:} \\[.1cm]
\hspace*{.4cm} $\hat p(\mX) = \sum_{h=1}^H\frac{1}{H}\mathds{1}\left\{\mX\notin\hat S\left(\pi_h\right)\right\}.$\\[.1cm]
\KwOut{ Level sets $\hat S(\pi_h)$ for $h =1,\ldots H$ and regression function $\hat p(\mX)$.}
    \caption{{\bf Level set \& Regression Algorithm} }
\end{algorithm}

\section{Extension to nonlinear boundaries}\label{subsec:nonlinear class}
We propose a family of matrix kernels, which are building blocks for defining functions in matrix space. Kernel methods defined on non-vector objects have recently evolved into a rapidly developing branch of learning on structured data. Informally, a matrix kernel is a distance measure between two matrices with the same size using proper notion of similarity. Unlike vectors, matrix inputs represent two-way relationship across rows and columns at a time. Taking into account of this two-way relationship is essential in the kernel development.
Our proposed kernel uses concept from latent factor models and incorporates the two-way similarities via low rank regularization. 
We generalize classical kernel method in vector spaces to matrix spaces. We briefly summarize kernel method for vector features and introduce new notations and operations which will be used later.

It has been popular and successful to extend linear classifiers in vector space to nonlinear classifiers using kernel method. Classical linear classifier finds linear boundaries in the input vector feature space. By introducing feature mapping which maps input feature space to enlarged dimension space, learning nonlinear classifier becomes possible. In fact, we need not specify the feature mapping at all to obtain an optimal function that minimizes pre-determined loss function. Instead, the learning process only requires knowledge of the kernel function that computes inner products in the transformed enlarged space, thereby avoiding heavy computation. We generalize this kernel approach to the case when input feature is matrix valued.


Before proposing a new matrix feature mapping and kernel, we introduce notations and operations needed later. Let $\phi_i\colon\mathbb{R}^{d_i}\rightarrow \tH_i$ be feature mappings with a classical kernel defined on vectors $K_i\colon\mathbb{R}^{d_i}\times \mathbb{R}^{d_i}\rightarrow \mathbb{R}$ for $i = 1,2.$ $\tH_i$ denotes enlarged feature space by $\phi_i$ and a possibly infinite dimensional Hilbert space.  Let $\tH^{d_1\times d_2}$ = $\{\mX\colon\mX = \entry{x_{ij}},x_{ij}\in\tH\}$ denote the collection of $d_1$ by $d_2$ matrices with each entry taking value in a Hilbert space $\tH$.   Matrix algebraic operations are carried over from operations on real valued matrices. One can check exact definitions of operations in Supplement.


Now we present matrix the matrix kernel and associated feature mapping.
We define matrix valued feature mapping over  the $d_1$-by-$d_2$ matrix space.
\begin{defn}\label{def:map}
Let $\phi_1\colon\mathbb{R}^{d_1}\rightarrow \tH_1 $ and $\phi_2\colon\mathbb{R}^{d_2}\rightarrow \tH_2 $  be classical feature mappings defined on vector space. Then $\Phi$ is matrix feature mappings defined on
\begin{align}\label{eq:featuremap}
    \Phi(\mX)\colon \mathbb{R}^{d_1\times d_2} &\rightarrow (\tH_1\times \tH_2)^{d_1\times d_2}\\
    \mX &\mapsto \Phi(\mX) = \entry{\Phi(\mX)_{ij}}\text{ where } \Phi(\mX)_{ij}\stackrel{\text{def}}{=}\left(\phi_2(\mX_{i:}),\phi_1(\mX_{:j})\right).
\end{align}
Notice that the matrix feature mapping considers both row-wise and column-wise enlarged features.
From the feature mapping, the linear function $f\colon \mathbb{R}^{d_1\times d_2}\rightarrow \mathbb{R}$ with respect to enlarged space $\Phi(\mX)\in(\tH_1\times \tH_2)^{d_1\times d_2}$ is defined as,
\begin{align}\label{eq:linearfcn}
    f(\mX) \stackrel{\text{def}}{=}\langle \mB,\Phi(\mX) \rangle ,\text{ where }& \mB=\entry{(\mb^{\text{row}}_i,\mb^{\text{col}}_j)}\in(\tH_1\times \tH_2)^{d_1\times d_2},\\\text{ with }& \mb^{\text{row}}_i\in\tH_1\text{ and } \mb^{\text{col}}_j\in\tH_2 \text{ for all } (i,j)\in[d_1]\times[d_2].
\end{align}
\end{defn}
Notice we impose this structure on $\mB$ for identifiability issue.
These matrix valued feature mapping \eqref{eq:featuremap} and corresponding linear function \eqref{eq:linearfcn} are generalization from existing classical kernel method in vector spaces and can be extended naturally to tensor case (see Supplement for the details).
 We assume that the coefficient $\mB$ in \eqref{eq:linearfcn} admits low rank decomposition as in Section \ref{subsec:linear class}, 
 \begin{align}\label{eq:lowrk}
     \mB = \mP_1\mC\mP_2^T,\text{ where }& \mP_1\in\mathbb{R}^{d_1\times r},\mP_2\in\mathbb{R}^{d_2\times r} \text{ and } \mC = \entry{(\mc_i^{\text{row}},\mc_j^{\text{col}})}\in(\tH_1\times \tH_2)^{r\times r},\\
     \text{ with }&\mc^{\text{row}}_i\in\tH_1\text{ and } \mc^{\text{col}}_j\in\tH_2 \text{ for all } i,j\in[r].
 \end{align}
 Again, we have the structured $\mC$ for identifiability. 
  When feature mapping $\phi_i$ is identity for $i=1,2$ implying the linear case in Section \ref{subsec:linear class}, we show that considered linear functions \eqref{eq:linearfcn} with low-rank $r$ defined by \eqref{eq:lowrk} are equivalent to the linear functions in Section \ref{subsec:linear class} with the low-rank $r$ constraint \eqref{eq:lowrank}.
  Therefore, our matrix feature mapping is generalization of classical feature mapping on vector spaces and extension to nonlinear case from linear functions on matrix features.
  
Now we define matrix kernel associated with the matrix feature mapping.
\begin{defn}\label{def:kernel}
Let $K_i(\cdot,\cdot)$ be classical kernels which can be represented as $K_i(\cdot,\cdot) = \langle \phi_i(\cdot),\phi_i(\cdot)\rangle$ for $i=1,2$. Let weight matrices $\mW_i = \entry{w^{(i)}_{jk}}\in\mathbb{R}^{d_i\times d_i}$ be  rank-$r$ semi-positive definite matrices for $i = 1,2$. Then $\{\mW_i,K_i\}_{i=1,2}$  induce matrix kernel defined by
\begin{align}
    \mK\colon\mathbb{R}^{d_1\times d_2}\times \mathbb{R}^{d_1\times d_2}&\rightarrow \mathbb{R}\\
    (\mX,\mX')&\mapsto \mK(\mX,\mX')  = \sum_{j,k\in[d_1]}w^{(1)}_{jk}K_1(\mX_{j:},\mX'_{k:})+\sum_{j,k\in[d_2]}w^{(2)}_{jk}K_2(\mX_{:j},\mX'_{:k}).
\end{align}
\end{defn}
The matrix kernel incorporates classical kernel in vector spaces. Like classical kernel, we can associate the feature mapping in Definition \ref{def:map} with the matrix kernel. Given $\{\mW_i,K_i\}_{i=1,2}$, we have
\begin{align}\mK(\mX,\mX')& = \sum_{j,k\in[d_1]}w^{(1)}_{jk}K_1(\mX_{j:},\mX'_{k:})+\sum_{j,k\in[d_2]}w^{(2)}_{jk}K_2(\mX_{:j},\mX'_{:k}) \\
&=\langle (\mW_1,\mW_2)\Phi(\mX),(\mW_1,\mW_2)\Phi(\mX')\rangle.
\end{align}
We can view the matrix kernel as weighted inner product of the feature mappings. From the kernel representation, we learn nonlinear function successfully  avoiding specification of feature mapping $\Phi(\mX)$ as in classical vector case given pre-specified row and column-wise kernels $K_1,K_2$.
\section{Numerical experiments}



\section*{Acknowledgements}


\bibliographystyle{chicago}
\bibliography{tensor_wang}



\end{document}
