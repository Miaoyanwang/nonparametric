\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{enumerate}
\usepackage{natbib}
\usepackage{url} % not crucial - just used below for the URL 

%\pdfminorversion=4
% NOTE: To produce blinded version, replace "0" with "1" below.
\newcommand{\blind}{0}
\newcommand*{\KeepStyleUnderBrace}[1]{%f
\mathop{%
\mathchoice
{\underbrace{\displaystyle#1}}%
{\underbrace{\textstyle#1}}%
{\underbrace{\scriptstyle#1}}%
{\underbrace{\scriptscriptstyle#1}}%
}\limits
}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs}
\usepackage{amsmath,amssymb,amsthm,bm,hyperref,enumerate}
\usepackage{dsfont,listings}

\usepackage[ruled,vlined]{algorithm2e}

\usepackage{enumitem}
\newtheorem{schm}{Scheme}
\newtheorem*{schm*}{Scheme}

% DON'T change margins - should be 1 inch all around.
\addtolength{\oddsidemargin}{-.5in}%
\addtolength{\evensidemargin}{-.5in}%
\addtolength{\textwidth}{1in}%
\addtolength{\textheight}{-.3in}%
\addtolength{\topmargin}{-.8in}%

\input macros.tex
\begin{document}

%\bibliographystyle{natbib}

\def\spacingset#1{\renewcommand{\baselinestretch}%
{#1}\small\normalsize} \spacingset{1}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\if1\blind
{
  \title{\bf Model free probability estimation for matrix feature}
  \author{Author 1\thanks{
    The authors gratefully acknowledge \textit{please remember to list all relevant funding sources in the unblinded version}}\hspace{.2cm}\\
    Department of YYY, University of XXX\\
    and \\
    Author 2 \\
    Department of ZZZ, University of WWW}
  \maketitle
} \fi

\if0\blind
{
  \bigskip
  \bigskip
  \bigskip
  \begin{center}
    {\LARGE\bf Large margin classification and probability estimation for matrix feature}
\end{center}
  \medskip
} \fi

\bigskip
\begin{abstract}
The text of your abstract. 200 or fewer words.
\end{abstract}

\noindent%
{\it Keywords:}  3 to 6 keywords, that do not appear in the title
\vfill

\newpage
\spacingset{1.5} % DON'T change the spacing!
\section{Introduction}
\label{sec:intro}

\section{Methods}
\label{sec:meth}
We derive our methodology for classification and probability estimation on a set of data matrices.  In particular, we propose a large-margin classifier considering matrix features.  Based on the new classification method, we suggest training a series of weighed classifiers and using them to construct the probability estimation. 
\subsection{Classification}
We consider linear predictors on matrix feature of the form 
\begin{align}
f_{\mB}(\mX) = \langle \mB,\mX\rangle,
\end{align}
where $\mB,\mX\in\mathbb{R}^{d_1\times d_2}$ and $\langle \mX,\mX'\rangle = \text{Tr}(\mX^T\mX')$. We extend the linear model to non-linear case in Section \ref{sec:nonlinear}.
This matrix representation has advantage of regularizing the coefficient matrix $\mB$ and restricting the number of parameters. Specifically, we assume that $\mB$ has low-rank structure such that
\begin{align}
\mB = \mU\mV^T \text{ where } \mU \in\mathbb{R}^{d_1\times r} ,\mV\in\mathbb{R}^{d_2\times r}\text{ and } r\leq\min(d_1,d_2)
\end{align}
We show how low rankness let us consider column and row wise structure of feature matrices and prevents overfitting in Section  \ref{sec:thm}.
Assume we are given a set of training data and label pairs $\{\mX_i,y_i\}_{i=1}^n$ where $\mX_i\in\mathbb{R}^{d_1\times d_2}$ and $y_i\in\{-1,+1\}$. Our goal is to learn a model with a low error on the training data. One successful approach is  large-margin classifiers.
A large-margin classifier minimizes a cost function in $f$ over a decision function class $\tF$:
\begin{align}
\label{eq:large-margin}
\min_{f\in\tF}n^{-1}\sum_{i=1}^n L\left(y_i f(\mX_i)\right)+\lambda J(f),
\end{align}
where $J(f)$ is a penalty term for model complexity and $L(z)$ is a margin loss that is a function of the functional margin $yf(\mX)$. Examples of such loss functions are the hinge loss function $L(z) = (1-z)_+$ and the logistic loss function $L(z) =\log(1+e^{-z})$.  
For demonstration, we focus on the hinge loss functions. However, our estimation schemes and theorems are applicable to general large-margin classifiers. Consider linear decision function class with low rank coefficient  $\tF = \{f: f(\cdot) = \langle \mU\mV,\cdot\rangle \text{ where }\mU\in\mathbb{R}^{d_1\times r},\mV\in\mathbb{R}^{d_2\times r}\}$.
The solution $f(\mX)$ of Equation \eqref{eq:large-margin}  is shown to have the form  (check Supplement)
\begin{align}\label{eq:form}
f(\mX) = \sum_{i=1}^n \alpha_i y_i\langle \mP_r\mX_i, \mP_r \mX\rangle,
\end{align}
where $\{\alpha_i\}_{i=1}^n$ are solution  (spare)  in the dual problem of Equation \eqref{eq:large-margin} and $\mP_r\in\mathbb{R}^{r\times d_1}$ is the projection matrix induced by low rank coefficient $\mU,\mV$. Let $\langle \cdot,\cdot\rangle_{\mP_r}$ denote the low rank linear kernel for a pair of matrices:
\begin{align}\label{eq:linear kernel}
\langle \mX,\mX'\rangle_{\mP_r} \stackrel{\text{def}}{=}\langle \mP_r\mX,\mP_r\mX'\rangle, \quad\text{ for all } \mX,\mX'\in\mathbb{R}^{d_1\times d_2},.
\end{align}
Therefore, we consider the decision function of the form:
\begin{align}
f(\cdot) = \sum_{i=1}^n \alpha_iy_i\langle\mX_i,\cdot\rangle_{\mP_r}.
\end{align}
We can think of our considered classification function class as the reproducing kernel Hilbert space induced by rank-$r$ linear kernels $\{\langle \cdot,\cdot\rangle_{\mP}: \mP\in\mathbb{R}^{r\times d_1}\text{ is a projection matrix} \}$.
We estimate coefficient $\{\hat \alpha_i\}_{i=1}^n$ and the projection matrix $\hat \mP_r$ from a given dataset. Detailed estimation algorithm appears in Section \ref{sec:alg}. We obtain our classification rule as
\begin{align}
G(\mX) = \text{sign}\left(\sum_{i=1}^n \hat \alpha_i y_i\langle \mX_i,\mX\rangle_{\hat \mP_r}\right).
\end{align}



\subsection{Probability function estimation}
Our proposed method is designed to estimate $p(\mX)\stackrel{\text{def}}{=}\mathbb{P}(y = 1|\mX)$ at any $\mX$ which does not necessarily belong to the observed training data set. We consider the weighed version of \eqref{eq:large-margin},
\begin{align}
\label{eq:weighted}
\min_{f\in\tF}n^{-1}\sum_{i=1}^n \omega_{\pi}(y_i)L\left(y_if(\mX_i)\right)+ \lambda J(f),
\end{align}
where $\omega_\pi(y) = 1-\pi $ if $y = 1$ and $\pi$ if $y = -1$.
\citet{wang2008probability} showed that The minimizer $\hat f_\pi$ to Equation \eqref{eq:weighted} is a consistent estimate of $\text{sign}(p(\mX)-\pi)$. Therefore,  for a given smoothing parameter $H\in\mathbb{N}_+,$ We estimate the target probability through two main steps.
\begin{align}
\label{eq:approx}
p(\mX) &\stackrel{\text{step1}}{\approx} \sum_{h=1}^H\frac{h-1}{H}\mathds{1}\left\{\mX:\frac{h-1}{H}\leq p(\mX)<\frac{h}{H}\right\}\\&\stackrel{\text{step2}}{\approx} \sum_{h=1}^H\frac{h-1}{H}\mathds{1}\left\{\mX:\text{sign}(\hat f_{\frac{h-1}{H}}) = 1, \text{sign}(\hat f_{\frac{h}{H}}) = -1\right\},
\end{align}
where Step 1 approximates the target probability by linear combination of step functions and Step 2 uses the fact that the solution of \eqref{eq:weighted} is consistent to Bayes rule.
The probability estimation scheme can be summarized as follows.\\
{\bf Shceme}\vspace{-.4cm}
\begin{enumerate}[label={S.\arabic*}]
\item Choose a sequence of weight $\pi_h = \frac{h}{H}$, for $h = 1,\ldots, H$.
\item For each weight $\pi_h\in[0,1]$, solve Equation \eqref{eq:weighted} with $\omega_{\pi_h}(y)$
\item Denote the sequence of solutions and decision regions 
\begin{align}
\{\hat f_h\}_{h=1}^H \text{ and } \{\hat \tD_h\}_{h=1}^H = \left\{\left\{\mX:\text{sign}(\hat f_{\frac{h-1}{H}}) = 1, \text{sign}(\hat f_{\frac{h}{H}}) = -1\right\}\right\}_{h=1}^H
\end{align}
\item Estimate the target probability function by 
\begin{align}
\hat p(mX) = \sum_{h=1}^H\frac{h-1}{H}\mathds{1}\left\{\mX\in \hat \tD_h\right\}.
\end{align}
\end{enumerate}
Notice that this scheme can be applied to any large-margin classifiers though we  focus on the hinge loss function in this paper. Solution of weighted hinge loss in Equation \eqref{eq:weighted} is solved with simple modification from classification algorithm in Section \ref{sec:alg}.


\section{Algorithm}
\label{sec:alg}
In this Section, we describe the algorithm to seek the optimizer of Equation  \eqref{eq:large-margin} in the case of hinge loss function $L(z) = (1-z)_+$ and linear function class $\tF = \{f:f(\cdot)= \langle \mU\mV^T,\cdot\rangle, \text{ where }\mU\in\mathbb{R}^{d_1\times r}\mV\in\mathbb{R}^{d_2\times r}\}$.
Equation \eqref{eq:large-margin} is written as 
\begin{align}
\label{eq:opt}
\min_{\{(\mU,\mV)\in\mathbb{R}^{d_1\times r}\times \mathbb{R}^{d_2\times r}\}}n^{-1}\sum_{i=1}^n \left(1-y_i\langle \mU\mV^T,\mX_i\rangle\right)_++\lambda \FnormSize{}{\mU\mV^T}^2
\end{align}
We optimize Equation \eqref{eq:opt} with a coordinate descent algorithm that solves one block holding the other block fixed.  Each step is a convex optimization and can be solved with quadratic programming.
To be specific, when we fix $\mV$ and update $\mU$ we have the following equivalent dual problem 
\begin{align}
 \hspace{1.5cm}\max_{\malpha\in\mathbb{R}^n:\malpha\geq0}& \left(\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\langle \mX_i,\mX_j \mV(\mV^T\mV)^{-1}\mV^T\rangle\right)\\
    \text{subject to}&\quad\sum_{i=1}^Ny_i\alpha_i = 0,\quad0\leq\alpha_i\leq\frac{1}{2\lambda n},\quad i=1.\cdots,n,
    \end{align}
   We use quadratic programming to solve this dual problem and update $\mU = \sum_{i=1}^n\alpha_iy_i\mX_i\mV(\mV^T\mV)^{-1}.$
Similar approach is applied to update $\mV$ fixing $\mU$.  The Algorithm \ref{alg:linear} gives the full description.

 \begin{algorithm}[h]
 \label{alg:linear}
\KwIn{$(\mX_1,y_1),\cdots,(\mX_n,y_m)$, rank $r$}
{\bf Parameter:} U,V\\
{\bf Initizlize:} $\mU^{(0)}, \mV^{(0)}$\\
{\bf Do until converges}\\
\hspace*{.5cm}{\bf Update} $\mU$ fixing $\mV$ :\\[.1cm]
\hspace*{.4cm} Solve $ \max_{\malpha}\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n \sum_{j=1}^n\alpha_i\alpha_jy_iy_j\langle \mX_i,\mX_j\mV(\mV^T\mV)^{-1}\mV^T\rangle$.\\
\hspace{.5cm} $\mU = \sum_{i=1}^n\alpha_iy_i \mX_i\mV(\mV^T\mV)^{-1}$.\\[.1cm]
\hspace*{.5cm}{\bf Update} $\mV$ fixing $\mU$ :\\[.1cm]
\hspace*{.4cm} Solve  $ \max_{\malpha}\sum_{i=1}^n\alpha_i-\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_jy_iy_j\langle \mX_i,\mU(\mU^T\mU)^{-1}\mU^T\mX_j\rangle$.\\
\hspace{.5cm} $\mV = \sum_{i=1}^n\alpha_iy_i \mX_i^T\mU(\mU^T\mU)^{-1}$.\\[.1cm]
\KwOut{ $\mB = \mU\mV^T$}
    \caption{{\bf Linear classification algorithm} }
\label{alg:smm}
\end{algorithm}

\section{Extension to nonlinear case}
\label{sec:nonlinear}
We extend linear function class to non-linear class with kernel trick. We enlarge feature space through feature mapping $\mh:\mathbb{R}^{d_1\times d_2}\rightarrow \mathbb{R	}^{d_1\times d_2'}$. Once this mapping fixed, the procedure is the same as before.
We fit the linear classifier using pair of input feature and label $\{\mh(\mX_i),y_i\}_{i=1}^n$.
Define a nonlinear low rank kernel in similar way to linear case.
\begin{align}\label{eq:nonlinear kernel}
\langle \mX,\mX'\rangle_{\mP_r,h} &\stackrel{\text{def}}{=} \langle \mP_r h(\mX),\mP_r h(\mX')\rangle = \text{trace}\left[\mK(\mX,\mX')\mP_r^T\mP_r\right]\quad\text{ for all } \mX,\mX'\in\mathbb{R}^{d_1\times d_2},
\end{align}
where $\mK(\mX,\mX')\stackrel{\text{def}}{=} h(\mX)h^T(\mX')
\in \mathbb{R}^{d_1\times d_1}$ denotes the matrix product of mapped features.
The solution function $ f(\cdot)$ of \eqref{eq:large-margin} on enlarged feature can be written 
\begin{align}
f(\cdot) = \sum_{i=1}^n\alpha _i y_i \langle \mP_r h(\mX_i), \mP_r h(\cdot)\rangle =  \sum_{i=1}^n  \alpha _i y_i \langle \mX_i, \cdot\rangle_{ \mP_r,h}  =  \sum_{i=1}^n  \alpha _i y_i \text{trace}\left[\mK(\mX_i,\cdot)\ \mP_r^T \mP_r\right],
\end{align}
which involves feature mapping $h(\mX)$ only thorough inner products. In fact, we need not specify the the transformation $h(\mX)$ at all but only requires knowledge of the $
\mK(\mX,\mX')$. A sufficient condition and a necessary condition for $\mK$ being reasonable appear in Supplement.
Three popular choices for $\mK$ are
\begin{itemize}
\item Linear kernel: $\mK(\mX,\mX')=\mX\mX'^T$.
\item Polynomial kernel with degree $m$: $\mK(\mX,\mX')=(\mX\mX'^T+\lambda\mI)^{\circ m}$.
\item Gaussian kernel: the $(i,j)$-th entry of $\mK(\mX,\mX')$ is 
\[
\left[\mK(\mX,\mX')\right]_{(i,j)}=\exp\left\{-{1\over 2\sigma^2} \|\mX[i,\colon]-\mX'[j,\colon]\|_2^2\right\}
\]
for all $(i,j)\in[d_1]\times[d_1]$.
\end{itemize}
One can check detailed description for non-linear case algorithm in Supplement.
\section{Theory}
\label{sec:thm}
\section{Conclusion}
\label{sec:conc}


\bigskip
\begin{center}
{\large\bf SUPPLEMENTARY MATERIAL}
\end{center}


\bibliographystyle{Chicago}
\bibliography{nonpara}

\end{document}
