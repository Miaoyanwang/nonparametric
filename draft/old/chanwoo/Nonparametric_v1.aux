\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand \oddpage@label [2]{}
\citation{zhou2014regularized}
\citation{Shashua2004Pedestrian}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}}
\newlabel{sec:intro}{{1}{2}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Introduction (skipped)}{2}{section.2}}
\citation{man2005expression}
\@writefile{toc}{\contentsline {section}{\numberline {3}Three learning problems}{5}{section.3}}
\citation{willett2007minimax,scott2007regression,wang2008probability}
\newlabel{eq:classloss}{{1}{6}{Three learning problems}{equation.3.1}{}}
\MT@newlabel{eq:classloss}
\newlabel{eq:bayes}{{2}{6}{Three learning problems}{equation.3.2}{}}
\MT@newlabel{eq:bayes}
\MT@newlabel{eq:classloss}
\MT@newlabel{eq:bayes}
\newlabel{eq:level}{{3}{6}{Three learning problems}{equation.3.3}{}}
\newlabel{eq:risklevel}{{4}{7}{Three learning problems}{equation.3.4}{}}
\MT@newlabel{eq:classloss}
\MT@newlabel{eq:risklevel}
\newlabel{eq:regression}{{5}{7}{Three learning problems}{equation.3.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}From classification to regression: a new deal}{8}{section.4}}
\newlabel{sec:idea}{{4}{8}{From classification to regression: a new deal}{section.4}{}}
\citation{gibou2018review}
\citation{gibou2018review}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) Our learning reduction approach (solid line) to three learning problems and classical plug-in approaches (dashed line). (b) Schematic diagram for nonparametric function estimation via level set estimation (Figure modified from\nobreakspace  {}\cite  {gibou2018review}).}}{9}{figure.1}}
\newlabel{fig:diagram}{{1}{9}{(a) Our learning reduction approach (solid line) to three learning problems and classical plug-in approaches (dashed line). (b) Schematic diagram for nonparametric function estimation via level set estimation (Figure modified from~\cite {gibou2018review})}{figure.1}{}}
\MT@newlabel{eq:classloss}
\MT@newlabel{eq:risklevel}
\MT@newlabel{eq:regression}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Level set approaches to nonparametric matrix regression}{10}{subsection.4.1}}
\newlabel{sec:bridge}{{4.1}{10}{Level set approaches to nonparametric matrix regression}{subsection.4.1}{}}
\newlabel{eq:stepfunction}{{6}{10}{Level set approaches to nonparametric matrix regression}{equation.4.6}{}}
\MT@newlabel{eq:risklevel}
\newlabel{eq:constrained}{{7}{10}{Level set approaches to nonparametric matrix regression}{equation.4.7}{}}
\MT@newlabel{eq:level}
\newlabel{ass:identifiability}{{1}{10}{Identifiability}{assumption.1}{}}
\newlabel{eq:identity}{{8}{10}{Identifiability}{equation.4.8}{}}
\MT@newlabel{eq:identity}
\MT@newlabel{eq:identity}
\newlabel{thm:twobounds}{{4.1}{11}{Nonparametric regression via weighted classifications}{thm.4.1}{}}
\MT@newlabel{eq:stepfunction}
\newlabel{eq:approximation}{{9}{11}{Nonparametric regression via weighted classifications}{equation.4.9}{}}
\MT@newlabel{eq:approximation}
\newlabel{eq:mass}{{10}{12}{Level set approaches to nonparametric matrix regression}{equation.4.10}{}}
\MT@newlabel{eq:mass}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Sparse and low-rank function boundaries}{12}{subsection.4.2}}
\newlabel{subsec:linear class}{{4.2}{12}{Sparse and low-rank function boundaries}{subsection.4.2}{}}
\MT@newlabel{eq:constrained}
\MT@newlabel{eq:constrained}
\newlabel{eq:optimization}{{11}{12}{Sparse and low-rank function boundaries}{equation.4.11}{}}
\newlabel{eq:class}{{12}{13}{Sparse and low-rank function boundaries}{equation.4.12}{}}
\MT@newlabel{eq:stepfunction}
\MT@newlabel{eq:optimization}
\MT@newlabel{eq:class}
\newlabel{eq:proposal}{{13}{13}{Sparse and low-rank function boundaries}{equation.4.13}{}}
\MT@newlabel{eq:class}
\citation{zhou2014regularized,guha2020bayesian,relion2019network}
\citation{hu2020matrix}
\MT@newlabel{eq:optimization}
\MT@newlabel{eq:class}
\MT@newlabel{eq:approximation}
\MT@newlabel{eq:level}
\MT@newlabel{eq:optimization}
\MT@newlabel{eq:optimization}
\MT@newlabel{eq:class}
\@writefile{toc}{\contentsline {section}{\numberline {5}Large-margin learning with high dimensional matrices}{15}{section.5}}
\newlabel{sec:theory}{{5}{15}{Large-margin learning with high dimensional matrices}{section.5}{}}
\MT@newlabel{eq:proposal}
\citation{bartlett2006convexity}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Classification of high dimensional matrices}{16}{subsection.5.1}}
\newlabel{subsec:pb1}{{5.1}{16}{Classification of high dimensional matrices}{subsection.5.1}{}}
\MT@newlabel{eq:optimization}
\newlabel{eq:large-margin}{{14}{16}{Classification of high dimensional matrices}{equation.5.14}{}}
\MT@newlabel{eq:class}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:optimization}
\MT@newlabel{eq:optimization}
\MT@newlabel{eq:optimization}
\MT@newlabel{eq:large-margin}
\newlabel{ass:main}{{2}{17}{Conditions for matrix classification}{assumption.2}{}}
\MT@newlabel{eq:bayes}
\newlabel{eq:mean-variance}{{15}{18}{Conditions for matrix classification}{Item.4}{}}
\MT@newlabel{eq:mean-variance}
\MT@newlabel{eq:identity}
\MT@newlabel{eq:large-margin}
\newlabel{thm:main}{{5.1}{18}{Accuracy for matrix classification}{thm.5.1}{}}
\MT@newlabel{eq:bayes}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:class}
\newlabel{eq:riskbound}{{16}{18}{Accuracy for matrix classification}{equation.5.16}{}}
\MT@newlabel{eq:class}
\citation{tsybakov2004optimal,audibert2007fast,tsybakov2006discussion}
\citation{scott2011surrogate,bartlett2006convexity}
\citation{scott2011surrogate}
\MT@newlabel{eq:riskbound}
\MT@newlabel{eq:riskbound}
\MT@newlabel{eq:riskbound}
\MT@newlabel{eq:riskbound}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Level set estimation in high dimensional matrix space}{19}{subsection.5.2}}
\newlabel{subsec:pb2}{{5.2}{19}{Level set estimation in high dimensional matrix space}{subsection.5.2}{}}
\newlabel{eq:weighted}{{17}{19}{Level set estimation in high dimensional matrix space}{equation.5.17}{}}
\MT@newlabel{eq:weighted}
\newlabel{eq:excess}{{18}{20}{Level set estimation in high dimensional matrix space}{equation.5.18}{}}
\MT@newlabel{eq:excess}
\newlabel{ass:identifiable}{{3}{20}{Identifiability under surrogate loss}{assumption.3}{}}
\newlabel{eq:identitysurrogate}{{19}{20}{Identifiability under surrogate loss}{equation.5.19}{}}
\MT@newlabel{eq:mean-variance}
\MT@newlabel{eq:identitysurrogate}
\MT@newlabel{eq:mean-variance}
\citation{singh2009adaptive}
\MT@newlabel{eq:identity}
\MT@newlabel{eq:excess}
\newlabel{thm:level}{{5.2}{21}{Accuracy for level set estimation}{thm.5.2}{}}
\MT@newlabel{eq:weighted}
\newlabel{eq:levelset}{{5.2}{21}{Accuracy for level set estimation}{thm.5.2}{}}
\MT@newlabel{eq:identity}
\MT@newlabel{eq:weighted}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Nonparametric matrix regression}{22}{subsection.5.3}}
\newlabel{subsec:pb3}{{5.3}{22}{Nonparametric matrix regression}{subsection.5.3}{}}
\newlabel{eq:empirical}{{20}{22}{Nonparametric matrix regression}{equation.5.20}{}}
\MT@newlabel{eq:proposal}
\newlabel{thm:regression}{{5.3}{22}{Accuracy for nonparametric matrix regression}{thm.5.3}{}}
\MT@newlabel{eq:empirical}
\newlabel{eq:final}{{21}{23}{Nonparametric matrix regression}{equation.5.21}{}}
\MT@newlabel{eq:final}
\MT@newlabel{eq:riskbound}
\@writefile{toc}{\contentsline {section}{\numberline {6}Alternating optimization for structural risk minimization}{24}{section.6}}
\newlabel{sec:alg}{{6}{24}{Alternating optimization for structural risk minimization}{section.6}{}}
\MT@newlabel{eq:large-margin}
\MT@newlabel{eq:class}
\newlabel{eq:opt}{{22}{24}{Alternating optimization for structural risk minimization}{equation.6.22}{}}
\MT@newlabel{eq:opt}
\MT@newlabel{eq:opt}
\newlabel{eq:dual1}{{6}{24}{Alternating optimization for structural risk minimization}{equation.6.22}{}}
\newlabel{eq:C}{{23}{24}{Alternating optimization for structural risk minimization}{equation.6.23}{}}
\MT@newlabel{eq:C}
\MT@newlabel{eq:opt}
\newlabel{eq:dual2}{{24}{25}{Alternating optimization for structural risk minimization}{equation.6.24}{}}
\MT@newlabel{eq:dual2}
\MT@newlabel{eq:C}
\MT@newlabel{eq:dual2}
\newlabel{eq:helpeq}{{6}{25}{Alternating optimization for structural risk minimization}{equation.6.24}{}}
\MT@newlabel{eq:dual2}
\newlabel{eq:output}{{25}{25}{Alternating optimization for structural risk minimization}{equation.6.25}{}}
\MT@newlabel{eq:weighted}
\MT@newlabel{eq:output}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces {\bf  Classification algorithm} }}{26}{algocf.1}}
\newlabel{alg:svm}{{1}{26}{Alternating optimization for structural risk minimization}{algocf.1}{}}
\MT@newlabel{eq:weighted}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces {\bf  Level set \& Regression Algorithm} }}{27}{algocf.2}}
\newlabel{alg:regest}{{2}{27}{Alternating optimization for structural risk minimization}{algocf.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Extension to nonlinear boundaries}{28}{section.7}}
\newlabel{sec:nonlinear}{{7}{28}{Extension to nonlinear boundaries}{section.7}{}}
\newlabel{def:map}{{1}{29}{}{defn.1}{}}
\newlabel{eq:featuremap}{{26}{29}{}{equation.7.26}{}}
\newlabel{eq:linearfcn}{{27}{29}{}{equation.7.27}{}}
\MT@newlabel{eq:featuremap}
\MT@newlabel{eq:linearfcn}
\MT@newlabel{eq:linearfcn}
\newlabel{eq:lowrk}{{28}{30}{Extension to nonlinear boundaries}{equation.7.28}{}}
\MT@newlabel{eq:linearfcn}
\MT@newlabel{eq:lowrk}
\MT@newlabel{eq:class}
\newlabel{def:kernel}{{2}{30}{}{defn.2}{}}
\bibstyle{chicago}
\bibdata{tensor_wang}
\@writefile{toc}{\contentsline {section}{\numberline {8}Numerical results}{31}{section.8}}
\newlabel{sec:numerical}{{8}{31}{Numerical results}{section.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Discussion}{31}{section.9}}
